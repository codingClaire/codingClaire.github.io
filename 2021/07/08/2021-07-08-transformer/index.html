<!DOCTYPE html>
<html lang="zh-CN">
  <head><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="Transformer总结及源码分析"/><meta name="keywords" content="NLP, Transformer, attention, Ruoting Wu's Blog" /><link rel="alternate" href="/atom.xml" title="Ruoting Wu's Blog"><link rel="shortcut icon" type="image/x-icon" href="/shiba.png?v=2.11.0" />
<link rel="canonical" href="https://codingClaire.github.io/2021/07/08/2021-07-08-transformer/"/>

<link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" /><script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.0" />

<script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "UAlwruFunxBGx8T5d5R2vDKT-gzGzoHsz",
      appKey: "c0YVFIAlMCeKLx899vBJAnMV"
    });
  </script><script>
  window.config = {"leancloud":{"app_id":"UAlwruFunxBGx8T5d5R2vDKT-gzGzoHsz","app_key":"c0YVFIAlMCeKLx899vBJAnMV"},"toc":true,"fancybox":true,"pjax":"","latex":true};
</script>

    <title>Transformer总结及源码分析 - Ruoting Wu's Blog</title>
  <meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="Ruoting Wu's Blog" type="application/atom+xml">
</head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Ruoting Wu's Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/">
        <li class="mobile-menu-item">首页
          </li>
      </a><a href="/tags">
        <li class="mobile-menu-item">标签
          </li>
      </a><a href="/about">
        <li class="mobile-menu-item">关于
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Ruoting Wu's Blog</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/">
            首页
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            标签
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/about">
            关于
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">Transformer总结及源码分析
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2021-07-08
        </span><span class="post-visits"
             data-url="/2021/07/08/2021-07-08-transformer/"
             data-title="Transformer总结及源码分析">
          阅读次数 0
        </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#回顾Attention"><span class="toc-text">回顾Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Q、K与V"><span class="toc-text">Q、K与V</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scaled-Dot-Product-Attention"><span class="toc-text">Scaled-Dot-Product Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-Attention"><span class="toc-text">Self-Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Head-Self-Attention"><span class="toc-text">Multi-Head Self-Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-vanilla-Transformer"><span class="toc-text">Transformer(vanilla Transformer)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder"><span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder"><span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-wise-Feed-Forward-Network"><span class="toc-text">Positional-wise Feed-Forward Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Residual-Connection-and-Normalization"><span class="toc-text">Residual Connection and Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training"><span class="toc-text">Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer中的注意力机制"><span class="toc-text">Transformer中的注意力机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-Family"><span class="toc-text">Transformer Family</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#隐藏状态重用"><span class="toc-text">隐藏状态重用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#相对位置编码"><span class="toc-text">相对位置编码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reformer（ICLR-2020）"><span class="toc-text">Reformer（ICLR 2020）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sparse-Transformer"><span class="toc-text">Sparse Transformer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-text">参考文献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-NMT相关项目、源码"><span class="toc-text">Transformer&#x2F;NMT相关项目、源码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献-1"><span class="toc-text">参考文献</span></a></li></ol>
    </div>
  </div><div class="post-content"><p>[toc]</p>
<h2 id="回顾Attention"><a href="#回顾Attention" class="headerlink" title="回顾Attention"></a>回顾Attention</h2><h3 id="Q、K与V"><a href="#Q、K与V" class="headerlink" title="Q、K与V"></a>Q、K与V</h3><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled-Dot-Product Attention"></a>Scaled-Dot-Product Attention</h3><div align="center">    
<img src="/2021/07/08/2021-07-08-transformer/3.png" width="30%" height="30%">
</div>

<p>$$<br>Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V=AV<br>$$</p>
<p>${d_k}$是key和value的维度，当${d_k}$越大，则$QK^T$越大，经过$softmax$函数可能会在梯度极小的区域，为了避免梯度消失的问题，scaled-dot-product的操作是将$QK^T$除以$\sqrt{d_k}$，但仍然$QK^T$保持方差为1。</p>
<p>在Transformer的<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">源码</a>中，scale-dot-product attention的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>自注意力机制（Self-attention）考虑的是输入元素之间的相关性，而非输入元素和输出元素之间的相关性，其目标序列和输入的原始序列相同。</p>
<div align="center">    
<img src="/2021/07/08/2021-07-08-transformer/7.png" width="40%" height="40%">
</div>
<center>图1 self-attention原理图</center>

<h3 id="Multi-Head-Self-Attention"><a href="#Multi-Head-Self-Attention" class="headerlink" title="Multi-Head Self-Attention"></a>Multi-Head Self-Attention</h3><div align="center">    
<img src="/2021/07/08/2021-07-08-transformer/4.png" width="30%" height="30%">
</div>
<center>图2 多头注意力机制</center>


<p>多头注意力机制让模型能够在不同的位置上共同关注来自不同representation的子空间的信息。多个平行的attention层分别获得各自的Q、K、V：<br>$$<br>MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^O\space where\space head_i=Attention(QW_i^Q,KW_i^K,VW_I^V)<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h <span class="comment"># heads论文中取8</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        <span class="comment"># self-attention中query、key、value全部相同，size为[batch_size,len,d_model]</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment"># 还原序列</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure>



<h2 id="Transformer-vanilla-Transformer"><a href="#Transformer-vanilla-Transformer" class="headerlink" title="Transformer(vanilla Transformer)"></a>Transformer(vanilla Transformer)</h2><div align="center">    
<img src="/2021/07/08/2021-07-08-transformer/1.png" width="80%" height="80%">
</div>

<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul>
<li>层数$N=6$，每层有两个sub-layers，分别是多头自注意力机制和根据位置的全连接前馈网络</li>
<li>两个子层之前使用残差连接[11]和正则化</li>
<li>生成K、V矩阵</li>
</ul>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li><p>层数$N=6$，在encoder的两个子层之间加入第三层</p>
</li>
<li><p>对encoder的输出使用多头注意力机制</p>
</li>
<li><p>对decoder的第一层多头注意力层进行了修改-&gt;masking multi-head attention 保证第一层注意力机制在预测当前位置的单词时只依赖当前位置之前的单词</p>
</li>
<li><p>生成Q矩阵</p>
</li>
</ul>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><ul>
<li>$d_{model}$是词embedding的维度，论文中取512。</li>
<li>偶数位置：$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$ </li>
<li>奇数位置：$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$ </li>
</ul>
<p>绝对位置向量中蕴含着相对位置的信息，相对位置会在注意力机制中消失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model) <span class="comment">#d_model论文中取512</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment">#维度增加</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model)) <span class="comment">#相对位置公式</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment">#奇数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment">#偶数列</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment">#增加维度</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h3 id="Positional-wise-Feed-Forward-Network"><a href="#Positional-wise-Feed-Forward-Network" class="headerlink" title="Positional-wise Feed-Forward Network"></a>Positional-wise Feed-Forward Network</h3><p>$$<br>FFN(x)=max(0,xW_1+b_1)W_2+b_2<br>$$<br>FFN将每个位置的attention结果映射到一个更大维度的特征空间，然后用ReLU引入非线性层进行筛选，最后恢复原始维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>

<h3 id="Residual-Connection-and-Normalization"><a href="#Residual-Connection-and-Normalization" class="headerlink" title="Residual Connection and Normalization"></a>Residual Connection and Normalization</h3><div align="center">    
<img src="/2021/07/08/2021-07-08-transformer/5.png" width="40%" height="40%">
</div>

<p>残差结构：输出的Z和经过位置编码的X对位相加，作为输出。</p>
<p>Layer Normalization：不需要像Batch Normalization一样考虑所有batch，只需要考虑同一个example中的不同feature计算均值和方差，然后对example的向量进行normalization。</p>
<p>以下是实现的代码：</p>
<p>在子层与子层之间进行了残差连接与归一化，子层的输出为$LayerNorm(x+Sublayer(x))$。tensor2tensor的实现是$x+SubLayer(LayerNorm(x))$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>

<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li>teacher forcing: 使用正确的truth作为input。</li>
</ul>
<h2 id="Transformer中的注意力机制"><a href="#Transformer中的注意力机制" class="headerlink" title="Transformer中的注意力机制"></a>Transformer中的注意力机制</h2><ul>
<li>self-attention in encoder and decoder:  $Q=K=V=X$</li>
<li>masked self-attention:  在transformer的decoder中，masked是保证attention加权计算时忽略当前位置后面的单词，保证信息来源于当前位置以及之前的位置。</li>
<li>cross-attention:  keys和values来源于最后一层的encoder的输入。</li>
</ul>
<h2 id="Transformer-Family"><a href="#Transformer-Family" class="headerlink" title="Transformer Family"></a>Transformer Family</h2><div align="center">    
<img src="/2021/07/08/2021-07-08-transformer/6.png" width="60%" height="60%">
</div>
### Transformer-XL（ACL 2019）

<p>标准Transformer有一个固定有限的attention范围。在每一个更新的步中，模型只能够处理同一segment中的其他元素，信息无法在分离的segment中传递，也就意味着Transformer很难捕捉长期的依赖关系，在上下文较短的情况下，难以进行预测。同时，当segment每右移一位时，新的segment会重新处理，尽管有重叠的标记。</p>
<p><strong>Transformer-XL</strong> (<a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Dai et al., 2019</a>)进行了如下的改进：</p>
<ul>
<li><strong>重用</strong>segment之间的隐藏状态。</li>
<li>采用新的适合重用状态的<strong>位置编码</strong>。</li>
</ul>
<h4 id="隐藏状态重用"><a href="#隐藏状态重用" class="headerlink" title="隐藏状态重用"></a>隐藏状态重用</h4><p>Transformer-XL通过连续使用前一个segment的隐藏状态，将segment之间的递归引入到模型中。如下图，一个segment的长度为4，可以看出Transformer和Transformer-XL的区别。</p>
<div align="center">    
<img src="/2021/transformer-XL-training.png" width="80%" height="80%">
</div>
<center>图1 Transformer和Transformer-XL的对比（图片来源：Dai et al.,2019）</center>



<p>第$(\tau + 1)$段的第n层的隐藏状态记为 $\mathbf{h}<em>{\tau+1}^{(n)} \in \mathbb{R}^{L \times d}$，第n-1层的隐藏状态为 $\mathbf{h}</em>{\tau+1}^{(n-1)}$。前一个段的第n层隐藏状态为$\mathbf{h}_{\tau}^{(n)}$，模型有如下的公式，通过合并之前的隐藏状态的信息，实现注意力范围的延长。</p>
<p>$$\begin{aligned} {\widetilde{\mathbf{h}}<em>{\tau+1}^{(n-1)}} &amp;= [\text{stop-gradient}(\mathbf{h}</em>{\tau}^{(n-1)}) \circ \mathbf{h}<em>{\tau+1}^{(n-1)}] \ \mathbf{Q}</em>{\tau+1}^{(n)} &amp;= \mathbf{h}<em>{\tau+1}^{(n-1)}\mathbf{W}^q \ \mathbf{K}</em>{\tau+1}^{(n)} &amp;= {\widetilde{\mathbf{h}}<em>{\tau+1}^{(n-1)}} \mathbf{W}^k \ \mathbf{V}</em>{\tau+1}^{(n)} &amp;= {\widetilde{\mathbf{h}}<em>{\tau+1}^{(n-1)}} \mathbf{W}^v \ \mathbf{h}</em>{\tau+1}^{(n)} &amp;= \text{transformer-layer}(\mathbf{Q}<em>{\tau+1}^{(n)}, \mathbf{K}</em>{\tau+1}^{(n)}, \mathbf{V}_{\tau+1}^{(n)}) \end{aligned}$$</p>
<p>key和value都依赖于扩展后的隐藏状态，而query只依赖于当前的隐藏状态。</p>
<h4 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h4><p>Transformer-XL提出了一种新的位置编码来适应隐藏状态重用。在基础的Transformer中，编码绝对位置的话，前一段和当前段将会被相同的编码。在Transformer-XL中并不需要。为了保持段之间的位置信息的流动，Transformer-XL提出了相对位置编码，它只要知道位置的偏移量就可以做出预测。</p>
<p>$i$位置的query和$j$位置的key的attention score为：</p>
<p>$$\begin{aligned} a_{ij} &amp;= \mathbf{q}_i {\mathbf{k}_j}^\top = (\mathbf{x}_i + \mathbf{p}_i)\mathbf{W}^q ((\mathbf{x}_j + \mathbf{p}_j)\mathbf{W}^k)^\top \ &amp;= \mathbf{x}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{x}_j^\top + \mathbf{x}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{p}_j^\top + \mathbf{p}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{x}_j^\top + \mathbf{p}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{p}_j^\top \end{aligned}$$</p>
<p>整理可得：</p>
<p>$$a_{ij}^\text{rel} = \underbrace{ \mathbf{x}<em>i\mathbf{W}^q { {\mathbf{W}_E^k}^\top } \mathbf{x}_j^\top }_\text{content-based addressing} + \underbrace{ \mathbf{x}_i\mathbf{W}^q { {\mathbf{W}_R^k}^\top } {\mathbf{r}</em>{i-j}^\top} }<em>\text{content-dependent positional bias} + \underbrace{ {\mathbf{u}} { {\mathbf{W}_E^k}^\top } \mathbf{x}_j^\top }_\text{global content bias} + \underbrace{ {\mathbf{v}} { {\mathbf{W}_R^k}^\top } {\mathbf{r}</em>{i-j}^\top} }_\text{global positional bias}$$</p>
<p>其中：</p>
<ul>
<li>$\mathbf{p}<em>j$ 被替换为相对位置编码 $\mathbf{r}</em>{i-j} \in \mathbf{R}^{d}$;</li>
<li>$\mathbf{p}_i\mathbf{W}^q$ 被替换为两个可训练的参数 ${u}$  和 ${v}$ ，分别用于计算content和location信息</li>
<li>$\mathbf{W}^k$ 被分成两个矩阵， $\mathbf{W}^k_E$ 处理content信息， $\mathbf{W}^k_R$ 处理location信息。</li>
</ul>
<h3 id="Reformer（ICLR-2020）"><a href="#Reformer（ICLR-2020）" class="headerlink" title="Reformer（ICLR 2020）"></a>Reformer（ICLR 2020）</h3><p>Reformer(<a href="https://arxiv.org/abs/2001.04451" target="_blank" rel="noopener">Kitaev, et al. 2020</a>)主要解决了Transformer以下的几个问题：</p>
<ul>
<li>具有$N$层的模型中的内存是单层模型中的$N$倍。</li>
<li>中间Feed Forward层通常相当大。</li>
<li>长度为$L$的序列上的attention matrix通常需要$O（L^2）$的内存和时间。</li>
</ul>
<p>Reformer主要的改进为：</p>
<ul>
<li>将dot-product attention替换为 <strong>locality-sensitive hashing (LSH) attention</strong>，将复杂度从 $O(L^2)$ 降低到$O(L\log L)$。</li>
<li>将残差模块替换为<strong>reversible的残差层</strong>，在训练期间仅存储一次激活（activation），而不是$N$次。</li>
</ul>
<div align="center">    
<img src="/2021/LSH-attention-matrix.png" width="80%" height="80%">
</div>
<center>图2. LSH attention原理 (图片来源：Kitaev, et al. 2020）</center>

<div align="center">    
<img src="/2021/LSH-attention.png" width="60%" height="60%">
</div>
<center>图3. LSH attention的计算步骤 (图片来源：Kitaev, et al. 2020）</center>

<h3 id="Sparse-Transformer"><a href="#Sparse-Transformer" class="headerlink" title="Sparse Transformer"></a>Sparse Transformer</h3><p>(<a href="https://arxiv.org/abs/1904.10509" target="_blank" rel="noopener">Child et al., 2019</a>) 通过稀疏矩阵分解，提出了<strong>factorized self-attention</strong>，使得在16384的序列长度上训练数百层的密集注意力网络成为可能，否则在硬件上是不可行的。</p>
<p>给定一组注意力连接模式（attention connectivity pattern） $\mathcal{S} = {S_1, \dots, S_n}$, 其中每个 $S_i$ 记录了第i个query vector的注意到的一组key position，有：</p>
<p>$$\begin{aligned} \text{Attend}(\mathbf{X}, \mathcal{S}) &amp;= \Big( a(\mathbf{x}<em>i, S_i) \Big)</em>{i \in {1, \dots, L}}  \text{ where } a(\mathbf{x}<em>i, S_i)  = \text{softmax}\Big(\frac{(\mathbf{x}_i \mathbf{W}^q)(\mathbf{x}_j \mathbf{W}^k)</em>{j \in S_i}^\top}{\sqrt{d_k}}\Big) (\mathbf{x}<em>j \mathbf{W}^v)</em>{j \in S_i} \end{aligned}$$</p>
<p>在自回归模型中，一个注意力范围（attention span ）被定义为 $S_i = {j: j \leq i}$ ，因为它允许每个token关注过去的所有位置。</p>
<p>在factorized self-attention中, $S_i$ 会被分解成一个依赖关系的树，对每一对 $(i, j)$ 其中$j \leq i$,  都有一条将$i$ 连接到 $j$ 的路径 ，$i$ 可以直接或间接地注意到 $j$。</p>
<p>$S_i$被分成p个非重叠的子集，第m个子集表示为$A^{(m)}_i \subset S_i, m = 1,\dots, p$，这样输出位置i和任意一个j的最大距离为$p+1$。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><h2 id="Transformer-NMT相关项目、源码"><a href="#Transformer-NMT相关项目、源码" class="headerlink" title="Transformer/NMT相关项目、源码"></a>Transformer/NMT相关项目、源码</h2><table>
<thead>
<tr>
<th>项目</th>
<th>来源</th>
<th>文档</th>
<th>Github</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Transformers</td>
<td>hugging face</td>
<td>[<a href="https://huggingface.co/transformers/index.html" target="_blank" rel="noopener">doc</a>]</td>
<td>[<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">source code</a>]</td>
<td></td>
</tr>
<tr>
<td>OpenNMT</td>
<td>Harvard NLP group and SYSTRAN</td>
<td><a href="https://opennmt.net/OpenNMT-py/" target="_blank" rel="noopener">[doc]</a></td>
<td>[<a href="https://github.com/OpenNMT/OpenNMT" target="_blank" rel="noopener">source code</a>]</td>
<td></td>
</tr>
<tr>
<td>Tensor2Tensor</td>
<td>Google Brain team</td>
<td>[<a href="https://github.com/tensorflow/tensor2tensor/blob/master/README.md" target="_blank" rel="noopener">Readme</a>]</td>
<td><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">[source code]</a></td>
<td></td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U sacremoses</span><br></pre></td></tr></table></figure>

<h2 id="参考文献-1"><a href="#参考文献-1" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Li Hungyi. <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf" target="_blank" rel="noopener">“Self-Attention” </a> lecture PPT,2021.</p>
<p>[2] Ashish Vaswani, et al. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">“Attention is all you need.”</a> NIPS,2017.</p>
<p>[3] Alexander Rush. <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">“The Annotated Transformer”</a> 2018.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onmt_translate -model toy-ende&#x2F;trans9_step_5000.pt -src toy-ende&#x2F;src-test.txt -output toy-ende&#x2F;pred_trans9.txt -gpu 0 -verbose</span><br></pre></td></tr></table></figure>

      </div>
      <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="https://codingClaire.github.io">Ruoting Wu</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="https://codingclaire.github.io/2021/07/08/2021-07-08-transformer/">https://codingclaire.github.io/2021/07/08/2021-07-08-transformer/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>
      <div class="post-reward">
    <input type="checkbox" name="reward" id="reward" hidden />
    <label class="reward-button" for="reward">赞赏支持</label>
    <div class="qr-code"><label class="qr-code-image" for="reward">
          <img class="image" src="/cat.jpg" title="wechat">
        </label>
      <label class="qr-code-image" for="reward">
          <img class="image" src="/cat.jpg" title="alipay">
        </label>
      </div>
  </div><footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/NLP/">NLP</a>
            <a href="/tags/Transformer/">Transformer</a>
            <a href="/tags/attention/">attention</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/2021/07/15/2021-07-15-transformer2/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Transformer Family汇总</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    <a class="next" href="/2021/07/01/2021-07-01-attention/">
        <span class="next-text nav-default">Attention Mechanism总结</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments"></div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
        <a href="https://github.com/codingClaire" target="_blank" rel="noopener" class="iconfont icon-github" title="github"></a>
        <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    </div><div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even" target="_blank" rel="noopener">Even</a>
  </span>

  <span class="copyright-year">&copy;2019 - 2021<span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Ruoting Wu</span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.0"></script>
</body>
</html>

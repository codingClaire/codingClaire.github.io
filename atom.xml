<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ruoting Wu&#39;s Blog</title>
  
  
  <link href="https://codingclaire.github.io/atom.xml" rel="self"/>
  
  <link href="https://codingclaire.github.io/"/>
  <updated>2021-07-09T17:09:39.889Z</updated>
  <id>https://codingclaire.github.io/</id>
  
  <author>
    <name>Ruoting Wu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer总结及源码分析</title>
    <link href="https://codingclaire.github.io/2021/07/08/2021-07-08-transformer/"/>
    <id>https://codingclaire.github.io/2021/07/08/2021-07-08-transformer/</id>
    <published>2021-07-08T02:25:27.000Z</published>
    <updated>2021-07-09T17:09:39.889Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h2 id="回顾Attention"><a href="#回顾Attention" class="headerlink" title="回顾Attention"></a>回顾Attention</h2><h3 id="Q、K与V"><a href="#Q、K与V" class="headerlink" title="Q、K与V"></a>Q、K与V</h3><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled-Dot-Product Attention"></a>Scaled-Dot-Product Attention</h3><div align="center">    <img src="/2021/07/08/2021-07-08-transformer/3.png" width="30%" height="30%"></div><p>$$<br>Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V=AV<br>$$</p><p>${d_k}$是key和value的维度，当${d_k}$越大，则$QK^T$越大，经过$softmax$函数可能会在梯度极小的区域，为了避免梯度消失的问题，scaled-dot-product的操作是将$QK^T$除以$\sqrt{d_k}$，但仍然$QK^T$保持方差为1。</p><p>在Transformer的<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">源码</a>中，scale-dot-product attention的实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>自注意力机制（Self-attention）考虑的是输入元素之间的相关性，而非输入元素和输出元素之间的相关性，其目标序列和输入的原始序列相同。</p><div align="center">    <img src="/2021/07/08/2021-07-08-transformer/7.png" width="40%" height="40%"></div><center>图1 self-attention原理图</center><h3 id="Multi-Head-Self-Attention"><a href="#Multi-Head-Self-Attention" class="headerlink" title="Multi-Head Self-Attention"></a>Multi-Head Self-Attention</h3><div align="center">    <img src="/2021/07/08/2021-07-08-transformer/4.png" width="30%" height="30%"></div><p>多头注意力机制让模型能够在不同的位置上共同关注来自不同representation的子空间的信息。多个平行的attention层分别获得各自的Q、K、V：<br>$$<br>MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^O\space where\space head_i=Attention(QW_i^Q,KW_i^K,VW_I^V)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h <span class="comment"># heads论文中取8</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        <span class="comment"># self-attention中query、key、value全部相同，size为[batch_size,len,d_model]</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment"># 还原序列</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><h2 id="Transformer-vanilla-Transformer"><a href="#Transformer-vanilla-Transformer" class="headerlink" title="Transformer(vanilla Transformer)"></a>Transformer(vanilla Transformer)</h2><div align="center">    <img src="/2021/07/08/2021-07-08-transformer/1.png" width="80%" height="80%"></div><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li>层数$N=6$，每层有两个sub-layers，分别是多头自注意力机制和根据位置的全连接前馈网络</li><li>两个子层之前使用残差连接[11]和正则化</li><li>生成K、V矩阵</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li><p>层数$N=6$，在encoder的两个子层之间加入第三层</p></li><li><p>对encoder的输出使用多头注意力机制</p></li><li><p>对decoder的第一层多头注意力层进行了修改-&gt;masking multi-head attention 保证第一层注意力机制在预测当前位置的单词时只依赖当前位置之前的单词</p></li><li><p>生成Q矩阵</p></li></ul><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><ul><li>$d_{model}$是词embedding的维度，论文中取512。</li><li>偶数位置：$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$ </li><li>奇数位置：$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$ </li></ul><p>绝对位置向量中蕴含着相对位置的信息，相对位置会在注意力机制中消失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model) <span class="comment">#d_model论文中取512</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment">#维度增加</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model)) <span class="comment">#相对位置公式</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment">#奇数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment">#偶数列</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment">#增加维度</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h3 id="Positional-wise-Feed-Forward-Network"><a href="#Positional-wise-Feed-Forward-Network" class="headerlink" title="Positional-wise Feed-Forward Network"></a>Positional-wise Feed-Forward Network</h3><p>$$<br>FFN(x)=max(0,xW_1+b_1)W_2+b_2<br>$$<br>FFN将每个位置的attention结果映射到一个更大维度的特征空间，然后用ReLU引入非线性层进行筛选，最后恢复原始维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure><h3 id="Residual-Connection-and-Normalization"><a href="#Residual-Connection-and-Normalization" class="headerlink" title="Residual Connection and Normalization"></a>Residual Connection and Normalization</h3><div align="center">    <img src="/2021/07/08/2021-07-08-transformer/5.png" width="40%" height="40%"></div><p>残差结构：输出的Z和经过位置编码的X对位相加，作为输出。</p><p>Layer Normalization：不需要像Batch Normalization一样考虑所有batch，只需要考虑同一个example中的不同feature计算均值和方差，然后对example的向量进行normalization。</p><p>以下是实现的代码：</p><p>在子层与子层之间进行了残差连接与归一化，子层的输出为$LayerNorm(x+Sublayer(x))$。tensor2tensor的实现是$x+SubLayer(LayerNorm(x))$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul><li>teacher forcing: 使用正确的truth作为input。</li></ul><h2 id="Transformer中的注意力机制"><a href="#Transformer中的注意力机制" class="headerlink" title="Transformer中的注意力机制"></a>Transformer中的注意力机制</h2><ul><li>self-attention in encoder and decoder:  $Q=K=V=X$</li><li>masked self-attention:  在transformer的decoder中，masked是保证attention加权计算时忽略当前位置后面的单词，保证信息来源于当前位置以及之前的位置。</li><li>cross-attention:  keys和values来源于最后一层的encoder的输入。</li></ul><h2 id="Transformer-Family"><a href="#Transformer-Family" class="headerlink" title="Transformer Family"></a>Transformer Family</h2><div align="center">    <img src="/2021/07/08/2021-07-08-transformer/6.png" width="60%" height="60%"></div>### Transformer-XL<p>如何赋予编码器捕获长距离依赖的能力(代码表示的场景下，代码长度较长，如何捕捉长距离依赖)</p><p>【TODO】</p><h2 id="Transformer-NMT相关项目、源码"><a href="#Transformer-NMT相关项目、源码" class="headerlink" title="Transformer/NMT相关项目、源码"></a>Transformer/NMT相关项目、源码</h2><table><thead><tr><th>项目</th><th>来源</th><th>文档</th><th>Github</th><th>描述</th></tr></thead><tbody><tr><td>Transformers</td><td>hugging face</td><td>[<a href="https://huggingface.co/transformers/index.html" target="_blank" rel="noopener">doc</a>]</td><td>[<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">source code</a>]</td><td></td></tr><tr><td>OpenNMT</td><td>Harvard NLP group and SYSTRAN</td><td><a href="https://opennmt.net/OpenNMT-py/" target="_blank" rel="noopener">[doc]</a></td><td>[<a href="https://github.com/OpenNMT/OpenNMT" target="_blank" rel="noopener">source code</a>]</td><td></td></tr><tr><td>Tensor2Tensor</td><td>Google Brain team</td><td>[<a href="https://github.com/tensorflow/tensor2tensor/blob/master/README.md" target="_blank" rel="noopener">Readme</a>]</td><td><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">[source code]</a></td><td></td></tr></tbody></table><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Li Hungyi. <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf" target="_blank" rel="noopener">“Self-Attention” </a> lecture PPT,2021.</p><p>[2] Ashish Vaswani, et al. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">“Attention is all you need.”</a> NIPS,2017.</p><p>[3] Alexander Rush. <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">“The Annotated Transformer”</a> 2018.</p><p>[4] <a href="http://xtf615.com/2020/07/05/transformers/" target="_blank" rel="noopener">http://xtf615.com/2020/07/05/transformers/</a></p><p>[5] <a href="https://arxiv.org/pdf/2106.04554.pdf" target="_blank" rel="noopener">A Survey of Transformers</a></p><p>[6] <a href="https://arxiv.org/abs/2009.06732" target="_blank" rel="noopener">Efficient Transformer A survey</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h2 id=&quot;回顾Attention&quot;&gt;&lt;a href=&quot;#回顾Attention&quot; class=&quot;headerlink&quot; title=&quot;回顾Attention&quot;&gt;&lt;/a&gt;回顾Attention&lt;/h2&gt;&lt;h3 id=&quot;Q、K与V&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://codingclaire.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://codingclaire.github.io/tags/Transformer/"/>
    
    <category term="attention" scheme="https://codingclaire.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>Attention Mechanism总结</title>
    <link href="https://codingclaire.github.io/2021/07/01/2021-07-01-attention/"/>
    <id>https://codingclaire.github.io/2021/07/01/2021-07-01-attention/</id>
    <published>2021-07-01T08:29:52.000Z</published>
    <updated>2021-07-09T16:19:02.890Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h2 id="Neural-Machine-Translation-NMT"><a href="#Neural-Machine-Translation-NMT" class="headerlink" title="Neural Machine Translation(NMT)"></a>Neural Machine Translation(NMT)</h2><p>Neural Machine Translation,简称NMT, 是指使用深度学习来完成机器翻译任务。NMT任务是一个端到端的学习任务，输入一个序列直接输出对应的目标序列。如图1所示，模型输入句子“ABC&lt;EOS&gt;”, 输出对应的翻译句子“WXYZ&lt;EOS&gt;”，该图可以分为两个部分：编码（encode）和解码（decode）。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/2.png" width="90%" height="90%"></div><center>图1 （图片来源：Sutskever et al.2014）</center><p>左侧encoder输入”ABC&lt;EOS&gt;“，右侧decoder逐个预测出翻译的单词。decoder并不能直接确定预测单词的个数（句子的长度），预测的结束以通过预测到&lt;EOS&gt;结束，当decoder输入Z时，模型预测出&lt;EOS&gt;字符（end-of-sentence)，表示预测结束。</p><h2 id="Seq2seq模型"><a href="#Seq2seq模型" class="headerlink" title="Seq2seq模型"></a>Seq2seq模型</h2><p>整个框架也被称为<strong>Sequence-to-sequence模型（简称为Seq2seq）</strong>，是一种使用RNN结构来解决NLP相关的问题的模型。最早由 (<a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" target="_blank" rel="noopener">Sutskever et al.,2014</a>)提出，该方法是基于encoder-decoder框架下使用LSTM对文本进行翻译。encoder-decoder框架是sequence-to-sequence任务中的一个标准的模型，如图2。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/1.png" width="50%" height="50%"></div><center>图2 encoder-decoder框架</center><p>encoder和decoder通常都是RNN单元，如LSTM模型或GRU模型。encoder读入原始的序列，生成整个输入序列的representation（即上下文向量context vector），其中encoder的输出会被丢弃，只有隐藏状态（hidden state）会被保留。</p><p>decoder使用encoder输出的representation，生成目标序列。decoder的初始状态由encoder的最后一个LSTM单元的最终状态进行初始化。每一个LSTM单元都会接受上一个单元的隐藏状态，并生成自己的隐藏状态作为输出。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/5.png" width="90%" height="90%"></div><center>图3 Seq2seq模型原理图</center><h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h3><p>如图3，在编码阶段，模型输入语句 $x=(x_1,…x_{T_x})$ ，使用RNN计算上下文向量（context vector），其中t时刻的隐含状态计算公式如下：<br>$$<br>h_t=f(x_t,h_{t-1})<br>$$<br>通过encoder，能够用隐含状态计算出输入语句的embedding，也被称为上下文向量（context vector)，记作$C$：<br>$$<br>C=q({h_1,…h_{T_x}})<br>$$<br>其中$f$和$q$是两个非线性函数， (<a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" target="_blank" rel="noopener">Sutskever et al.,2014</a>)使用LSTM作为$f$, $q({h_1,…h_T })= h_T$作为$q$，即将最后一个隐藏状态$h_T$作为上下文向量。</p><h3 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h3><p>如图3，在解码阶段，模型逐个预测单词。当预测单词$y_{t^{‘}}$时，给定的上下文向量$C$和所有之前预测的单词${y_1,…y_{t^{‘}-1}}$将会用于预测。decoder生成的目标序列$y=(y_1,…y_{T_y})$，条件概率为：<br>$$<br>p(y)=\prod_{t=1}^{T}p(y_t│{y_1,…y_{t-1}},C)<br>$$</p><p>最终decoder确定最终预测的单词是概率向量中概率最大的词。<br>$$<br>y_{t^{‘}}=arg\mathop{max}\limits_{y}p(y|x)=arg\mathop{max}\limits_{y}\prod_{t=1}^{T}p(y_t│y_{&lt;t},x)<br>$$<br>当decoder使用RNN，则条件概率可以被表示为$p(y_t│y_1,…y_{t-1},C)=g(y_{t-1},s_t,C)$，其中$g$也是一个非线性的多层函数。decoder的第$t$个单元生成的隐含状态$s_t$和前一个单元的输出$y_{t-1}$作为输入，生成当前单元的输出$y_t$的概率向量。</p><blockquote><p>decoder还会使用”Teacher Forcing”提高训练的效率，此处不展开介绍，可以参考这篇<a href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" target="_blank" rel="noopener">文章</a>。</p></blockquote><h3 id="Seq2seq的问题"><a href="#Seq2seq的问题" class="headerlink" title="Seq2seq的问题"></a>Seq2seq的问题</h3><ol><li>Seq2seq模型中context vector是一个固定的向量，将输入的整句话编码成固定尺寸的向量，可能会损失句子的信息，限制模型的性能。</li><li>对RNN结构来说，序列长度越长，神经网络越深，这将导致梯度消失，结构的效果会有所下降。尽管LSTM可以一定程度上防止该问题，但仍然有可能出现长程梯度消失问题。</li></ol><h2 id="Attention-in-NMT"><a href="#Attention-in-NMT" class="headerlink" title="Attention in NMT"></a>Attention in NMT</h2><p>为了解决Seq2seq模型存在的问题，（<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al.,2015</a>）将attention思想最早由运用在NMT任务中。简单地说，注意力可以被解释为一个权重向量，这个向量能够被用来估计被预测的元素（如句子中的一个单词）和其他元素的相关性。元素通过注意力向量加权和会被作为目标的近似值。网络中的注意力部分会从输入序列中映射出重要的和相关性高的词，赋予这些词更高的权重，从而提高预测的准确性。（<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al.,2015</a>）论文整体的框架如图4所示，这种attention机制通常也被称为Additive Attention，它保证了encoder不需要将所有的原始语句都编码为一个固定长度的向量。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/4.png" width="45%" height="45%"></div><center>图4 模型框架（图片来源：Bahdanau et al.,2015）</center><h3 id="encoder：BiRNN"><a href="#encoder：BiRNN" class="headerlink" title="encoder：BiRNN"></a>encoder：BiRNN</h3><p>在encoder部分，使用的是一个双向的RNN（BiRNN），会生成一个前向的隐藏状态序列和后向的隐藏状态序列，将前向和后向对应的状态进行连接（concate），BiRNN使得每一个$h_j$都包含之前和之后单词的信息。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/9.png" width="40%" height="40%"></div><center>图5 双向RNN原理</center><p>Seq2seq模型中，encoder产生的最后一个状态将作为context vector，与Seq2seq不同的是，这里encoder产生的每一个状态都将在decoder用于计算context vector，且context vector成为了一个动态的表示。</p><h3 id="decoder：Additive-Attention"><a href="#decoder：Additive-Attention" class="headerlink" title="decoder：Additive Attention"></a>decoder：Additive Attention</h3><p>实际上，在Additive Attention中，context vector的计算需要利用encoder和decoder产生的状态，具体的流程如下图所示：</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/6.png" width="100%" height="100%"></div><center>图6 如何计算context vector</center><p>在decoder中，条件概率被定义为：<br>$$<br>p(y_t│y_1,…y_{t-1},x)=g(y_{i-1},s_i,c_i)<br>$$<br>$s_i$是decoder中RNN的第$i$隐藏状态，$s_i=f(s_{i-1},y_{i-1},c_i)$，如图6，$s_i$的计算需要：1）上一个隐藏状态$s_i$，2）上一个单元生成的$y_{i-1}$,3)由上一个隐藏状态计算出的当前的上下文向量$c_i$。这里和seq2seq模型的区别是，seq2seq仅会产生一个上下文向量$c$，但通过attention机制会生成多个上下文向量。每个上下文向量由encoder的隐藏状态的加权和计算得出：<br>$$<br>c_i=\Sigma_{j=1}^{T_x}\alpha_{ij}h_j<br>$$<br>这个值相当于计算所有的状态的期望。$\alpha_{ij}$表示$i$时刻第$j$个状态的权重，这个权重可以被理解成目标语言生成的$y_i$能够翻译源语言的单词$x_j$的概率，$\alpha_{ij}$也可以被称为attention score，计算公式为：<br>$$<br>\alpha_{ij}=\frac{exp⁡(e_{ij})}{\Sigma_{k=1}^{T_x}exp⁡(e_{ik})}<br>$$</p><p>这个式子说明$\alpha_{ij}$其实就是$e_{ij}$的softmax值，$e_{ij}$相当于一个分数，表示输入$j$位置和输出的$i$位置的匹配程度，计算公式为：<br>$$<br>e_{ij}=a(s_{i-1},h_j)=v_a^Ttanh(W_as_{i-1}+U_ah_j)<br>$$<br>$a$表示一个alignment model，通过feedforward神经网络进行训练，$W_a\in R^{n\times n}$，$U_a\in R^{n\times 2n}$和$v_a\in R^n$都是权重矩阵。$U_ah_j$可以被预先计算出来。</p><blockquote><p>关于alignment：</p><p>alignment在NMT中，指的就是不同种的语言单词语义上的相近，如机器学习翻译为machine learning，机器和machine在语义上是等价的，这就是一种alignment。alignment也可以理解为correspondence，以翻译文本为例，除了一对一的alignment，还可能会出现多对一、一对多的alignment。</p><p>soft alignment和hard alignment：（<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al.,2015</a>）使用的加权矩阵就可以被称为attention soft-alignment matrix,基本上可以把attention机制理解成soft alignment。</p></blockquote><p>下图表示翻译句子时的alignment scores matrix，灰度颜色越浅表示source word和target word的相关度越高（alignment score越大）。在这个图中能够看到在生成target word时，哪些source word更重要。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/result.png" width="50%" height="50%"></div><center>图7 alignment scores matrix（图片来源：Bahdanau et al.,2015）</center><h2 id="Q-query-、V-values-和K-keys"><a href="#Q-query-、V-values-和K-keys" class="headerlink" title="Q(query)、V(values)和K(keys)"></a>Q(query)、V(values)和K(keys)</h2><p>用QKV的视角来理解attention最早是在（<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">Vaswani et al.,2017</a>）中，在图6，标注了计算context vector过程中参与计算的Q、K、V矩阵。</p><p>values/keys: 把编码好的输入的表示看成key-value pairs$(K,V)$  ,长度为N, key和value皆是encoder的隐藏状态。</p><p>query：在decoder阶段，之前的输出会被压缩成一个query$(Q)$,长度为M。decoder的输出用于映射这个query和key-value pairs的集合。模型decoder会输出一个word的distribution, query就是最有可能的单词的表示向量。</p><h2 id="Attention-Family"><a href="#Attention-Family" class="headerlink" title="Attention Family"></a>Attention Family</h2><div align="center">    <img src="/2021/07/01/2021-07-01-attention/8.png" width="100%" height="100%"></div><center>图8 Attention Family</center><h3 id="soft-amp-hard-attention"><a href="#soft-amp-hard-attention" class="headerlink" title="soft&amp;hard attention"></a>soft&amp;hard attention</h3><p>attention机制在NLP领域被提出后，很快被应用到了CV领域(<a href="https://arxiv.org/pdf/1502.03044.pdf" target="_blank" rel="noopener">Xu et al. 2015</a>)。在这篇文章中，首次提出了soft attention和hard attention的概念。</p><p>soft attention指相对“柔和地”利用attention score来加权计算context vector，相当于求的是context vector的期望。（<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al.,2015</a>）中提出的模型就是soft attention。hard attention是将context vector看做随机变量，context vector的取值是利用参数为attention score的多项式分布在value中进行采样获得的。下图是(<a href="https://arxiv.org/pdf/1502.03044.pdf" target="_blank" rel="noopener">Xu et al. 2015</a>)中两种attention机制的实验中的attention矩阵的可视化，通过下图能够直观地理解soft attention和hard attention的区别。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/10.png" width="80%" height="80%"></div><center>图9 Attention的可视化（图片来源：Xu et al.,2015）</center><p>soft attention是differentiable的，但当输入较长的时候模型代价较大，hard attention在预测时需要的计算更少，但模型是non-differentiable的，在训练时需要更多的技巧。</p><h3 id="global-amp-local-attention"><a href="#global-amp-local-attention" class="headerlink" title="global&amp;local attention"></a>global&amp;local attention</h3><p>根据计算context vector时利用encoder的隐藏状态的多少，可以分为global attention和local attention（<a href="https://nlp.stanford.edu/pubs/emnlp15_attn.pdf" target="_blank" rel="noopener">Luong et al.,2015</a>）。</p><h4 id="global-attention"><a href="#global-attention" class="headerlink" title="global attention"></a>global attention</h4><p>global attention的原理和（<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al.,2015</a>）较为相似，其中不同的部分是：</p><p>1.在计算$score(s_i,h_j)$时，将$s_i$和$h_j$直接进行concat。</p><p>2.简化了计算过程：$h_t \rightarrow a_t \rightarrow c_t \rightarrow \widetilde h_t$ （Bahdanau et al.,2015的计算过程为$h_{t-1} \rightarrow a_t \rightarrow c_t \rightarrow h_t$ ）。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/global.png" width="40%" height="40%"></div><center>图10 global attention原理（图片来源：Luong et al.,2015）</center><h4 id="local-attention"><a href="#local-attention" class="headerlink" title="local attention"></a>local attention</h4><p>local attention的引入是为了解决global attention中，复杂度较高的问题。local attention会根据目标词的隐藏状态$h_t$计算出相应的对齐位置$p_t$，context vector的计算是通过$[p_t-D,p_t+D]$区间内的encoder中的隐含状态的加权和来进行计算的。D是根据经验选择的。<br>$$<br>p_t=S \times sigmoid(v_p^T tanh(W_ph_t))<br>$$<br>$S$是原序列的长度，以$p_t$为中心的高斯核函数进行衰减，aligned weights被定义为：<br>$$<br>a_t(s)=align(h_t,\overline h_s)exp(-\frac{(s-p_t)^2}{2\sigma^2})<br>$$<br>其中，$\sigma=\frac{D}{2}$。</p><div align="center">    <img src="/2021/07/01/2021-07-01-attention/local.png" width="40%" height="40%"></div><center>图11 local attention原理（图片来源：Luong et al.,2015）</center><p>当前对目标单词的预测值，没有利用已经预测的输出单词（encoder的隐藏状态）作为输入，也没有利用目标词位置前一时刻的decoder隐状态$h_{t−1}$，仅利用了当前decoder的隐藏状态$h_t$来计算。说明每个目标单词的决策是独立的。</p><h3 id="attention-score不同的计算方式"><a href="#attention-score不同的计算方式" class="headerlink" title="attention score不同的计算方式"></a>attention score不同的计算方式</h3><p>根据attention score计算方式的不同，有如下的attention机制：</p><ul><li>additive attention：（<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al.,2015</a>）</li><li>dot-product (multiplicative) attention：(<a href="https://nlp.stanford.edu/pubs/emnlp15_attn.pdf" target="_blank" rel="noopener">Luong et al.,2015</a>）</li><li>general attention (<a href="https://nlp.stanford.edu/pubs/emnlp15_attn.pdf" target="_blank" rel="noopener">Luong et al.,2015</a>）</li><li>concat attention (<a href="https://nlp.stanford.edu/pubs/emnlp15_attn.pdf" target="_blank" rel="noopener">Luong et al.,2015</a>）</li><li>location attention (<a href="https://nlp.stanford.edu/pubs/emnlp15_attn.pdf" target="_blank" rel="noopener">Luong et al.,2015</a>）</li><li>scaled Dot-Product Attention：（<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">Vaswani et al.,2017</a>）</li></ul><p>不同的attention score具体计算方式图8所示。</p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>自注意力机制和普通的注意力机制的区别在于，自注意力机制考虑的是输入元素（source word）之间的相关性，而非输入元素和输出元素之间的相关性（source word和target word），因此，自注意力机制能够使用上述任意一种attention score的计算方式，只是其目标序列和输入的原始序列相同。我们可以把它理解成一个全连接层，权重是由输入的成对关系动态生成的。</p><p>关于self-attention具体的原理和Query、Value和Keys的进一步解释，将在后续介绍Transformer的文章中展开介绍。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. <a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" target="_blank" rel="noopener">“Sequence to sequence learning with neural networks.”</a> <em>Advances in neural information processing systems</em>. 2014.</p><p>[2] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">“Neural machine translation by jointly learning to align and translate.”</a> ICLR. 2015.</p><p>[3] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. <a href="https://nlp.stanford.edu/pubs/emnlp15_attn.pdf" target="_blank" rel="noopener">“Effective Approaches to Attention-based Neural Machine Translation.”</a> EMNLP. 2015.</p><p>[4] Yang, Shuoheng, Yuxin Wang, and Xiaowen Chu. <a href="https://arxiv.org/pdf/2002.07526.pdf" target="_blank" rel="noopener">“A survey of deep learning techniques for neural machine translation.”</a> <em>arXiv preprint arXiv:2002.07526</em> (2020).</p><p>[5]Xu, Kelvin, et al. <a href="https://arxiv.org/pdf/1502.03044.pdf" target="_blank" rel="noopener">“Show, attend and tell: Neural image caption generation with visual attention.”</a> <em>International conference on machine learning</em>. PMLR, 2015.</p><p>[6] Ashish Vaswani, et al. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">“Attention is all you need.”</a> NIPS,2017.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h2 id=&quot;Neural-Machine-Translation-NMT&quot;&gt;&lt;a href=&quot;#Neural-Machine-Translation-NMT&quot; class=&quot;headerlink&quot; title=&quot;Neural Machine Tran</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://codingclaire.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://codingclaire.github.io/tags/Transformer/"/>
    
    <category term="attention" scheme="https://codingclaire.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>【异常检测5】基于集成学习的异常检测</title>
    <link href="https://codingclaire.github.io/2021/05/23/2021-05-23-anomaly-detection-5/"/>
    <id>https://codingclaire.github.io/2021/05/23/2021-05-23-anomaly-detection-5/</id>
    <published>2021-05-23T04:33:16.000Z</published>
    <updated>2021-07-01T13:58:54.363Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%BA%94%E3%80%81%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95.md" target="_blank" rel="noopener">DataWhale组队学习活动</a></p></blockquote><blockquote><p>集成学习主要分为三种方法：</p><p>1.Boosting（提升）：主要包括两种方法AdaBoost和Gradient Boost，其中Gradient Boost方法的代表是梯度提升决策树（GDBT)</p><p>2.Bagging: 主要方法有随机森林</p><p>3.Stacking</p></blockquote><p>通过集成学习来进行异常检测的方法主要有两种，分别是特征Feature Bagging和孤立森林（Isolation Forest)。</p><h2 id="Feature-Bagging"><a href="#Feature-Bagging" class="headerlink" title="Feature Bagging"></a>Feature Bagging</h2><p>Feature Bagging将Bagging的思想应用在特征上。它结合了多个异常检测算法的结果，每个异常检测算法使用的特征都是从原始特征集合中随机选取的特征子集合。每种异常检测的方法会检测出不同的异常点，然后通过异常点分值来对结果进行合并。</p><h3 id="基础框架"><a href="#基础框架" class="headerlink" title="基础框架"></a>基础框架</h3><p><img src="/2021/05/23/2021-05-23-anomaly-detection-5/image-20210523133318831.png" alt="image-20210523133318831"></p><p>上述为这种方法的基本框架，每个异常检测的算法会选取每所有样本的d/2-d个特征，d表示原始的特征数，输出不同的分数向量$AS_t(j)$，表示第$t$个方法中，数据集中数据$j$是异常点的概率。由于总共有$T$个方法，那么会有$T$个异常分数向量，最后使用COMBINE函数对向量进行合并，最后生成一个$AS_{FINAL}$向量，表示数据点是异常点的最终的概率。</p><h3 id="COMBINE方法"><a href="#COMBINE方法" class="headerlink" title="COMBINE方法"></a>COMBINE方法</h3><h4 id="1-Breadth-First-广度优先"><a href="#1-Breadth-First-广度优先" class="headerlink" title="1.Breadth-First 广度优先"></a>1.Breadth-First 广度优先</h4><p><img src="/2021/05/23/2021-05-23-anomaly-detection-5/image-20210523133448116.png" alt="image-20210523133448116"></p><h4 id="2-Cumulative-Sum-累积求和"><a href="#2-Cumulative-Sum-累积求和" class="headerlink" title="2.Cumulative Sum 累积求和"></a>2.Cumulative Sum 累积求和</h4><p><img src="/2021/05/23/2021-05-23-anomaly-detection-5/image-20210523133521315.png" alt="image-20210523133521315"></p><h3 id="优劣"><a href="#优劣" class="headerlink" title="优劣"></a>优劣</h3><p>优势：</p><p>feature bagging能够降低方差（bagging方法使用有放回抽样，数据集间会有重复的样本，每个模型之间具有相关关系，设相关系数为$\rho$，模型均值的方差可以被表示为：</p><p><img src="/2021/05/23/2021-05-23-anomaly-detection-5/image-20210524003416272.png" alt="image-20210524003416272"></p><p>当n增大时，模型整体的方差会趋向于$\rho\sigma^2$，模型方差会降低。</p><h2 id="孤立森林（Isolation-Forests）"><a href="#孤立森林（Isolation-Forests）" class="headerlink" title="孤立森林（Isolation Forests）"></a>孤立森林（Isolation Forests）</h2><h3 id="整体思想"><a href="#整体思想" class="headerlink" title="整体思想"></a>整体思想</h3><p>假设我们用一个随机超平面来切割数据空间，切一次可以生成两个子空间。然后我们继续用随机超平面来切割每个子空间并循环，直到每个子空间只有一个数据点为止。直观上来讲，那些具有高密度的簇需要被切很多次才会将其分离，而那些低密度的点很快就被单独分配到一个子空间了。孤立森林认为这些很快被孤立的点就是异常点。</p><p>孤立森林使用集成方法得到收敛值，将多种切割的方法进行平均，使得结果更为可靠。</p><h3 id="孤立树的生成"><a href="#孤立树的生成" class="headerlink" title="孤立树的生成"></a>孤立树的生成</h3><p>孤立森林是由t棵孤立的树构成，每个树是随机二叉树，对异常点来说，它会很快地被划分到叶子节点，因此叶子节点到根节点的路径越短，数据可能越异常。在这个过程中，不需要知道样本的标签，可以直接通过孤立森林构造树的过程来判断样本是否异常，所以孤立森林的方法是无监督的。树的构造方法如下：</p><blockquote><p>1)从训练数据中随机选择一个样本子集，放入树的根节点；</p><p>2)随机指定一个属性，随机产生一个切割点V，即属性A的最大值和最小值之间的某个数；</p><p>3)根据属性A对每个样本分类，把A小于V的样本放在当前节点的左孩子中，大于等于V的样本放在右孩子中，这样就形成了2个子空间；</p><p>4) 在孩子节点中递归步骤2和3，不断地构造左孩子和右孩子，直到孩子节点中只有一个数据，或树的高度达到了限定高度。</p></blockquote><p>孤立森林的不同的分支对应于数据的不同局部子空间区域，较小的路径对应于孤立子空间的低维，因此这也是一种基于子空间的方法。</p><h3 id="路径长度计算"><a href="#路径长度计算" class="headerlink" title="路径长度计算"></a>路径长度计算</h3><p><img src="https://gitee.com/datawhalechina/team-learning-data-mining/raw/master/AnomalyDetection/img/image-20210103183909407.png" alt="img"></p><h3 id="优劣-1"><a href="#优劣-1" class="headerlink" title="优劣"></a>优劣</h3><p>优势：</p><ul><li>计算成本相比基于距离或基于密度的算法更小。</li><li>具有线性的时间复杂度。</li><li>在处理大数据集上有优势。</li></ul><p>劣势：</p><ul><li>不适用于超高维数据，每次随机选取维度，如果维度过高，则会存在过多噪音。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%BA%94%E3%80%81%E9%9B%86</summary>
      
    
    
    
    
    <category term="异常检测" scheme="https://codingclaire.github.io/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
    <category term="集成学习" scheme="https://codingclaire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>【异常检测4】基于相似度的方法</title>
    <link href="https://codingclaire.github.io/2021/05/20/2021-05-20-anomaly-detection-4/"/>
    <id>https://codingclaire.github.io/2021/05/20/2021-05-20-anomaly-detection-4/</id>
    <published>2021-05-20T15:31:53.000Z</published>
    <updated>2021-05-23T05:30:35.918Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E5%9B%9B%E3%80%81%E5%9F%BA%E4%BA%8E%E9%82%BB%E8%BF%91%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95.md" target="_blank" rel="noopener">DataWhale组队学习活动</a></p></blockquote><p>基于相似度的异常检测方法中，主要考虑异常点和正常点的相似度。相似度的度量有两种方式：基于距离的和基于密度的。</p><h2 id="基于距离的度量"><a href="#基于距离的度量" class="headerlink" title="基于距离的度量"></a>基于距离的度量</h2><p>基于距离的度量<strong>通过最近邻距离来定义异常值</strong>，当某个点的k邻近距离远大于正常点时，它被定义为异常点。k邻近距离指的是在空间中离该点最近的k个邻居的平均距离。那么通常查找某个点的k邻近距离可以使用循环嵌套的方法。首先循环遍历每个数据，然后进行异常判断，计算当前点与其他点的距离，如果当前节点中有k个数据点与它的距离小于D，那么这个点就是非异常的。这种方法的时间复杂度为$O(N^{2})$，数据量较大时，可以使用修剪方法加快计算。<br>以下是两种修剪的方法：</p><h3 id="1-基于单元的方法"><a href="#1-基于单元的方法" class="headerlink" title="1.基于单元的方法"></a>1.基于单元的方法</h3><p>数据空间被划分为单元格，单元格的宽度是阈值D和数据维数的函数。</p><h3 id="2-基于索引的方法"><a href="#2-基于索引的方法" class="headerlink" title="2.基于索引的方法"></a>2.基于索引的方法</h3><p>利用多维索引结构(如 $\mathrm{R}$ 树、$k-d$ 树)来搜索每个数据对象 $A$ 在半径 $D$ 范围 内的相邻点。</p><h2 id="基于密度的度量"><a href="#基于密度的度量" class="headerlink" title="基于密度的度量"></a>基于密度的度量</h2><p>基于密度的算法主要有局部离群因子(LocalOutlierFactor,LOF)，以及LOCI、CLOF等基于LOF的改进算法。</p><h3 id="1-k-距离（k-distance-p-）"><a href="#1-k-距离（k-distance-p-）" class="headerlink" title="1.k-距离（k-distance(p)）"></a>1.k-距离（k-distance(p)）</h3><p>p点的k-距离就是p距离数据集的每一个点的距离中第k近的距离，就是k-距离。</p><h3 id="2-k-邻域（k-distance-neighborhood）"><a href="#2-k-邻域（k-distance-neighborhood）" class="headerlink" title="2.k-邻域（k-distance neighborhood）"></a>2.k-邻域（k-distance neighborhood）</h3><p>k-距离引申出k-邻域，k-邻域是一个集合，这个集合包含所有到点p的距离小于等于k-距离的所有点。</p><h3 id="3-可达距离（reachability-distance）"><a href="#3-可达距离（reachability-distance）" class="headerlink" title="3.可达距离（reachability distance）"></a>3.可达距离（reachability distance）</h3><p>给定点p关于o的可达距离的计算取决于p是否在o的k-邻域内。如果p在o的k-邻域内，那么可达距离就是o的k-距离，如果不在k-邻域内，那么科大距离就是p和o的实际距离。</p><blockquote><p>可达距离可以减少距离的计算开销，用一个阈值把需要计算的部分截断了，$k$的值越高，无需计算的邻近点越多，计算开销越小。但是另一方面，$k$的值变高，可能意味着可达距离变远，对集群点和离群点的区分度可能变低。因此，如何选择$k$值，是LOF算法能否达到效率与效果平衡的重要因素。</p></blockquote><h3 id="4-局部可达密度（local-reachability-density）"><a href="#4-局部可达密度（local-reachability-density）" class="headerlink" title="4.局部可达密度（local reachability density）"></a>4.局部可达密度（local reachability density）</h3><p>给定点p关于o的局部可达密度是p到o的k-邻域内所有点的可达距离平均值的导数。<br>计算时需要避免数据集内所有数据落在同一点上，此时可达距离之和为0，局部密度就是∞。</p><h3 id="5-局部异常因子-LOF"><a href="#5-局部异常因子-LOF" class="headerlink" title="5.局部异常因子(LOF)"></a>5.局部异常因子(LOF)</h3><p>局部异常银子是通过每个点的局部可达密度和它们的k个邻点的局部可达密度进行比较，得到LOF。<br>LOF是对象p的邻居点o的局部可达密度的平均值与p的局部可达密度的比值。<br>LOF数值就是离群点分数。</p><p>数据的LOF值越高，通常会有更稀疏的邻居，更可能是异常点。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E5%9B%9B%E3%80%81%E5%9F%BA</summary>
      
    
    
    
    
    <category term="异常检测" scheme="https://codingclaire.github.io/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【异常检测3】线性模型（线性回归、主成分分析）</title>
    <link href="https://codingclaire.github.io/2021/05/17/2021-05-17-anomaly-detection-3/"/>
    <id>https://codingclaire.github.io/2021/05/17/2021-05-17-anomaly-detection-3/</id>
    <published>2021-05-17T01:53:05.000Z</published>
    <updated>2021-07-03T07:42:25.098Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%B8%89%E3%80%81%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.md" target="_blank" rel="noopener">DataWhale组队学习活动</a></p></blockquote><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>回归问题在某种程度上等价于函数的拟合，即选择一条函数曲线，使其能很好地拟合已知数据，并较好地预测未知数据。回归问题的目的是求解数据的相关性。解决回归问题主要可以分为两类，一类是线性回归，通过其他变量预测某一个属性值；另一类是主成分分析，是通过潜在变量来代表数据。</p><p>注意回归问题的求解有两个假设：</p><p>1.近似线性相关假设。</p><p>2.子空间假设。</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="什么是线性回归？"><a href="#什么是线性回归？" class="headerlink" title="什么是线性回归？"></a>什么是线性回归？</h3><p>线性回归通过线性方程组来建模不同维度的向量之间的关系。由于样本数通常远大于数据维度，无法直接求解线性方程组，因此通过优化最小化模型预测值和真实数据的误差。</p><p>线性回归一般研究的是自变量对因变量的影响，相当于找到自变量和因变量的关系，可以用于解决回归、分类或预测等问题。线性回归根据自变量的个数可以分为一元线性回归和多元线性回归。一元线性回归指自变量只有一个，研究的是单变量对因变量的影响。多元线性回归则是研究多个自变量对因变量的影响。</p><h3 id="线性回归中的异常检测"><a href="#线性回归中的异常检测" class="headerlink" title="线性回归中的异常检测"></a>线性回归中的异常检测</h3><p>在线性回归中的异常值主要指的是在特定模型的基础上偏离预测值的值，而非考虑样本特征（自变量）之间的关系。<strong>在线性方法中，异常检测的目标是找到低维子空间，其中离群点的行为与其他数据点非常不同。</strong>由于异常点会对模型性能产生影响，因此异常检测是为了用于数据降噪，防止异常点对模型产生影响。</p><h3 id="有哪些线性回归的方法？"><a href="#有哪些线性回归的方法？" class="headerlink" title="有哪些线性回归的方法？"></a>有哪些线性回归的方法？</h3><h4 id="基于自变量与因变量的线性回归"><a href="#基于自变量与因变量的线性回归" class="headerlink" title="基于自变量与因变量的线性回归"></a>基于自变量与因变量的线性回归</h4><h5 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h5><p>以一个多元线性回归为例，最小二乘法的原理如下：</p><p>$X_{1}…X_{d}$为一系列因变量，也就是输入值，变量$Y$为因变量，也就是我们要预测的值，线性回归的方法可以将$Y$表示为：</p><p>$$<br>Y=\sum_{i=1}^{d} a_{i} \cdot X_{i}+a_{d+1} \tag{1}<br>$$</p><p>其中系数$a_{1}…a_{d+1}$是可以被学习到的参数。假设数据共包含$N$个样本，第$j$个样本包含的数据为$x_{j1}…x_{jd}$和$y_{j}$，带入式如下式所示：</p><p>$$<br>y_{j}=\sum_{i=1}^{d} a_{i} \cdot x_{j i}+a_{d+1}+\epsilon_{j} \tag{2}<br>$$<br>这里$\epsilon_{j}$为第$j$个样本的误差。以$Y$ 代表 $N \times 1$ 的因变量矩阵${(y_{1}…y_{N})}^{T}$，即样本中的真实值；以$U$代表$N \times (d+1)$的自变量矩阵，其中第$j$行为$(x_{j1}…x_{jd}, 1)$；以$A$ 代表 $(d+1) \times 1$ 的系数矩阵$(a_{1}…a_{d+1})^{T}$。则模型可表示为：<br>$$<br>f(U, A) = U \cdot A \tag{3}<br>$$</p><p>对A求导，可得：<br>$$<br>\frac{\partial L(A)}{\partial A} = \frac{1}{2}\times\frac{\partial{|Y - U \cdot A|}^2}{\partial A}= - {U^T}(Y - U \cdot A) \tag{4}<br>$$<br>令$\frac{\partial L(A)}{\partial A}=0$，得到最优参数为：</p><p>定义目标函数为：<br>$$<br>L(A) = \frac{1}{2}{\left| {Y - U \cdot A} \right|^2} \tag{5}<br>$$<br>目标函数是关于$A$的凸函数，其对$A$求偏导为：</p><p>$$<br>A=\left(U^{T} \cdot U\right)^{-1} \cdot\left(U^{T} \cdot Y\right) \tag{6}<br>$$</p><h5 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h5><p>优化目标是最小化损失函数，深度学习是利用随机梯度下降有限次地迭代模型参数，尽可能降低损失函数的值，相当于求出了<strong>数值解</strong>。但线性回归和平方误差是能够直接求出误差的最小化，因此是求出了<strong>解析解</strong>。</p><h4 id="基于异常检测的线性回归"><a href="#基于异常检测的线性回归" class="headerlink" title="基于异常检测的线性回归"></a>基于异常检测的线性回归</h4><p>一组变量 $X_{1}… X_{d}$， 对应的回归平面如下：</p><p>$$<br>a_{1} \cdot X_{1}+\ldots+a_{d} \cdot X_{d}+a_{d+1}=0 \tag{7}<br>$$<br>为了后续计算的方便，对参数进行如下约束：<br>$$<br>\sum\limits_{i = 1}^d {a_i^2 = 1} \tag{8}<br>$$<br>以$L_{2}$范数作为目标函数：<br>$$<br>L = {\left| {U \cdot A} \right|_2} \tag{9}<br>$$<br>这种方式是以相似的方式对待所有的变量，通过最小化数据对平面的投影误差来确定最佳回归平面。</p><h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><h3 id="什么是主成分分析？"><a href="#什么是主成分分析？" class="headerlink" title="什么是主成分分析？"></a>什么是主成分分析？</h3><p>主成分分析能够在上述的方法上通过找到一个比原始维数更低的维度表示的最优超平面，它相当于一种降维的操作。</p><h3 id="主成分分析的原理"><a href="#主成分分析的原理" class="headerlink" title="主成分分析的原理"></a>主成分分析的原理</h3><p>对于 $d$ 维，包含 $N$ 个样本的数据，用 $R_{i}$ 表示其中第 $i$ 行为：$[x_{i1}… x_{id}]$。由此可以得到 $d \times d$ 的协方差矩阵$Σ$ （标准的PCA应当计算相关系数矩阵，即对数据进行均值为0方差为1的标准化处理，而协方差矩阵只需要减去均值即可）:</p><p>$$<br>Σ = (R - \bar{R})^{T} \cdot (R - \bar{R}) \tag{10}<br>$$<br>协方差矩阵是对称并且半正定的，因此可以进行相似对角化：</p><p>$$<br>Σ = P \cdot D \cdot P^{T} \tag{11}<br>$$<br>对角化后，$D$是特征值组成的对角矩阵，$P$ 是标准正交矩阵，每一行为对应的特征向量；这些标准正交向量提供了数据应该投影的轴线方向。</p><p>获取特征值和特征向量后，可以将原始的数据转换到新的坐标系中。$Y_{1}…Y_{k}$ 表示新坐标系中的数据，这些数据可以通过原始向量 $R_{i}$ 与包含新轴系的标准正交特征向量矩阵 $P$ 的乘积来实现。<br>$$<br>{Y_i} = {R_i} \cdot P \tag{12}<br>$$<br>通常高维数据的很多特征值接近0，相当于他们对原始数据特征向量的贡献不高，可以被忽略，当然这些较小的特征值也可以被看成异常。具体与异常检测相关的性质如下所示。</p><h3 id="异常检测相关性质"><a href="#异常检测相关性质" class="headerlink" title="异常检测相关性质"></a>异常检测相关性质</h3><ol><li>最大的前k个特征值的特征向量定义的k维超平面是所有维度为k的超平面中数据点到它的均方距离尽可能小的平面。</li><li>如果将数据转换为与正交特征向量对应的轴系，则转换后的数据沿每个特征向量维的方差等于相应的特征值。在这种新表示中，转换后的数据的协方差为0。（？）</li><li>由于沿特征值小的特征向量的转换数据的方差很低（没有更好地保持在原空间上的关系），因此沿这些方向的变换数据与平均值的显着偏差可能表示离群值。</li></ol><h3 id="主成分分析中的异常检测"><a href="#主成分分析中的异常检测" class="headerlink" title="主成分分析中的异常检测"></a>主成分分析中的异常检测</h3><p>对于特征值较小（方差较小）的特征向量 $j$，第 $i$ 条记录的 $y_{ij}$ 与 $y_{kj}$ 的其他值的偏差较大，说明有离群行为。这是因为当$j$固定而$k$变化时，$y_{kj}$ 的值应当变化不大。因此，$y_{ij}$ 值是不常见的。</p><p>使用特征值来计算数据点沿每个主分量方向到质心的归一化距离。设$e_{j}$为第$j$个特征向量，$λ_j$为沿该方向的方差(特征值)。数据点$\bar{X}$相对于对数据质心$\bar{\mu}$的总体归一化异常得分可以由下式给出：</p><p>$$<br>S \operatorname{core}(\bar{X})=\sum_{j=1}^{d} \frac{|(\bar{X}-\bar{\mu}) \cdot \bar{e_j}|^{2}}{\lambda_j} \tag{13}<br>$$</p><blockquote><p>注意在使用PCA时，需要对数据进行归一化操作，进行均值为0方差为1的标准化处理。这隐含地导致在主成分分析中使用相关矩阵而不是协方差矩阵。</p></blockquote><h2 id="回归分析的优劣"><a href="#回归分析的优劣" class="headerlink" title="回归分析的优劣"></a>回归分析的优劣</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>1.线性建模提供一种有效的工具来从底层数据中移除异常值或者进行异常检测。</p><p>2.主成分分析提供了去除异常值和进行异常检测最有效的方法，因为它对存在少数异常值的数据更有鲁棒性。</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1.数据不相关，但在某些区域高度聚集时，这种方法可能不会有效。</p><p>2.数据中相关性本质上可能不是全局的，主成分分析发现的全局子空间对异常检测可能是次优的，可能需要将线性模型和临近模型进行结合。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%B8%89%E3%80%81%E7%BA%BF</summary>
      
    
    
    
    
    <category term="异常检测" scheme="https://codingclaire.github.io/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
    <category term="回归" scheme="https://codingclaire.github.io/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>【异常检测2】基于统计学的异常检测方法</title>
    <link href="https://codingclaire.github.io/2021/05/14/2021-05-14-anomaly-detection-2/"/>
    <id>https://codingclaire.github.io/2021/05/14/2021-05-14-anomaly-detection-2/</id>
    <published>2021-05-14T15:13:17.000Z</published>
    <updated>2021-05-18T01:49:33.336Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%B8%80%E3%80%81%E6%A6%82%E8%BF%B0.md" target="_blank" rel="noopener">DataWhale组队学习活动</a></p></blockquote><p>基于统计学来进行异常检测的方法就是学习一个拟合数据集的生成模型，认为模型中低概率区域的对象就是识别出的异常点。</p><p>异常检测的方法可以分为参数方法和非参数方法。它们最大的区别是，参数方法的假定了先验的统计模型，然后运用概率密度公式来计算分布产生概率，非参数方法没有假定先验的统计模型，相当于是运用输入数据来确定模型，这里的参数不是模型训练的参数，而是指先验分布的参数。</p><h2 id="参数方法"><a href="#参数方法" class="headerlink" title="参数方法"></a>参数方法</h2><p>主要思路就是先确定一个分布，然后再通过输入数据学习分布的参数，低概率的点就被识别为异常点。如可以选择正态分布进行求解。</p><p>在异常点中分为一元异常点和多元异常点，区别是多元异常点会有多个特征。这时候在维度为1的数据中的分布需要被扩充到更高维度上，如果各个维度，也就是说数据的每一个特征都相互独立，那么就可以直接利用一元异常点的异常检测进行扩充，但是如果特征之间有相关关系且符合多元高斯分布，那么就可以求相应参数。如果实际数据复杂，还可以采用混合的参数分布。</p><h2 id="非参数方法"><a href="#非参数方法" class="headerlink" title="非参数方法"></a>非参数方法</h2><h3 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h3><p>利用数据构造直方图，异常点如果在直方图中就是正常的，否则是异常的。模型容易受到超参数影响如直方图箱数或箱尺寸。当箱尺寸太小，可能正常对象会被误识别为异常点，当箱尺寸太大，异常节点可能会被判定为正确的，造成错误。</p><h3 id="角度"><a href="#角度" class="headerlink" title="角度"></a>角度</h3><p>基于角度的异常节点检测主要的思想在于如果某一个数据点距离其余点越远，那么潜在角度可能越小，该点越有可能是异常点。使用加权的余弦可计算出异常的分数。</p><h3 id="HBOS"><a href="#HBOS" class="headerlink" title="HBOS"></a>HBOS</h3><p>该方法全称Histogram-based Outlier Score，是将单变量方法组合，适用于互相独立的特征，然后对每个维度进行区间划分，<strong>区间密度越高，异常评分越低。</strong>计算方法是对每个维度做出数据直方图（静态宽度直方图、动态宽度直方图），用箱子高度表示密度估计，归一化处理计算出HBOS值。</p><p>$ H B O S(p)=\sum_{i=0}^{d} \log \left(\frac{1}{\text {hist}_{i}(p)}\right) $$</p><p>这一方法的缺点是不能检测局部异常值。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%B8%80%E3%80%81%E6%A6%82</summary>
      
    
    
    
    
    <category term="异常检测" scheme="https://codingclaire.github.io/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【异常检测1】基本概念</title>
    <link href="https://codingclaire.github.io/2021/05/11/2021-05-11-anomaly-detection-1/"/>
    <id>https://codingclaire.github.io/2021/05/11/2021-05-11-anomaly-detection-1/</id>
    <published>2021-05-11T11:46:14.000Z</published>
    <updated>2021-05-12T02:14:11.065Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%B8%80%E3%80%81%E6%A6%82%E8%BF%B0.md" target="_blank" rel="noopener">DataWhale组队学习活动</a></p></blockquote><h2 id="什么是异常检测"><a href="#什么是异常检测" class="headerlink" title="什么是异常检测?"></a>什么是异常检测?</h2><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p>在统计学中，离群值（Outliers）是与其他观测值显著不同的数据点，也成为异常点。异常点的出现可能是因为观测的可变性或实验的错误。异常是指在大量的数据中较为稀少的数据点、事件或者行为。异常通常与正常数据不同，通常代表着数据中出现的一些问题，如欺诈行为、网络、文字的错误等。异常也被成为噪音、偏差和异常。异常可以被分为三类：点异常、条件异常和群体异常。</p><h3 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h3><p>异常检测是在数据集中找到非正常的数据、条件或群体。找到异常节点面临着一些挑战，如用于异常检测的数据集有样本类别不均衡的问题，还有异常节点是不规则的，不同的异常可能表现上完全不同。</p><h2 id="异常检测的方法有哪些？"><a href="#异常检测的方法有哪些？" class="headerlink" title="异常检测的方法有哪些？"></a>异常检测的方法有哪些？</h2><h3 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h3><p>有监督学习方法适用于有标签的数据集，也就是说在训练过程中会知道数据是否是异常的，这就相当于一个分类问题。可以用很多基础的机器学习分类算法进行检测，如SVM，决策树、GBDT、XGBoost等进行分类。但值得注意的是，包含异常的数据集是不均衡的，因此可能会影响性能。可以考虑使用集成学习的方法，如feature bagging。</p><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>无监督学习方法适用于没有标签的训练集，可以对数据集进行聚类。用于聚类的方法可以运用在异常检测的场景中，如DBSCAN算法、KNN算法、LOF(local outlier factor)算法等。但无监督学习聚类的方法，有时候会面临维度灾难，可能相似性的度量在高维数据重失效。</p><h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><p>半监督学习的异常检测指的是在训练集中只有正常的数据集，没有异常的实例参与训练。然后会构造出一个表示正常行为的模型，然后会测试该模型生成的实例的可能性。比较适用于数据的标签不足的时候。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%B8%80%E3%80%81%E6%A6%82</summary>
      
    
    
    
    
    <category term="异常检测" scheme="https://codingclaire.github.io/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>如何读取HDF5保存的权重</title>
    <link href="https://codingclaire.github.io/2021/04/20/2021-04-20-hdf5-weights/"/>
    <id>https://codingclaire.github.io/2021/04/20/2021-04-20-hdf5-weights/</id>
    <published>2021-04-20T11:23:37.000Z</published>
    <updated>2021-05-18T01:38:28.771Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://keras.io/api/layers/base_layer/#set_weights-method" target="_blank" rel="noopener">https://keras.io/api/layers/base_layer/#set_weights-method</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(<span class="string">"encoder_weights_0.hdf5"</span>, <span class="string">"r"</span>)</span><br><span class="line">print(f.filename, <span class="string">":"</span>)</span><br><span class="line">print(f[<span class="string">'dense_1'</span>])</span><br><span class="line">print([key <span class="keyword">for</span> key <span class="keyword">in</span> f.keys()], <span class="string">"\n"</span>)</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> f.keys():</span><br><span class="line">    print(key,f[key])</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> f[key].keys():</span><br><span class="line">        print(k,f[key][k])</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> f[key][k].keys():</span><br><span class="line">            print(l, f[key][k][l])</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;HDF5 group "/dense_1" (1 members)&gt;</span><br><span class="line">['dense_1', 'dense_2', 'dense_3', 'input_1']</span><br><span class="line"></span><br><span class="line">dense_1 &lt;HDF5 group "/dense_1" (1 members)&gt;</span><br><span class="line">dense_1 &lt;HDF5 group "/dense_1/dense_1" (2 members)&gt;</span><br><span class="line">bias:0 &lt;HDF5 dataset "bias:0": shape (64,), type "&lt;f4"&gt;</span><br><span class="line">kernel:0 &lt;HDF5 dataset "kernel:0": shape (100, 64), type "&lt;f4"&gt;</span><br><span class="line">dense_2 &lt;HDF5 group "/dense_2" (1 members)&gt;</span><br><span class="line">dense_2 &lt;HDF5 group "/dense_2/dense_2" (2 members)&gt;</span><br><span class="line">bias:0 &lt;HDF5 dataset "bias:0": shape (16,), type "&lt;f4"&gt;</span><br><span class="line">kernel:0 &lt;HDF5 dataset "kernel:0": shape (64, 16), type "&lt;f4"&gt;</span><br><span class="line">dense_3 &lt;HDF5 group "/dense_3" (1 members)&gt;</span><br><span class="line">dense_3 &lt;HDF5 group "/dense_3/dense_3" (2 members)&gt;</span><br><span class="line">bias:0 &lt;HDF5 dataset "bias:0": shape (8,), type "&lt;f4"&gt;</span><br><span class="line">kernel:0 &lt;HDF5 dataset "kernel:0": shape (16, 8), type "&lt;f4"&gt;</span><br><span class="line">input_1 &lt;HDF5 group "/input_1" (0 members)&gt;</span><br></pre></td></tr></table></figure><p>a Dense layer returns a list of two values– per-output weights and the bias value.<br>在</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://keras.io/api/layers/base_layer/#set_weights-method&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://keras.io/api/layers/base_laye</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Boosting提升</title>
    <link href="https://codingclaire.github.io/2021/04/14/2021-04-14-boosting/"/>
    <id>https://codingclaire.github.io/2021/04/14/2021-04-14-boosting/</id>
    <published>2021-04-14T01:56:20.000Z</published>
    <updated>2021-05-23T06:27:55.993Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Boosting"><a href="#什么是Boosting" class="headerlink" title="什么是Boosting"></a>什么是Boosting</h2><p>TODO</p><h2 id="Boosting的两种方法"><a href="#Boosting的两种方法" class="headerlink" title="Boosting的两种方法"></a>Boosting的两种方法</h2><h3 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h3><h3 id="Gradient-Boost"><a href="#Gradient-Boost" class="headerlink" title="Gradient Boost"></a>Gradient Boost</h3><h2 id="Gradient-Boost-Decision-Tree-GBDT"><a href="#Gradient-Boost-Decision-Tree-GBDT" class="headerlink" title="Gradient Boost Decision Tree(GBDT)"></a>Gradient Boost Decision Tree(GBDT)</h2><p>在每个树节点中找到最佳分割点非常耗时，而且会消耗内存</p><h2 id="Boosting框架"><a href="#Boosting框架" class="headerlink" title="Boosting框架"></a>Boosting框架</h2><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><h3 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h3><ul><li><p>基于histogram</p></li><li><p>leaf-wise</p></li></ul><p><img src="/2021/04/14/2021-04-14-boosting/image-20210414095754315.png" alt="image-20210414095754315"></p><p><img src="/2021/04/14/2021-04-14-boosting/image-20210414095812474.png" alt="image-20210414095812474"></p><p><img src="/2021/04/14/2021-04-14-boosting/image-20210414095908847.png" alt="image-20210414095908847"></p><p>直方图优化：基于分桶，减少内存的使用，正则化不容易overfit</p><p>控制max_depth来控制num_leaves</p><p>num_leaves=2^max_depth</p><p>lightGBM控制num_leaves，而不是树的最大深度，因为lightGBM不会生成满二叉树，因此通过控制num_leaves确保树的深度不过大，防止过拟合。</p><h4 id="防止过拟合的方法"><a href="#防止过拟合的方法" class="headerlink" title="防止过拟合的方法"></a>防止过拟合的方法</h4><p><img src="/2021/04/14/2021-04-14-boosting/image-20210414101304982.png" alt="image-20210414101304982"></p><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><h5 id="num-leaves"><a href="#num-leaves" class="headerlink" title="num_leaves"></a>num_leaves</h5><p>num_leaves越大，增加了训练集的精确度，但增加了过拟合的几率</p><h5 id="num-iterations"><a href="#num-iterations" class="headerlink" title="num_iterations"></a>num_iterations</h5><h2 id="Bagging-和-Boosting的区别"><a href="#Bagging-和-Boosting的区别" class="headerlink" title="Bagging 和 Boosting的区别"></a>Bagging 和 Boosting的区别</h2><p>Bagging：</p><ul><li>训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的</li><li>使用均匀取样，每个样例的权重相等</li><li>所有预测函数的权重相等</li><li>各个预测函数可以并行生成</li></ul><p>Boosting：</p><ul><li>每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化，权值是根据上一轮的分类结果进行调整</li><li>根据错误率不断调整样例的权值，错误率越大则权重越大</li><li>每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重</li><li>各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果</li></ul><hr><p>参考资料：</p><p>1.微软亚洲研究院AI头条分享-<a href="https%3A//v.qq.com/x/page/k0362z6lqix.html">Introduction to LightGBM</a>-Taifeng Wang</p><p>2.<a href="https://easyaitech.medium.com/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3-bagging-boosting-%E4%BB%A5%E5%8F%8A%E4%BB%96%E4%BB%AC%E7%9A%84-4-%E7%82%B9%E5%8C%BA%E5%88%AB-6e3c72df05b8#:~:text=%E8%80%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A0%B8%E5%BF%83,%E4%BD%9C%E2%80%9C%E5%A5%97%E8%A2%8B%E6%B3%95%E2%80%9D%EF%BC%89" target="_blank" rel="noopener">一文看懂集成学习（详解 bagging、boosting 以及他们的 4 点区别）</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是Boosting&quot;&gt;&lt;a href=&quot;#什么是Boosting&quot; class=&quot;headerlink&quot; title=&quot;什么是Boosting&quot;&gt;&lt;/a&gt;什么是Boosting&lt;/h2&gt;&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&quot;Boosting的两种方法&quot;&gt;&lt;a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>TensorBoard相关知识</title>
    <link href="https://codingclaire.github.io/2021/04/13/2021-04-13-Tensorboard/"/>
    <id>https://codingclaire.github.io/2021/04/13/2021-04-13-Tensorboard/</id>
    <published>2021-04-13T03:48:30.000Z</published>
    <updated>2021-04-13T03:49:37.963Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TensorBoard查看结果"><a href="#TensorBoard查看结果" class="headerlink" title="TensorBoard查看结果"></a>TensorBoard查看结果</h3><p><code>tensorboard –logdir /path/to/logs</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;TensorBoard查看结果&quot;&gt;&lt;a href=&quot;#TensorBoard查看结果&quot; class=&quot;headerlink&quot; title=&quot;TensorBoard查看结果&quot;&gt;&lt;/a&gt;TensorBoard查看结果&lt;/h3&gt;&lt;p&gt;&lt;code&gt;tensorboard </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>图神经网络里的监督/半监督和转导/归纳概念解析</title>
    <link href="https://codingclaire.github.io/2021/04/09/2021-04-09-GNN-classification/"/>
    <id>https://codingclaire.github.io/2021/04/09/2021-04-09-GNN-classification/</id>
    <published>2021-04-09T13:23:45.000Z</published>
    <updated>2021-04-09T13:40:52.261Z</updated>
    
    <content type="html"><![CDATA[<p>监督(supervised)和半监督(semi-supervised)其实是转导（transductive)和归纳(inductive)</p><p>对于节点分类来说，如何区分监督学习和半监督学习的关键在于节点在训练过程中被使用的区别，也就是训练集节点是否被用在GNN的消息传递操作中，并且会被用于计算损失。</p><p>transductive的测试节点是无标签的，并且不会被使用在损失计算中，但是这些点和相关的边会被用于消息传递中，也就是说图神经网络会生成测试集节点的潜在表示中，但是最后一层的表示不会被用在损失函数的计算中。</p><p>indutive的测试节点既不会用在GNN的消息传递过程中，也不会用在损失函数计算中，也就是说，inductive在GNN训练的过程中完全不会被包括。</p><p>半监督指的就是GNN会使用transductive的测试节点组成的测试集，也就是说在训练的过程中实际上是能够看到测试的节点的（但不能看到label）。监督指的就是在做归纳式的节点分类时，测试的节点是完全不会被检测到的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;监督(supervised)和半监督(semi-supervised)其实是转导（transductive)和归纳(inductive)&lt;/p&gt;
&lt;p&gt;对于节点分类来说，如何区分监督学习和半监督学习的关键在于节点在训练过程中被使用的区别，也就是训练集节点是否被用在GNN的消</summary>
      
    
    
    
    
    <category term="图" scheme="https://codingclaire.github.io/tags/%E5%9B%BE/"/>
    
    <category term="机器学习" scheme="https://codingclaire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Keras的一些基本操作</title>
    <link href="https://codingclaire.github.io/2021/04/08/2021-04-08-keras/"/>
    <id>https://codingclaire.github.io/2021/04/08/2021-04-08-keras/</id>
    <published>2021-04-08T02:47:25.000Z</published>
    <updated>2021-05-11T12:17:11.949Z</updated>
    
    <content type="html"><![CDATA[<h2 id="生成模型的关键步骤"><a href="#生成模型的关键步骤" class="headerlink" title="生成模型的关键步骤"></a>生成模型的关键步骤</h2><h3 id="model-add"><a href="#model-add" class="headerlink" title="model.add()"></a><code>model.add()</code></h3><h3 id="model-summary"><a href="#model-summary" class="headerlink" title="model.summary()"></a><code>model.summary()</code></h3><p><code>model.summary() #打印神经网络结构，统计参数数目</code></p><h3 id="model-compile"><a href="#model-compile" class="headerlink" title="model.compile()"></a><code>model.compile()</code></h3><p>在配置训练方法时，告知训练时用的优化器、损失函数和准确率评测标准</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer = 优化器</span><br><span class="line">              loss = 损失函数，</span><br><span class="line">              metrics = [<span class="string">"准确率"</span>])</span><br></pre></td></tr></table></figure><h3 id="model-fit"><a href="#model-fit" class="headerlink" title="model.fit()"></a><code>model.fit()</code></h3><p>The history object returned by model.fit() is a simple class with some fields, e.g. a reference to the model, a params dict and, most importantly, a history dict. It stores the values of loss and acc (or any other used metric) at the end of each epoch.</p><h2 id="模型的保存"><a href="#模型的保存" class="headerlink" title="模型的保存"></a>模型的保存</h2><h3 id="保存模型参数：model-to-json"><a href="#保存模型参数：model-to-json" class="headerlink" title="保存模型参数：model.to_json()"></a>保存模型参数：<code>model.to_json()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"model.json"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">        json_file.write(model_json)</span><br></pre></td></tr></table></figure><h3 id="保存-weights-model-save-weights"><a href="#保存-weights-model-save-weights" class="headerlink" title="保存 weights:model.save_weights()"></a>保存 weights:<code>model.save_weights()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_weights(<span class="string">"model.h5"</span>)</span><br></pre></td></tr></table></figure><p>.h5 文件</p><h3 id="保存某一层的输出：model-layers-index-output"><a href="#保存某一层的输出：model-layers-index-output" class="headerlink" title="保存某一层的输出：model.layers[index].output"></a>保存某一层的输出：<code>model.layers[index].output</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inp = model.input</span><br><span class="line">outputs = [layer.output <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers]</span><br></pre></td></tr></table></figure><p>outputs 里的元素的类型是：<br><code>&lt;class &#39;tensorflow.python.framework.ops.Tensor&#39;&gt;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># serialize model to JSON</span></span><br><span class="line">model_json = model.to_json()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"model.json"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json_file.write(model_json)</span><br><span class="line"><span class="comment"># serialize weights to HDF5</span></span><br><span class="line">model.save_weights(<span class="string">"model.h5"</span>)</span><br><span class="line">print(<span class="string">"Saved model to disk"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># later...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load json and create model</span></span><br><span class="line">json_file = open(<span class="string">'model.json'</span>, <span class="string">'r'</span>)</span><br><span class="line">loaded_model_json = json_file.read()</span><br><span class="line">json_file.close()</span><br><span class="line">loaded_model = model_from_json(loaded_model_json)</span><br><span class="line"><span class="comment"># load weights into new model</span></span><br><span class="line">loaded_model.load_weights(<span class="string">"model.h5"</span>)</span><br><span class="line">print(<span class="string">"Loaded model from disk"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate loaded model on test data</span></span><br><span class="line">loaded_model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">score = loaded_model.evaluate(X, Y, verbose=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">"%s: %.2f%%"</span> % (loaded_model.metrics_names[<span class="number">1</span>], score[<span class="number">1</span>]*<span class="number">100</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights_0_list = new_model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights_0_list)):</span><br><span class="line">    print(weights_0_list[i].shape)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;生成模型的关键步骤&quot;&gt;&lt;a href=&quot;#生成模型的关键步骤&quot; class=&quot;headerlink&quot; title=&quot;生成模型的关键步骤&quot;&gt;&lt;/a&gt;生成模型的关键步骤&lt;/h2&gt;&lt;h3 id=&quot;model-add&quot;&gt;&lt;a href=&quot;#model-add&quot; class</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Keras中RNNLayer的输入输出总结</title>
    <link href="https://codingclaire.github.io/2021/04/07/2021-04-07-keras-LSTM/"/>
    <id>https://codingclaire.github.io/2021/04/07/2021-04-07-keras-LSTM/</id>
    <published>2021-04-07T14:00:24.000Z</published>
    <updated>2021-04-12T03:48:33.407Z</updated>
    
    <content type="html"><![CDATA[<p>网上关于 Keras 的 RNNLayer 中的输入写的很不清楚，整理如下：</p><h2 id="LSTM-的输入"><a href="#LSTM-的输入" class="headerlink" title="LSTM 的输入"></a>LSTM 的输入</h2><h3 id="tf-keras-layers-LSTM-参数"><a href="#tf-keras-layers-LSTM-参数" class="headerlink" title="tf.keras.layers.LSTM()参数"></a><code>tf.keras.layers.LSTM()</code>参数</h3><p><a href="https://keras.io/api/layers/recurrent_layers/lstm/" target="_blank" rel="noopener">文档</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.LSTM(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'sigmoid'</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, recurrent_regularizer=<span class="literal">None</span>, bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>, kernel_constraint=<span class="literal">None</span>, recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">    return_sequences=<span class="literal">False</span>, return_state=<span class="literal">False</span>, go_backwards=<span class="literal">False</span>, stateful=<span class="literal">False</span>,</span><br><span class="line">    time_major=<span class="literal">False</span>, unroll=<span class="literal">False</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="input-dim、input-length、input-shape-的关系"><a href="#input-dim、input-length、input-shape-的关系" class="headerlink" title="input_dim、input_length、input_shape 的关系"></a>input_dim、input_length、input_shape 的关系</h3><p>LSTM 的输入是一个三维的张量（numpy narray), 三维张量的 shape 是[samples, time steps, features]，也就是[样本数量，时间步长（序列数量），特征长度]。LSTM layer 的参数需要确定其中的两个，在 model.fit 时，就能够对 trainX 进行训练。因此 input_dim 表示单个样本的特征长度，可以用 trainX.shape[2]赋值； input_length 表示的就是时间步长，序列长度，可以用 trainX.shape[1]进行赋值。</p><p>另外一种写法是 input_shape，其实就是这两个量的结合：input_shape = (input_length, input_dim)</p><p>因此以下的两种写法是等价的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.add(LSTM(units=<span class="number">256</span>, return_sequences=<span class="literal">True</span>,</span><br><span class="line">            input_dim=trainX.shape[<span class="number">2</span>], input_length=trainX.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.add(LSTM(units=<span class="number">256</span>, return_sequences=<span class="literal">True</span>,</span><br><span class="line">            input_shape=(trainX.shape[<span class="number">1</span>], trainX.shape[<span class="number">2</span>])))</span><br></pre></td></tr></table></figure><p>但比较奇怪的是这样设置最终的结果第一维会是 None,最终输出的是<code>[None,timesteps, feature]</code>。如果设置<code>input_size=trainX.size</code>的话，会出现以下错误：<br><code>ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4.</code><br>但是如果使用 batch_input_shape=trainX.shape就可以正常运行，并且最终得到训练的每一个样本的 <code>[timesteps,feature]</code>张量。</p><p>后来查了keras LSTM的官方文档，它对input的定义是<code>[batch, timesteps, feature]</code>，也就是说第一个参数指的是 batch 的大小，如果没有就默认为 None。在<code>model.fit</code>里有一个batch_size，如果设置了该batch_size的值，那么LSTM的层的input会自动根据trainX.shape[0]和batch_size的值来确定每一个输入的batch的大小。</p><p>batch size 限制了在可以执行权重更新之前向网络显示的样本数。拟合模型时使用的 batch size 控制一次必须进行多少预测。</p><h2 id="GRU-的输入"><a href="#GRU-的输入" class="headerlink" title="GRU 的输入"></a>GRU 的输入</h2><h3 id="tf-keras-layers-GRU-参数"><a href="#tf-keras-layers-GRU-参数" class="headerlink" title="tf.keras.layers.GRU()参数"></a><code>tf.keras.layers.GRU()</code>参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.GRU(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'sigmoid'</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="literal">None</span>, bias_regularizer=<span class="literal">None</span>, activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>, recurrent_constraint=<span class="literal">None</span>, bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, return_sequences=<span class="literal">False</span>, return_state=<span class="literal">False</span>,</span><br><span class="line">    go_backwards=<span class="literal">False</span>, stateful=<span class="literal">False</span>, unroll=<span class="literal">False</span>, time_major=<span class="literal">False</span>,</span><br><span class="line">    reset_after=<span class="literal">True</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网上关于 Keras 的 RNNLayer 中的输入写的很不清楚，整理如下：&lt;/p&gt;
&lt;h2 id=&quot;LSTM-的输入&quot;&gt;&lt;a href=&quot;#LSTM-的输入&quot; class=&quot;headerlink&quot; title=&quot;LSTM 的输入&quot;&gt;&lt;/a&gt;LSTM 的输入&lt;/h2&gt;&lt;h3</summary>
      
    
    
    
    
    <category term="Keras" scheme="https://codingclaire.github.io/tags/Keras/"/>
    
    <category term="RNN" scheme="https://codingclaire.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>gitbook+Typora打造舒适的笔记环境</title>
    <link href="https://codingclaire.github.io/2020/06/22/gitbook-Typora%E6%89%93%E9%80%A0%E8%88%92%E9%80%82%E7%9A%84%E7%AC%94%E8%AE%B0%E7%8E%AF%E5%A2%83/"/>
    <id>https://codingclaire.github.io/2020/06/22/gitbook-Typora%E6%89%93%E9%80%A0%E8%88%92%E9%80%82%E7%9A%84%E7%AC%94%E8%AE%B0%E7%8E%AF%E5%A2%83/</id>
    <published>2020-06-22T08:53:38.000Z</published>
    <updated>2021-04-07T11:51:16.764Z</updated>
    
    <content type="html"><![CDATA[<p>GitBook 是一个基于 Node.js 的命令行工具，可使用它来制作精美的电子书。gitbook简洁而且高效，能够用一种结构化的方式组织文章或者笔记，所以不管是学习的输入还是撰写文章的输出，gitbook都不失为一个很好的工具。</p><p>网上关于如何安装gitbook的文章有很多，此处不进行总结了。</p><a id="more"></a><h1 id="为什么用Typora？"><a href="#为什么用Typora？" class="headerlink" title="为什么用Typora？"></a>为什么用Typora？</h1><p>Typora可以支持实时预览，比起印象笔记等分屏的markdown写作工具,Typora这种所见即所得的极简给做笔记或者写作带来的体验感是非常强的。Typora的大部分语言都是传统markdown，使用Typora在官网下载对应版本即可 。</p><p>emoji的使用方法，这个我之前不知道，是无意中触发的：</p><p>只要用<code>:emoji-name:</code>的形式就可以插入emoji，通常一个冒号后面加字母就会自动提示emoji了。</p><p>:accept::clinking_glasses::v::ok::zap:</p><h1 id="gitbook-常用命令"><a href="#gitbook-常用命令" class="headerlink" title="gitbook 常用命令"></a>gitbook 常用命令</h1><h2 id="1-gitbook-init"><a href="#1-gitbook-init" class="headerlink" title="1.gitbook init"></a>1.gitbook init</h2><p>这个命令会在指定文件夹创建README.md和SUMMARY.md。</p><h2 id="2-gitbook-build"><a href="#2-gitbook-build" class="headerlink" title="2.gitbook build"></a>2.gitbook build</h2><p>运行该命令后会在书籍的文件夹中生成一个 <code>_book</code> 文件夹, 里面的内容即为生成的 html 文件,可以将这个文件发布自己到github的仓库中，可以作为项目的文档或者其他笔记等，使用<code>nameofUser.github.io/nameofRepository</code>域名就可以访问到在线笔记。</p><p>注意如果是一个有其他文件的仓库的话，需要在git中创建<code>docs</code>分支，然后将<code>_book</code>的内容传入该仓库的该分支中才能够访问。</p><h2 id="3-gitbook-serve"><a href="#3-gitbook-serve" class="headerlink" title="3.gitbook serve"></a>3.gitbook serve</h2><p>这一命令能够让我们在浏览器预览gitbook，通常能够在<code>http://localhost:4000</code> 预览。</p><h1 id="常用插件"><a href="#常用插件" class="headerlink" title="常用插件"></a>常用插件</h1><p>gitbook支持很多插件，能够更方便地帮助你使用gitbook。插件安装时需要在gitbook所在根目录下新建book.json， 并按照下面的配置进行修改或创建，最后使用<code>gitbook install</code>命令将对应的node_modules下载。</p><h2 id="显示文章目录：toc"><a href="#显示文章目录：toc" class="headerlink" title="显示文章目录：toc"></a>显示文章目录：toc</h2><p>一般来说如果想要显示文章目录的话，在Typora中可以在文章最开始加入<code>[toc]</code>，Typora就能够自动生成对应目录，但这个目录无法在gitbook中显示，这个插件让文档能够插入目录，在浏览器显示时也能够看见目录。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"plugins"</span> : [</span><br><span class="line">        <span class="string">"toc"</span>,</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"pluginsConfig"</span>: &#123;</span><br><span class="line">        <span class="attr">"toc"</span>: &#123;</span><br><span class="line">            <span class="attr">"addClass"</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"className"</span>: <span class="string">"toc"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要目录时须在文章开始添加<code>&lt;!-- toc --&gt;</code>，这样才会显示目录。</p><h2 id="总目录折叠：expandable-chapters"><a href="#总目录折叠：expandable-chapters" class="headerlink" title="总目录折叠：expandable-chapters"></a>总目录折叠：expandable-chapters</h2><p>这个插件使目录具有折叠功能。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"plugins"</span> : [</span><br><span class="line">        <span class="string">"expandable-chapters"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>！这里将不断继续更新 ！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;GitBook 是一个基于 Node.js 的命令行工具，可使用它来制作精美的电子书。gitbook简洁而且高效，能够用一种结构化的方式组织文章或者笔记，所以不管是学习的输入还是撰写文章的输出，gitbook都不失为一个很好的工具。&lt;/p&gt;
&lt;p&gt;网上关于如何安装gitbook的文章有很多，此处不进行总结了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="gitbook" scheme="https://codingclaire.github.io/tags/gitbook/"/>
    
    <category term="小技巧" scheme="https://codingclaire.github.io/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>文本数据的聚类分析综述</title>
    <link href="https://codingclaire.github.io/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/"/>
    <id>https://codingclaire.github.io/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/</id>
    <published>2020-06-10T08:25:51.000Z</published>
    <updated>2020-06-14T16:33:32.909Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p>聚类分析是一种无监督学习方法，在模式识别中，对于给定的数据样本，类别标号已知的情况下，分类问题通过训练，使得能够对未知类别的样本进行分类。而现实世界中，相当多的数据是没有已知类别的，它们的类别缺失或者需要大量的人工标注才能获取类别。为了发现数据的内在知识、检测并分析异常点和从数据中提取模式，聚类分析是非常重要的。</p><p>聚类分析依据相似性，将给定数据样本划分成若干个类别，相似性越高的两个物体划分为同一类，最终会将数据形成若干个簇，簇与簇可根据它们的形状、大小和密度等有所区别。</p><p>生活中的多个方面聚类都能够辅助模式识别和数据挖掘。在产品市场上，聚类可以基于用户的购买对商品进行聚类，使得市场营销人员能够利用这些知识开发有针对性地计划；在城市规划上，聚类可以将相似性高的区域进行划分，为土地建设提供选址方案等。</p><p>随着全球信息化的不断发展，大量文本数据隐含着潜在的信息和知识。文本数据是一种非常常见的非结构化数据，针对文本数据的聚类应用领域也很广泛。在信息检索方向，文本聚类可对搜索引擎进行聚类，提升用户获取信息的精确度；在信息推荐方向，聚类还可以提取出热点主题或发现事件、自动归档文本并帮助完善文本可视化。</p><p>实现文本聚类主要由三个步骤组成：1.文档的表示（提取文档特征并对特征降维处理）；2.文本聚类算法的选择和应用；3.评估文本聚类算法的有效性。三个步骤将在接下来的4章中进行详细的探讨。</p><h1 id="二、文本数据的特征提取"><a href="#二、文本数据的特征提取" class="headerlink" title="二、文本数据的特征提取"></a>二、文本数据的特征提取</h1><p>计算机难以直接对字符串文本进行处理，需要将实际的文字转化成数值型数据。对文本本身来说，它具有一些显式的特征，如字数、词频、停止词数量、单词平均长度等。为了实现文本的聚类，上述的特征需要进行处理和调整，按照某种完整的模型对文档进行数值化或向量化。</p><p>当前的主要的文档模型可被分为五个类别：布尔模型、向量空间模型、概率模型、统计语言模型和分布表示模型。</p><h2 id="（一）布尔模型"><a href="#（一）布尔模型" class="headerlink" title="（一）布尔模型"></a>（一）布尔模型</h2><p>布尔模型具有简洁的形式，容易理解。它的基础是集合论和布尔代数。我们考虑单词在文档中出现或缺失时，一个文档能够用二进制向量表示。</p><h2 id="（二）向量空间模型"><a href="#（二）向量空间模型" class="headerlink" title="（二）向量空间模型"></a>（二）向量空间模型</h2><p>向量空间模型是将文档表达为向量空间的一个矢量或点，向量空间的维数是词的数量。在向量空间的文档向量的长度是由出现的词和词的权重共同决定的[1]。在向量空间中，单词的顺序并不被考虑，这种方法也称为词袋表示方法（Bag of Words）。它是一种简单、经典的表示方法，但它对出现在文本中的词无法判定其重要性的差异，导致准确率不高。</p><p>1983年，Salton等提出了扩展布尔模型[2]，它结合了布尔模型和向量空间模型，并表现出检索性能的提升。</p><p>1986年，TF-IDF被提出[3]，这种表示改进了词袋表示法，每个单词的词频都由逆文档频率（IDF）规范化。在单词集合中，出现频率更高的项权重更低，降低了常用词在文档中的重要性，保证后续文档聚类的结果更受文档出现频率低的词的影响。</p><h2 id="（三）概率模型"><a href="#（三）概率模型" class="headerlink" title="（三）概率模型"></a>（三）概率模型</h2><p>概率模型中，文档<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image002.png" alt="img">)与查询<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image004.png" alt="img">)的相似度有如下关系：<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">)表示相关文档集，<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image008.png" alt="img">)表示<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">的补集。</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615001825039.png" alt="image-20200615001825039"></p><p>对文档而言，根据独立性假设，文档的各个词相互独立，用<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image012.png" alt="img">表示词可得到：</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615001859217.png" alt="image-20200615001859217"></p><p>其中词权重<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image016.png" alt="img">。</p><p>用<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image018.png" alt="img">)表示相关文档数，<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image020.png" alt="img">)表示包含索引词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image022.png" alt="img">)的文档数，相关文档中<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image022.png" alt="img">)的分布<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image024.png" alt="img">, 不相关文档中<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image022.png" alt="img">)的分布<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615001927832.png" alt="image-20200615001927832">),<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image028.png" alt="img">)表示包含索引词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image022.png" alt="img">的文档数。</p><p>则可推出：</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615001955999.png" alt="image-20200615001955999"></p><p>概率模型的优点在于，文档可以按照相关概率递减顺序来计算秩；但概率模型需要把文档分为相关和不相关的两个集合，未考虑到单词的频率，没有权重系数[4]。 </p><h2 id="（四）-统计语言模型"><a href="#（四）-统计语言模型" class="headerlink" title="（四） 统计语言模型"></a>（四） 统计语言模型</h2><p>统计语言模型(Statistics Language Models)是基于统计学和概率论对语言进行建模的，主要思想是语言是字母表上的概率分布，该分布表示一种可能性：即任何一个字母序列成为该语言的一个句子。这一分布就是语言的统计语言模型。目前较流行的统计语言模型是n元模型（N-gram），表示一个词的出现与否和其前面的n-1个词有关。</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image002-1591777871572.png" alt="img"></p><h2 id="（五）分布表示模型"><a href="#（五）分布表示模型" class="headerlink" title="（五）分布表示模型"></a>（五）分布表示模型</h2><p>分布式表示模型不仅考虑将单词符号化，还考虑将语义信息融入到词表示中。 1954 年，Harris提出了分布假说（ distributional hypothesis）上下文相似的词，其语义也相似[5]，这一假说为语义信息的融入提供了理论基础。</p><p>分布式表示根据任务、算法的区别，可被分为基于矩阵的分布表示、基于聚类的分布表示和基于神经网络的分布表示。在基于聚类的分布表示中，较典型的算法为布朗聚类方法(Brown clustering)，在第四章会具体介绍该算法。</p><h1 id="三、样本的相似性度量"><a href="#三、样本的相似性度量" class="headerlink" title="三、样本的相似性度量"></a>三、样本的相似性度量</h1><p>文本聚类根据不同的粒度可以分为文档、段落、语句或者单词的聚类。样本在不同的粒度下代表的事物也有所区别，如文档聚类时，每一个样本表示一个文档。对文档进行聚类时，我们需要获知文档样本与样本之间的相似度，需要相似性的度量标准。</p><p>相似性度量可使用空间两点的欧式距离、向量内积、余弦相似度、Jaccard相似度等。</p><p>相似度的度量会在一定程度上影响算法的效果，目前也有大量的研究针对聚类的相似性度量，如2009年，Luo[6]等人应用了邻居和链接的概念，将全局信息引入到两个文档的相似性度量上，提出了新的相似性度量方式：使用余弦和链接函数组合等，总而言之，相似性度量并不存在最优的方法，需要和聚类算法结合。</p><h1 id="四、文本聚类方法"><a href="#四、文本聚类方法" class="headerlink" title="四、文本聚类方法"></a>四、文本聚类方法</h1><p>传统的聚类分析算法不仅可以用在文本数据上，其他数据也是通用的。针对文本表示的不同形式，使用的聚类算法也有所区别，文本聚类主要可以分为三类方法：划分聚类方法、层次聚类方法和基于标准参数化模型的方法。</p><h2 id="（一）划分聚类方法"><a href="#（一）划分聚类方法" class="headerlink" title="（一）划分聚类方法"></a>（一）划分聚类方法</h2><p>划分方法符合我们对聚类的直观感受，将多个样本点组织成多个簇，通常簇的个数会在聚类前被给定，融合了相关领域的主观知识。</p><p>划分方法最初指定类别的初始数目，并不断迭代分配样本点，最终收敛时确定所有簇。划分方法运用在文本领域的主要有K-means和K-medoids两种聚类算法。</p><h3 id="1-K-means聚类算法"><a href="#1-K-means聚类算法" class="headerlink" title="1. K-means聚类算法"></a>1. K-means聚类算法</h3><p>K-means算法最早是从不同的科学领域中提出来的，包括1956年的Steinhaus[7], 和1957年的Lloyd[8]，至今已经提出了近60年，但它仍然是目前应用于聚类的算法之一。</p><p>K-means通过判断根据平方误差法计算出的目标函数是否达到最优解，而逐步对聚类结果进行优化。在运行前需要指定簇的类别、初始的簇的中心点，在每次迭代中，将每个点分配给中心最近的聚类。中心是群中所有点的平均值，平均点的坐标是簇中所有点上每个维度的算术平均值。</p><p>原始的K-means的缺点主要有以下几点：首先，它只考虑了样本点之间的距离，通常结果均为球状簇。若从样本点的密度考虑，以DBSCAN算法为代表的基于密度的方法能够发现任意形状的簇。其次，K值、初始化分方向等均是需要用户给定的，容易陷入局部最优。</p><h3 id="2-K-medoids聚类算法"><a href="#2-K-medoids聚类算法" class="headerlink" title="2.K-medoids聚类算法"></a>2.K-medoids聚类算法</h3><p>K-medoids聚类算法使用类中的某个点来代表簇，最早提出的K-mediods算法之一PAM(Partitioning Around Medoids) [9]的基本思想就是最初选取k个代表对象作为初始的中心点，依据当前cluster中所有其他点到该中心点的距离之和最小的准则函数，不断迭代找到更好的中心点。</p><p>该算法在一定程度上削弱了异常值的影响，但缺点是计算较为复杂，耗费的计算机时间比K-means多。它能处理任意类型的属性，但对异常数据不敏感。</p><h2 id="（二）-层次聚类方法"><a href="#（二）-层次聚类方法" class="headerlink" title="（二） 层次聚类方法"></a>（二） 层次聚类方法</h2><p>按照层次的聚类方法源于对数据需要组成层次结构的需求，数据需要进行层次结构上的汇总和特征化，因此层次划分方法被引入。层次划分方法可分为凝聚和分裂两种策略。</p><p>凝聚策略是将每个样本点在聚类最初都形成一个簇，随着迭代的进行，会将所有簇合并，直到终止条件为止。分类策略与凝聚策略正好相反，它将所有的样本点都看成同一个簇，相当于层次结构的根，将簇不断划分为更小的簇，直到划分的每一个簇都达到凝聚的条件。</p><p>以文档聚类为例，凝聚层次聚类方法可以被分为三类[10]：单连接算法（Single Linkage Clustering）、平均连接算法（Group-Average Linkage Clustering）、全连接算法（Complete Linkage Clustering）。</p><p>单连接算法的基本思想是两个簇的距离度量是从两个簇中抽取的每一对样本的最小距离<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image002-1591777984132.png" alt="img">,一旦最近的两个簇的距离超过某个任意给定的阈值，则算法结束。平均连接的基本算法是两个簇的距离度量是所有样本对的距离的平均值，全连接算法的距离度量则是两个簇所有样本对的最坏情况。</p><p>在针对文本数据的聚类中流行的层次聚类算法包括：综合的层次聚类方法BIRCH[11]，其优点在于能够通过单词扫描获取一个较好的聚类效果，但它只适用于数值型数据；基于质心和代表对象方法的CURE聚类方法[12]从每个类中抽取固定数量、分布较好的点作为代表点，并乘收缩收缩因子，减小噪音对聚类的影响；适用于分类属性层次的聚类算法ROCK[13],和使用动态模型的层次聚类算法Chameleon[14]。</p><p>1992年提出的布朗聚类方法[15]是一种针对词汇聚类的算法，它借鉴了层次聚类的凝聚策略，它的输入时一个语料库，语料库是一个词序列，输出是一个二叉树，二叉树的叶子节点是词，中间节点是对应的类别。它的评价函数是对于<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image004-1591777984132.png" alt="img">)个连续的词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006-1591777984133.png" alt="img">)序列能否组成依据话的概率的对数的归一化结果，评价函数为:$Quality(C)=\frac{1}{n}logP(w_1,w_2…w_n)$。该函数描述了某个词上下文单词对当前聚类中单词的出现的预测程度。</p><p>Gil-García[16]等在2006年提出了一个基于图的凝聚层次聚类的通用框架，这一框架指定簇间相似度度量、β-相似度图的子图和覆盖例程，可以得到不同的层次的凝聚型的聚类算法；在2010年[17]，作者又提出了针对文档聚类的动态层次算法，该算法在获取和其他传统分层算法相似的聚类质量的前提下，层次结构更小、更利于浏览，可用于创建文档分类法和分层主题检测等模式识别问题。</p><p>层次聚类的鲁棒性较强，因为它通常需要比较所有的文档，因此复杂度达到<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image010-1591777984134.png" alt="img">。为了提升层次聚类方法的效率，多种改进方法被提出。如2018年，Zhang等人[18]提出了一个分区合并方案（PMHC）用于快速分层群集，它将数据对象分成适当的组并将它们合并到组中以节省计算成本。</p><h2 id="（三）基于标准参数化模型的方法"><a href="#（三）基于标准参数化模型的方法" class="headerlink" title="（三）基于标准参数化模型的方法"></a>（三）基于标准参数化模型的方法</h2><p>给定文档<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image012-1591777984134.png" alt="img">)，获取该文档属于不同簇的概率向量q，也是文档聚类的任务之一。考虑第二章中用统计语言模型表示文档的方式，可假定文档的生成过程是先以一定概率<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image014-1591777984134.png" alt="img">)选择簇<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image016-1591777984135.png" alt="img">),然后再按照词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image018-1591777984135.png" alt="img">)的概率分布<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image020-1591777984135.png" alt="img">)选择词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image018.png" alt="img">生成文档d,观测的所有文档在混合模型中被生成的概率为：</p><p>  <img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image024-1591777984136.png" alt="img"></p><p>其中<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image014-1591777984134.png" alt="img">为“聚簇参数”，可通过期望最大化算法学习。该方法会陷入局部最优导致收敛速度较慢。</p><p>基于EM算法进行聚类的研究主要是基于EM算法对聚类方法的改进和提升，如2005年，Rigutini[19]等人将EM算法与基于信息增益的特征选择技术相结合，该算法只需要少量文档初始化聚类，并且能够正确地提取隐藏在大量未标记集合中的规则。2011年，Kim[20]等人基于EM算法，提出了一种文本文档的主题聚类算法，使用EM方法确保文档被分配给正确的主题，从而收敛到局部最优解，其结果具有较好的性能和可解释性。</p><p>EM算法衍生出了主题建模，它是一种对文档进行聚类并提取主题的无监督学习方法，可用来识别大规模文档集或语料库中潜藏的主题信息，广泛应用在文本分类、文本聚类、摘要抽取、情感分析等领域。</p><p>主题建模起源于潜在语义分析（LSA）[21]，该方法通过奇异值分解，将高维文档向量近似地映射到一个低维潜在地语义空间上，以达到降低文档维数和消除词语存在的同义、多义等问题。在LSA基础上，Hofmann引入了概率统计的思想，提出了概率潜在语义分析模型[22]。然而pLSA模型的参数容易与特定的文档相关，有时会出现过拟合现象，因此，Blei等人在2003年提出了LDA概率主题模型[23]，把模型的参数也看作随机变量，引入控制参数的参数，实现进一步的概率化。LDA本质上是一种无监督无指导的机器学习模型，将高维文本单词空间表示为低维主题空间，忽略了和文本相关的类别信息。</p><h2 id="（四）其他聚类学习方法"><a href="#（四）其他聚类学习方法" class="headerlink" title="（四）其他聚类学习方法"></a>（四）其他聚类学习方法</h2><p>除了上述三点主要的聚类算法之外，针对文本数据的部分其他聚类算法将在本节进行简短的阐述。</p><p>模糊聚类需要根据研究对象本身的属性来构造模糊矩阵，并根据隶属度来构造模糊矩阵，最终确定聚类关系。它可允许一个文档属于不同的局促，使得聚类结果更稳定[24]。</p><p>半监督聚类是一种更新的研究算法，半监督聚类的核心思想是把半监督学习的思想结合到聚类中，通过少量的标签数据和先验知识提高聚类性能，得到性能更优的结果。在文本聚类中，使用半监督获取少量标签的聚类算法也有部分研究。</p><p>Zhang W 等提出了基于频繁项集和相似度计算的最大获取的文本聚类方法[25]。</p><h1 id="五、聚类结果的评价"><a href="#五、聚类结果的评价" class="headerlink" title="五、聚类结果的评价"></a>五、聚类结果的评价</h1><p>  聚类结果并没有没有适用于所有算法的统一的评价指标，聚类算法结果的好坏取决于聚类算法的使用的相似性度量和相应的聚类算法。首先好的聚类的簇需要满足两个特点：簇内高内聚，簇间低耦合。其次，好的聚类能够发现隐含的模式，簇的形状没有较大限制；最后从用户的角度来说，能够产生一个满足用户的聚类结果，结果具有可解释性、可理解性。</p><h2 id="（一）分类评价指标"><a href="#（一）分类评价指标" class="headerlink" title="（一）分类评价指标"></a>（一）分类评价指标</h2><p>  通常，聚类任务可以使用分类任务的数据集（包含分类标签），衡量聚类的质量可以使用分类任务的评价指标。</p><h3 id="1-召回率和准确率"><a href="#1-召回率和准确率" class="headerlink" title="1.召回率和准确率"></a>1.召回率和准确率</h3><p>对于信息检索的结果，其计算包括了两个指标：召回率（Recall Rate）和准确率（Precision Rate）。召回率表示检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率；准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；F 值为两者的调和平均值。</p><h3 id="2-宏平均和微平均"><a href="#2-宏平均和微平均" class="headerlink" title="2.宏平均和微平均"></a>2.宏平均和微平均</h3><p>宏平均（Macro-averaging），是先对每一个类统计指标值，然后在对所有类求算术平均值。微平均（Micro-averaging**），是对数据集中的每个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标[26]。</p><h2 id="（二）交叉检验方法"><a href="#（二）交叉检验方法" class="headerlink" title="（二）交叉检验方法"></a>（二）交叉检验方法</h2><p>将用于聚类的数据集划分为m个部分，随机使用m-1个部分建立聚类模型，并用剩下的1个部分检验聚类的质量。这一部分可以计算与他们最近形心的距离平方和作为度量，重复m次后，总体质量度量由质量度量的平均值计算出来，对不同的k，可以比较总体质量度量，最终选取最佳拟合数据的簇数[27]。</p><h2 id="（三）聚类质量的测定"><a href="#（三）聚类质量的测定" class="headerlink" title="（三）聚类质量的测定"></a>（三）聚类质量的测定</h2><p>当有专家构建的基准时，可将聚类模型和基准进行比较，比较时聚类质量度量Q如满足以下4项基本标准：簇的同质性、簇的完全性、碎布袋、小簇保持性，那么可以使用Q进行比较和评估。</p><p>当基准不存在时，可以采用轮廓系数对距离进行内部评估。</p><p>假设数据集D有<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image002-1591778129766.png" alt="img">)个样本被分为<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image004-1591778129766.png" alt="img">)个类别，则对于任意一个样本<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006-1591778129767.png" alt="img">),计算<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">)与<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">)所在簇中其他对象的平均距离<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image008-1591778129767.png" alt="img">),<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">)与其他簇的最小平均距离为<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image010-1591778129768.png" alt="img">。轮廓系数的定义为：</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615002122028.png" alt="image-20200615002122028"></p><p>  当轮廓系数为越接近1时，包含<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">的簇是紧凑的，当轮廓系数值为负时，这种情况是糟糕的，应该避免。</p><h1 id="六、聚类的局限性和挑战"><a href="#六、聚类的局限性和挑战" class="headerlink" title="六、聚类的局限性和挑战"></a>六、聚类的局限性和挑战</h1><p>不同的文本数据有不同的特性，目前文本数据聚类的局限性也给文本聚类这一领域带来了新的挑战。</p><p>目前文本数据仍存在数据稀疏等问题，文档的词汇可能很多，但这些词汇是相互关联的，数据中主成分的数量远小于特征空间的特征数量。因此上述的所有聚类方法并不能解决所有文本的聚类问题。</p><p>近年来社交网络媒体和在线聊天应用创造了大量的文本数据，特别是短文本，短文本表示维数大，如何探索出更有效率、更节省空间的数据表示形式、如何将表示形式与聚类算法更好地结合在一起，是未来仍值得研究的课题。</p><p>文本数据也越来越多地出现在异构应用程序中，有效地将基于文本的算法应用于异构多媒体场景是非常关键的。P2P分布式文档聚类算法解决了其中的一些难题[28]，但对于开发结合优化技术的新型混合算法的研究仍有很大的需求。近年来的研究热点也集中在高维数据的处理上，不断提高处理速度和规模。</p><p>【后记-如果你还能看到这里】<br>这是模式识别课程的最终提交论文（我靠着这个论文得了98分），找了几十篇论文掐头去尾粗略的看了，还是有很多不懂的地方，但至少对于这个方向有了一个框架上的概念。写综述真的很锻炼人哇…</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Salton, G. Some experiments in the generation of word and document associations [A].Proceedings of the December 4–6, 1962, fall joint computer conference[C].1962.234–250. </p><p>[2] Salton, G.&amp; E. A. Fox.&amp; H. Wu. Extended Boolean information retrieval[J]. Communications of the ACM, 1983,26(11):1022–1036. </p><p>[3]Salton, G.&amp;M.J.McGill. Introduction to modern information retrieval[M].New York.The McGraw-Hill Companies,1986.</p><p>[4]McCullagh, P. What is a statistical model?[J]. Annals of Statistics,2002,30:1225–1310. </p><p>[5]Harris, Z. Distributional structure[J]. Word,1954,10(23):146-162.</p><p>[6]Luo, C., Li, Y., &amp; Chung, S. M. Text document clustering based on neighbors[J]. Data &amp; Knowledge Engineering,2009,68(11):1271–1288.</p><p>[7]Steinhaus, H. Sur la division des corp materiels en parties[J]. Bull. Acad. Polon. Sci,1956, IV (C1.III):801–804.</p><p>[8]Lloyd, S. Least squares quantization in PCM[J].IEEE Trans Inform Theory.1982,28:129–137. </p><p>[9]Kaufman,L.&amp;,P.J.Rousseeuw.,Clustering by means of Medoids[J].Statistical Data Analysis Based on the L1–Norm and Related Methods,1987: 405–416.</p><p>[10]Aggarwal, C. C.&amp;C.Zhai.A Survey of Text Clustering Algorithms[J]. Mining Text Data,2012: 77–128.</p><p>[11]Charikar,M.&amp;C.Chekuri. Incremental clustering and dynamic information retrieval[J]. SIAM J Comput, 2004,33(6):1417-1440.</p><p>[12]Guha,S.&amp;R.Rastogi.CURE: an efficient clustering algorithm for large databases[J]. Inf Syst,2003,26(1):35-58.</p><p>[13]Dutta,M.&amp;AK.Mahanta.QROCK: a quick version of the ROCK algorithm for clustering of categorical data[J]. Pattern Recognit Letter, 2005,26(15):2364-2373.</p><p>[14]Karypi,G.&amp;EH.Han.Chameleon: a hierarchical clustering algorithm using dynamic modeling. Computer,1999,32:68-75.</p><p>[15]Brown,P,F&amp;V.J.Della Pietra.Class-Based n-gram Models of Natural Language[J].Computational Linguistics,1992,18:467-480.</p><p>[16]Gil-García,J.&amp;M. Badía-Contelles&amp;A.Pons-Porrata. Extended Star Clustering Algorithm[J]. Lecture Notes on Computer Sciences,2003,2905:480-487.</p><p>[17]Gil-García,R.&amp;A.Pons-Porrata.Dynamic hierarchical algorithms for document clustering[J].Pattern Recognition Letters,2010,31(6):469-477.</p><p>[18]Zhang, Y.&amp; Cheung, Y. A fast hierarchical clustering approach based on partition and merging scheme[A]. 2018 Tenth International Conference on Advanced Computational Intelligence (ICACI).[C].Xiamen,2018.846-851.</p><p>[19]Kim, S.&amp; Wilbur, W. Thematic clustering of text documents using an EM-based approach[J]. Journal of Biomedical Semantics, 2012,3(Suppl 3), S6.</p><p>[20]Rigutini,L&amp;U.Adegli Studi di Siena.A semi-supervised document clustering algorithm based on EM[A].IEEE/WIC/ACM International Conference on Web Intelligence[C], Compiègne (France): Proceedings of the IEEE/ACM/WI International Conference on Web Intelligence,2005.200-206.</p><p>[21]Deerwester,S&amp;S.Dumais.Indexing by latent semantic analysis[J].Journal of the American Society for Informatlon Science,1990,41(6):391-407.</p><p>[22]Hofmann,T.Probabilistic latent semantic analysis[A].Proc.of the Conference on Uncertainty in Artificial Intelligence[C].1999:289—296.</p><p>[23]Blei,D&amp;A.Ng A.Latent Dirichlet Allocation[J].Journal of Machine Learning Research,2003,3:993—1022．</p><p>[24]C. Borgelt and A. Nurnberger.Fast Fuzzy Clustering of Web Page Collections[A].Proc. of PKDD Workshop on Statistical Approaches for WebMining（SAWM)[C],Pisa(Italy) 2004.</p><p>[25]Zhang,W.&amp;T.Yoshida.Text Clustering Using Frequent Itemsets[J]. Knowledge-Based Systems,2010,23(5):379-388.</p><p>[26]Yang Y. An evaluation of statistical approaches to text categorization[J]. Information retrieval, 1999, 1(1-2): 69-90.</p><p>[27]Han,J&amp;M.Kamber.数据挖掘：概念与技术(原书第3版)[M].北京：机械工业出版社.2012.</p><p>[28]Judith, J.E.&amp;J.Jayakumari.Distributed document clustering algorithms: a recent survey[J].Int. J. Enterprise Network Management,2015,Vol. 6, No. 3:207–221.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、引言&quot;&gt;&lt;a href=&quot;#一、引言&quot; class=&quot;headerlink&quot; title=&quot;一、引言&quot;&gt;&lt;/a&gt;一、引言&lt;/h1&gt;&lt;p&gt;聚类分析是一种无监督学习方法，在模式识别中，对于给定的数据样本，类别标号已知的情况下，分类问题通过训练，使得能够对未知类别的</summary>
      
    
    
    
    
    <category term="文本挖掘" scheme="https://codingclaire.github.io/tags/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/"/>
    
    <category term="聚类分析" scheme="https://codingclaire.github.io/tags/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题5】中间代码生成</title>
    <link href="https://codingclaire.github.io/2020/05/23/%E4%B8%AD%E9%97%B4%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/"/>
    <id>https://codingclaire.github.io/2020/05/23/%E4%B8%AD%E9%97%B4%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/</id>
    <published>2020-05-23T09:27:48.000Z</published>
    <updated>2020-06-14T15:45:18.837Z</updated>
    
    <content type="html"><![CDATA[<p>中间代码生成就是把经过语法分析和语义分析的源程序中间表示翻译为中间代码展示，中间表示可能有多个种类，如语法树、DAG、后缀式、三地址代码等。</p><p>如果中间代码独立于机器的话，那么各便于编译系统的建立和移植，并且便于进行独立于机器的代码优化工作。</p><h1 id="三地址代码"><a href="#三地址代码" class="headerlink" title="三地址代码"></a>三地址代码</h1><p>三地址代码包含一个运算和三个地址，两个地址用于存放运算对象，一个用于存放运算结果。</p><p>具体实现：四元式、三元式、间接三元式。</p><h2 id="四元式"><a href="#四元式" class="headerlink" title="四元式"></a>四元式</h2><p>op、arg1、arg2、result</p><h2 id="三元式"><a href="#三元式" class="headerlink" title="三元式"></a>三元式</h2><p>op、arg1、arg2 使用运算x op y 的位置来表示计算的结果</p><h2 id="间接三元式"><a href="#间接三元式" class="headerlink" title="间接三元式"></a>间接三元式</h2><h1 id="类型和声明"><a href="#类型和声明" class="headerlink" title="类型和声明"></a>类型和声明</h1><p>类型表达式是用于表示类型的结构的，如基本类型int、char、float，</p><p>类型表达式名也是类型表达式。</p><p>类型构造算子:作用于类型表达式可以构造新的类型表达式。</p><p><strong>数组构造符array</strong></p><table><thead><tr><th align="center">类型</th><th align="center">类型表达式</th></tr></thead><tbody><tr><td align="center">int[3]</td><td align="center">array(3,int)</td></tr><tr><td align="center">int[2][3]</td><td align="center">array(2,array(3,int))</td></tr></tbody></table><p><strong>指针构造符pointer</strong></p><p><strong>笛卡尔乘积构造符x</strong></p><p><strong>函数构造符-&gt;</strong></p><p><strong>记录构造符record</strong></p><h2 id="类型检查-type-checking"><a href="#类型检查-type-checking" class="headerlink" title="类型检查 type checking"></a>类型检查 type checking</h2><p>保证参与的运算分量和运算符预期的类型相匹配。</p><p><strong>如果两个类型表达式相等，那么返回某种类型，否则出错</strong></p><h3 id="类型等价"><a href="#类型等价" class="headerlink" title="类型等价"></a>类型等价</h3><blockquote><p>两种类型之间结构等价当且仅当下面某个条件为真： </p><p>1.是相同的类型</p><p>2.是相同的类型构造算子应用于结构等价的类型而构造得到的。</p><p>3.一个类型是另一个类型表达式的名字</p></blockquote><p><strong>类型检查有两种形式：类型综合和类型推导。</strong></p><p>类型综合是根据子表达式的类型构造出表达式的类型，<strong>要求名字先声明再使用</strong>。表达式$E1+E2$的类型是根据$E1$和$E2$的类型定义的。</p><p>类型推导是根据一个语言结构的使用来确定结构的类型，就类似如果使用了某个类型才能用的函数的话，那么可以指出使用该函数的变量就是对应的类型。</p><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><p>浮点数和整型相加，编译器内部需要进行转换。</p><p>不同的语言有不同的类型转换，主要转换有两种：拓宽转换（保持信息）、窄化转换（丢失信息）。</p><h2 id="类型翻译"><a href="#类型翻译" class="headerlink" title="类型翻译"></a>类型翻译</h2><h2 id="类型的声明"><a href="#类型的声明" class="headerlink" title="类型的声明"></a>类型的声明</h2><p>语义分析在遇到声明语句时，主要做两件事情：1.收集标识符的类型等属性信息；2.为每一个名字分配一个相对地址。</p><h3 id="声明的SDT"><a href="#声明的SDT" class="headerlink" title="声明的SDT"></a>声明的SDT</h3><h2 id="表达式和赋值语句的翻译"><a href="#表达式和赋值语句的翻译" class="headerlink" title="表达式和赋值语句的翻译"></a>表达式和赋值语句的翻译</h2><h3 id="为赋值语句生成三地址码的SDD"><a href="#为赋值语句生成三地址码的SDD" class="headerlink" title="为赋值语句生成三地址码的SDD"></a>为赋值语句生成三地址码的SDD</h3><p>gen 一个函数，生成括号内代表信息的三地址码</p><table><thead><tr><th>Production</th><th>Semantic Rules</th></tr></thead><tbody><tr><td>$S\rightarrow id=E$</td><td>$S.code=E.code</td></tr><tr><td>$E\rightarrow E_1+E_2$</td><td>$E.addr=new Temp()$, $E.code=E1.code</td></tr><tr><td>$E\rightarrow -E_1$</td><td>$E.addr=new Temp()$ ,$E.code=E_1.code</td></tr><tr><td>$E\rightarrow (E_1)$</td><td>$E.addr=E1.addr$,$E.code=E_1.code$</td></tr><tr><td>$E\rightarrow id$</td><td>$E.addr=top.get(id.lexeme)$, $E.code=’’$</td></tr></tbody></table><p>将$a=b+-c;$ 编译成三地址码：</p><p>$S\Rightarrow id=E_0;$</p><p>$\Rightarrow id=E_1+E_2;$</p><p>$\Rightarrow id=E_1+-E_3;$ </p><p>$\Rightarrow id=E_1+-id;$</p><p>$\Rightarrow id=id+-id;$</p><table><thead><tr><th>产生式</th><th>属性变化</th></tr></thead><tbody><tr><td>$E_1\rightarrow id$</td><td>$E_1.addr=addr(b)$, $E_1.code=’’$</td></tr><tr><td>$E3\rightarrow id$</td><td>$E_3.addr=addr(c)$, $E_3.code=’’$</td></tr><tr><td>$E_2\rightarrow -E_3$</td><td>$E_2.addr=t1$ ,$E_2.code=E_3.code</td></tr><tr><td>$E_0\rightarrow E_1+E_2$</td><td>$E_0.addr=t2$,$E_0.code=E_1.code</td></tr><tr><td>$S\rightarrow id=E_0$</td><td>$S.code=E_0.code</td></tr></tbody></table><p>刚好三行就是赋值语句的三地址码。</p><h3 id="布尔表达式的翻译"><a href="#布尔表达式的翻译" class="headerlink" title="布尔表达式的翻译"></a>布尔表达式的翻译</h3><h4 id="短路代码"><a href="#短路代码" class="headerlink" title="短路代码"></a>短路代码</h4><p>跳转代码中&amp;&amp; || ！都被翻译成跳转指令。</p><p>语句：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(x&lt;<span class="number">100</span>||x&gt;<span class="number">200</span> &amp;&amp; x!=y)</span><br><span class="line">x=<span class="number">0</span>;</span><br></pre></td></tr></table></figure><p>三地址代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if x&lt;100 goto L2</span><br><span class="line">goto L3</span><br><span class="line">L3: if x&gt;200 goto L4</span><br><span class="line">goto L1</span><br><span class="line">L4: if x!&#x3D;y goto L2</span><br><span class="line">goto L1</span><br><span class="line">L2:x&#x3D;0</span><br><span class="line">L1:</span><br></pre></td></tr></table></figure><p>其实运算符并不在代码中，布尔表达式的值是通过代码序列中的位置来表示的。</p><h3 id="控制流语句"><a href="#控制流语句" class="headerlink" title="控制流语句"></a>控制流语句</h3><p>控制流语句：(S表示语句，B表示布尔表达式)</p><p>1.$P\rightarrow S$</p><p>2.$S\rightarrow assign$</p><p>3.$S\rightarrow if(B) S1$</p><p>4.$S\rightarrow if(B) \quad S1 \quad else \quad S2$</p><p>5.$S\rightarrow while(B)\quad S1$</p><p>6.$S\rightarrow S1 \quad S2$</p><p>SDD</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>$P\rightarrow S$</td><td>$S.next=newlable()$</td></tr><tr><td>$S\rightarrow assign$</td><td>$S.code=assign.code$</td></tr><tr><td>$S\rightarrow if(B) S1$</td><td>$B.true=newlabel()$,$B.false=S_1.next=S.next$, $S.code=B.code</td></tr><tr><td>$S\rightarrow if(B) \quad S1 \quad else \quad S2$</td><td>$B.true=newlabel()$,$B.false=newlabel()$,$S_1.next=S_2.next=S.next$,$S.code=B.code</td></tr><tr><td>$S\rightarrow while(B)\quad S1$</td><td></td></tr></tbody></table><p><strong>(1) $B\rightarrow E1 \quad rel \quad R2$ (假设形如$a&lt;b$)</strong></p><p>$B.true: if \quad a&lt;b\quad goto \quad B.true$ (j&lt;,a,b,B.true)</p><p>$B.FALSE: goto B.false$     (j,,,B.false)</p><p>(2) <strong>B是常量</strong>, 就直接翻译为跳转指令。</p><p>(3) 不需要为$B\rightarrow!B$产生新的代码，只需要将真假出口交换就可以了。(继承属性)。</p><p>(4) 对$B\rightarrow B1||B2$,</p><p>如果B1为真则B为真，B1.true从B.true继承而来，如果B1为假，则对B2求值，B1.false就可以设置为B2的代码的第一条指令的标号。B2的真假出口标号可直接从B继承获得。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;中间代码生成就是把经过语法分析和语义分析的源程序中间表示翻译为中间代码展示，中间表示可能有多个种类，如语法树、DAG、后缀式、三地址代码等。&lt;/p&gt;
&lt;p&gt;如果中间代码独立于机器的话，那么各便于编译系统的建立和移植，并且便于进行独立于机器的代码优化工作。&lt;/p&gt;
&lt;h1 i</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://codingclaire.github.io/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题4】语法制导翻译</title>
    <link href="https://codingclaire.github.io/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/"/>
    <id>https://codingclaire.github.io/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/</id>
    <published>2020-05-22T02:27:00.000Z</published>
    <updated>2020-06-14T15:45:00.324Z</updated>
    
    <content type="html"><![CDATA[<p>语法制导翻译，边做语法分析，边做语义分析。它使用CFG引导对语言的翻译，是一种面向文法的翻译技术。</p><h1 id="语义信息"><a href="#语义信息" class="headerlink" title="语义信息"></a>语义信息</h1><p><strong>如何表示语义信息？</strong></p><p>将语言结构的语义以属性(attribute)的形式赋予代表此结构的文法符号。</p><p><strong>如何计算语义属性？</strong></p><p>属性的计算以语义规则(semantic rules)的形式赋予由文法符号组成的产生式。在语法分析推导或归约的每一步骤中，通过语义规则实现对属性的计算，以达到对语义的处理。</p><p>换句话说就是：为每一个产生式配上语义规则并且在适当的时候执行这些规则。</p><h1 id="SDD-语法制导定义"><a href="#SDD-语法制导定义" class="headerlink" title="SDD 语法制导定义"></a>SDD 语法制导定义</h1><p>SDD是一个上下文无关文法和属性及规则的结合。属性和文法符号相关联，而规则和产生式相关联，有时也称为属性文法。<br>如果𝑿是一个符号，而𝒂是𝑿的一个属性，那么用𝑿.𝒂来表示在某个标号为𝑿的分析树节点上的属性值。属性可以有很多类型，比如变量的数据类型、表达式的值、变量的地址、数字的有效位数等等。</p><h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><p>属性分为综合属性和继承属性。</p><p><strong>综合属性</strong>只能由当前结点或者结点的子节点的属性值来计算。通常，产生式左侧的属性都来自右侧的话，那么左侧的属性就是综合属性。</p><p><strong>继承属性</strong>是由当前结点的父节点或兄弟节点或本身的属性值来定义的。（只要有父节点或兄弟结点定义就是继承属性了）</p><blockquote><p>终结符可以有综合属性，就是词法分析的词法值。终结符没有继承属性。</p></blockquote><p>属性文法写成表格形式，相同的非终结符需要用下标区分。</p><h2 id="S属性的SDD"><a href="#S属性的SDD" class="headerlink" title="S属性的SDD"></a>S属性的SDD</h2><p>只包含综合属性的SDD称为S属性的SDD。它可以按照任何自底向上的顺序进行求值。</p><p>L属性SDD的特例。</p><h2 id="L属性的SDD"><a href="#L属性的SDD" class="headerlink" title="L属性的SDD"></a>L属性的SDD</h2><p>要么是综合属性，要么是继承属性，且满足以下i条件：</p><p>对于产生式$A\rightarrow X_1 X_2 …X_n$,$X_i$的继承属性仅能依赖于：</p><ul><li><p>A的继承属性（如果是综合属性可能会有环路）</p></li><li><p>产生式$X_i$左侧的属性。（继承属性只能右侧的继承左侧的，规定了依赖图的边只能从左往右)</p></li></ul><h2 id="SDD的求值"><a href="#SDD的求值" class="headerlink" title="SDD的求值"></a>SDD的求值</h2><p>如果是综合属性，就可以按照任何自底向上的顺序进行求值，如果是同时具有继承属性和综合属性的话，首先要看有没有出现环状的依赖关系，最好不要出现循环的情况。</p><p>1.绘制依赖图dependency graph</p><p>2.求DAG的依赖图的拓扑排序（如果图存在环，就不存在拓扑排序）</p><p>拓扑排序不是唯一的，平行关系可以交换。</p><h1 id="SDT-语法制导的翻译方案"><a href="#SDT-语法制导的翻译方案" class="headerlink" title="SDT 语法制导的翻译方案"></a>SDT 语法制导的翻译方案</h1><p>SDT是在产生式中嵌入了程序片段的一个上下文无关文法。这些片段称为语义动作，它们可以出现在产生式的任何位置。默认用{}括起来。</p><blockquote><p>SDD时语言翻译的高层次规格说明，隐藏了很多具体实现细节，使用户不必显式地说明翻译发生的顺序。</p><p>SDT是SDD的一种补充，是SDD的具体实施方案，显式地指明了语义规则的计算顺序，以便说明某些实现细节。</p></blockquote><p>语法制导翻译可以用于抽象语法树的构建，</p><h2 id="如何用SDT实现两类重要的SDD"><a href="#如何用SDT实现两类重要的SDD" class="headerlink" title="如何用SDT实现两类重要的SDD"></a>如何用SDT实现两类重要的SDD</h2><p>产生式右侧的动作在它左边的所有文法符号后被匹配后立即执行。</p><p>将内嵌语义动作替换成一个新的非终结符，可以执行相应的语义动作。</p><h3 id="S属性的SDD-1"><a href="#S属性的SDD-1" class="headerlink" title="S属性的SDD"></a>S属性的SDD</h3><p><strong>后缀翻译方案：</strong></p><p>S属性的SDD可以构造出SDT: <strong>每个动作都放在产生式的结尾。</strong></p><p>所有属性都是综合属性。</p><h3 id="产生式内部带有语义动作的SDT"><a href="#产生式内部带有语义动作的SDT" class="headerlink" title="产生式内部带有语义动作的SDT"></a>产生式内部带有语义动作的SDT</h3><p>$B\rightarrow X{a}Y$</p><p>自底向上，X出现在分析栈栈顶时，立即执行动作a。</p><p>自顶向下，在展开Y的本次出现或者在输入中检测Y之前执行动作a。</p><h3 id="L属性的SDD-1"><a href="#L属性的SDD-1" class="headerlink" title="L属性的SDD"></a>L属性的SDD</h3><p>将计算某个非终结符号A的<strong>继承属性</strong>的动作插入到产生式<strong>右部中紧靠在A的本次出现之前的位置上</strong>。</p><p>将计算一个产生式左部符号的<strong>综合属性</strong>的动作放置在这个产生式右部的<strong>最右端</strong> <strong>。</strong></p><p><img src="/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/image-20200523200226341.png" alt="image-20200523200226341"></p><p>如果基本文法可以用LL分析，那么可以用递归下降、在LL预测分析过程中翻译(属性值存放在语法分析栈中)或者用LR分析。</p><h3 id="在递归下降分析中加入语义翻译"><a href="#在递归下降分析中加入语义翻译" class="headerlink" title="在递归下降分析中加入语义翻译"></a>在递归下降分析中加入语义翻译</h3><p>函数A的参数是非终结符A的继承属性<br>函数A的返回值是非终结符A的综合属性</p><p><img src="/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/image-20200523200609909.png" alt="image-20200523200609909"></p><p><img src="/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/image-20200523200643560.png" alt="image-20200523200643560"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;语法制导翻译，边做语法分析，边做语义分析。它使用CFG引导对语言的翻译，是一种面向文法的翻译技术。&lt;/p&gt;
&lt;h1 id=&quot;语义信息&quot;&gt;&lt;a href=&quot;#语义信息&quot; class=&quot;headerlink&quot; title=&quot;语义信息&quot;&gt;&lt;/a&gt;语义信息&lt;/h1&gt;&lt;p&gt;&lt;stro</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://codingclaire.github.io/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题3】语法分析的例子整理</title>
    <link href="https://codingclaire.github.io/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/"/>
    <id>https://codingclaire.github.io/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/</id>
    <published>2020-05-21T12:58:37.000Z</published>
    <updated>2020-06-14T15:44:54.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SLR-1"><a href="#SLR-1" class="headerlink" title="SLR(1)"></a>SLR(1)</h2><p>考虑文法：</p><p>$E\rightarrow E+T|T$</p><p>$T\rightarrow T*F|F$</p><p>$F\rightarrow (E) |id$</p><p>1.扩展文法：</p><p>$E’\rightarrow E$<br>$E\rightarrow E+T|T$<br>$T\rightarrow T*F|F$<br>$F\rightarrow (E) |id$</p><p>2.LR(0)项：</p><p><img src="/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/image-20200522080711106.png" alt="LR(0)项"></p><p>3.绘制LR(0)自动机：</p><p><img src="/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/image-20200521205727392.png" alt="LR(0)自动机"></p><p>4.由状态1、2、9可发现，这个语法有移进归约冲突，因此不是LR(0)文法，</p><p>而在状态1中，Follow(E’)={$},+不在E’的Follow集里面的，因此无歧义，在状态2和9中，Follow(E)={+,(,$},*不在E的Follow集里，也无歧义，该文法是SLR(1)文法。</p><p>5.构建SLR(1)分析表。</p><p><img src="/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/image-20200522095458808.png" alt="image-20200522095458808"></p><p>6.串(id+id)*id的分析过程:</p><table><thead><tr><th></th><th>stack</th><th>input</th><th>action</th></tr></thead><tbody><tr><td>1</td><td>$0</td><td>(id+id)*id$</td><td>S4</td></tr><tr><td>2</td><td>$0(4</td><td>id+id)*id$</td><td>S5</td></tr><tr><td>3</td><td>$0(4id5</td><td>+id)*id$</td><td>$r(F\rightarrow id)$</td></tr><tr><td>4</td><td>$0(4F3</td><td>+id)*id$</td><td>$r(T\rightarrow F)$</td></tr><tr><td>5</td><td>$0(4T2</td><td>+id)*id$</td><td>$r(E\rightarrow T)$</td></tr><tr><td>6</td><td>$0(4E8</td><td>+id)*id$</td><td>S6</td></tr><tr><td>7</td><td>$0(4E8+6</td><td>id)*id$</td><td>S5</td></tr><tr><td>8</td><td>$0(4E8+6id5</td><td>)*id$</td><td>$r(F\rightarrow id)$</td></tr><tr><td>9</td><td>$0(4E8+6F3</td><td>)*id$</td><td>$r(T\rightarrow F)$</td></tr><tr><td>10</td><td>$0(4E8+6T9</td><td>)*id$</td><td>$r(E\rightarrow E+T)$</td></tr><tr><td>11</td><td>$0(4E8</td><td>)*id$</td><td>S11</td></tr><tr><td>12</td><td>$0(4E8)11</td><td>*id$</td><td>$r(F\rightarrow (E))$</td></tr><tr><td>13</td><td>$0F3</td><td>*id$</td><td>$r(T\rightarrow F)$</td></tr><tr><td>14</td><td>$0T2</td><td>*id$</td><td>S7</td></tr><tr><td>15</td><td>$0T2*7</td><td>id$</td><td>S5</td></tr><tr><td>16</td><td>$0T2*7id5</td><td>$</td><td>$r(F\rightarrow id)$</td></tr><tr><td>17</td><td>$0T2*7F10</td><td>$</td><td>$r(T\rightarrow  T*F)$</td></tr><tr><td>18</td><td>$0T2</td><td>$</td><td>$r(E\rightarrow T)$</td></tr><tr><td>19</td><td>$0E1</td><td>$</td><td>accept</td></tr></tbody></table><p>因此该串被接受。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SLR-1&quot;&gt;&lt;a href=&quot;#SLR-1&quot; class=&quot;headerlink&quot; title=&quot;SLR(1)&quot;&gt;&lt;/a&gt;SLR(1)&lt;/h2&gt;&lt;p&gt;考虑文法：&lt;/p&gt;
&lt;p&gt;$E\rightarrow E+T|T$&lt;/p&gt;
&lt;p&gt;$T\rightarrow T</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://codingclaire.github.io/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题2】关于语法分析</title>
    <link href="https://codingclaire.github.io/2020/05/21/%E5%85%B3%E4%BA%8E%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>https://codingclaire.github.io/2020/05/21/%E5%85%B3%E4%BA%8E%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90/</id>
    <published>2020-05-21T02:07:43.000Z</published>
    <updated>2020-06-14T15:44:35.422Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这将是一个非常口语化的总结，因为这就是我口述的总结。</p></blockquote><p>语法分析过程主要包括两种方法：自底向上的语法分析和自顶向下的语法分析。其中，“底”指的就是原始串，而“顶”指的是开始符号。分析的目的就是确定某一个确定的字符串是否属于文法描述的语言。</p><p>而这些分析方法，最终都是要让串形成对应的语法分析树，因此它们将一个判定问题，转化成了生成语法分析树的过程。</p><h1 id="First集和Follow集"><a href="#First集和Follow集" class="headerlink" title="First集和Follow集"></a>First集和Follow集</h1><p>First集和Follow集应该是对于任意的文法都是能够确定的。</p><p>文法中的任意文法符号串都是有First集的，First集相当于这个文法符号串能推出的串中最左侧的终结符的集合。First集可以包含$\epsilon$。</p><blockquote><p>求First集的规则：</p><ol><li><p>把所有的终结符语法规则列出来（我感觉First集求的时候不能有或？还是也可以）</p></li><li><p>如果X是终结符或者$\epsilon$,$First(X)={X}$</p><p>如果X是非终结符，对每个产生式$X-&gt;X_1X_2…X_n$,$First(X_1)$是$First(X)$的子集。</p><p>如果有$X_1X_2…X_i\Rightarrow\epsilon(i&lt;n)$,那么$First(X_{i+1})$是$First(X)$的子集。</p></li></ol></blockquote><p>Follow集能够让一个非终结符消失（推出空），就是说Follow是确定当某一个非终结符后面出现了哪些终结符的时候，我们需要用推出空这个产生式。</p><blockquote><p>求Follow集的规则：</p><p>1.先将$放入Follow(S)中，S为开始字符。(构建LL(1)分析表的时候，如果有$S\Rightarrow\epsilon$,那么就可以写在[S,$]里，表示如果接受的是一个空串，就可以用这个产生式)</p><p>2.如果存在产生式$A\rightarrow\alpha B\beta$,那么求解Follow(B)的时候，要将$First(\beta)$中除了$\epsilon$所有的元素都加入Follow(B)。$\beta$可包含终结符或非终结符。</p><p>3.产生式右侧被推导出之后，左侧的Follow集就是右侧最右（需要考虑右侧是否为空，若为空就不断考虑向左移动的符号）的非终结符的Follow集的子集。</p><p>【如果存在产生式$A\rightarrow\alpha B\beta$,且$\beta$可空（或者说B的First集包含$\epsilon$)，那么$Follow(B)\Leftarrow Follow(A)$】</p></blockquote><p>因为我们确定某个非终结符的Follow集，都是通过它在右侧才能确定的，因此我们不需要考虑那些右侧全是终结符的产生式。</p><p>LL(1)分析表做的是这件事：横轴是预测的下一个字符，然后当前的栈顶的非终结符已知，那么要通过哪一个产生式能够最终推出预测的下一个字符。所以我们需要通过计算First集和Follow集来确定LL(1)分析表。</p><h1 id="自顶向下的语法分析"><a href="#自顶向下的语法分析" class="headerlink" title="自顶向下的语法分析"></a>自顶向下的语法分析</h1><p>从开始符号最终到实际的字符串，<strong>自顶向下</strong>中主要分为<strong>回溯分析程序</strong>和<strong>预测分析程序</strong>。我们主要学了两种预测分析方法：<strong>递归下降和LL(1)</strong>。</p><p><strong>为何叫“预测分析”，</strong>我们可以这么理解：首先，自顶向下分析方法的基础就是将字符串看成输入串，就是说从开始到结束，我们可以认为是逐步读取这个串的，因此字符之间有了先后被读取的，那么我们构建语法分析树也就是一个先根次序创建树的过程，我们也可以说<strong>自顶向下分析就是要找到对应串的最左推导</strong>。因此预测分析首先是要求给定的文法中没有左因子、左递归，文法不能是二义性的，其次预测分析需要看文法的下一个字符，也就是<strong>下一个输出符号</strong>，所以我们称之为“预测”。</p><h2 id="回溯分析程序"><a href="#回溯分析程序" class="headerlink" title="回溯分析程序"></a>回溯分析程序</h2><h2 id="预测分析程序"><a href="#预测分析程序" class="headerlink" title="预测分析程序"></a>预测分析程序</h2><h3 id="递归下降"><a href="#递归下降" class="headerlink" title="递归下降"></a>递归下降</h3><p>改写为$EBNF$(消除左递归、去除左因子)</p><h3 id="LL-1-分析算法"><a href="#LL-1-分析算法" class="headerlink" title="LL(1)分析算法"></a>LL(1)分析算法</h3><p>第一个L表示从左向右扫描输入，第二个L表示最左推导，1表示每一步中只需要向前看一个输入符号来决定语法分析动作。</p><h4 id="预测分析表的构建"><a href="#预测分析表的构建" class="headerlink" title="预测分析表的构建"></a>预测分析表的构建</h4><blockquote><p>$LL(1)$构建预测分析表的步骤：</p><ol><li>$First(\alpha)$中的每个记号$s$，都将$A\rightarrow\alpha$添加至$M[A,s]$中。</li><li>$\alpha$可空的话，对$Follow(A)$中的每一个元素$k$，将$A\rightarrow\alpha$添加到$M[A,k]$中。</li></ol><p>如果$M[A,\alpha]$没有产生式的话，就将其设置为$error$。</p></blockquote><h4 id="LL-1-文法"><a href="#LL-1-文法" class="headerlink" title="LL(1)文法"></a>LL(1)文法</h4><p>一个文法若满足以下条件，则该文法就是LL(1)文法：</p><p>在每个产生式$A\rightarrow{\alpha}_1 |{\alpha}_2⋯|{\alpha}_n$中，对于所有的i和j:$1≤i, j≤n, i≠j$，$First(α_i )∩First(α_j )$为空。（若不为空，假设有一个相同元素$k$,那么在$M[A,k]$就会加入两个产生式：$A\rightarrow{\alpha}_i$和$A\rightarrow{\alpha}_j$)</p><p>若对于非终结符A可空，那么$First(A)∩Follow(A)$为空。(若有相同元素k，根据分析表也会发现$M[A,k]$有两个产生式)</p><p><strong>如果一个文法G，由它构造的LL(1)分析表中的每个子项最多只含有一个产生式，那么它就是LL(1)文法。</strong></p><p>在LL(1)分析表中有两项产生式的文法不一定是二义性的文法，可能是有左递归的。</p><blockquote><p>一个不是$LL(1)$的文法同样可以用$LL(1)$方法。</p></blockquote><p>LL(1）方法对应的是非递归的预测分析器，显示维护栈结构，应该和计算理论里的下推自动机类似。下推自动机所定义的语言恰好就是上下文无关语言。</p><h1 id="自底向上的语法分析"><a href="#自底向上的语法分析" class="headerlink" title="自底向上的语法分析"></a>自底向上的语法分析</h1><p>归约其实就是推导的反向操作。如果反向构造一个推导过程，那么就会是最右推导的。推导的方法是从记号串开始，使用产生式进行归约，期望得到开始符号，如果能够得到开始符号，那么这个字符串就是文法可以识别的语句。</p><p>两个动作：移进 shift和规约 reduce。</p><p>自底向下就是从输入串到开始符号的归约，归约的方向是从左到右，可以认为是最左归约，逆向的过程就是最右推导。</p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="短语、直接短语和句柄"><a href="#短语、直接短语和句柄" class="headerlink" title="短语、直接短语和句柄"></a>短语、直接短语和句柄</h3><p>短语就是在一个句型中对应的分析树，里以非终结符为根的子树的所有叶子节点构成的排列就是对于该非终结符的短语，如果子树只有两层，那么就是直接短语。最左侧的非终结符的子树对应的短语就是句柄。</p><p>句柄的定义：如果$S\Rightarrow_{lm}^{*}\alpha A\omega \Rightarrow_{lm} \alpha \beta \omega$，A是输入串中最右的非终结符，则$\beta$称为一个句柄。</p><p>句柄可以理解为一个归约点，可以允许解析器通过进一步的归约操作回到开始符号的位置。而实际上我们做的归约就是最左归约。</p><p>对于下列文法：</p><p>$E\rightarrow E+T|T$<br>$T\rightarrow T*F|F$<br>$F\rightarrow (E) |id$</p><p>对于输入串$id*id$，从左到右相当于一个最左归约的过程。从左至右：</p><table><thead><tr><th>产生式</th><th>句柄</th><th>最右句型</th></tr></thead><tbody><tr><td>$F\rightarrow id$</td><td>$id$</td><td>id*id</td></tr><tr><td>$T\rightarrow F$</td><td>$F$</td><td>F*id</td></tr><tr><td>$F\rightarrow id $</td><td>$id$</td><td>T*id</td></tr><tr><td>$T\rightarrow T*F$</td><td>$T*F$</td><td>T*F</td></tr><tr><td>$E\rightarrow T$</td><td>$T$</td><td>T</td></tr></tbody></table><p><img src="/2020/05/21/%E5%85%B3%E4%BA%8E%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90/image-20200521195029382.png" alt="image-20200521195029382"></p><h3 id="可行前缀"><a href="#可行前缀" class="headerlink" title="可行前缀"></a>可行前缀</h3><h2 id="LR-0-分析算法"><a href="#LR-0-分析算法" class="headerlink" title="LR(0)分析算法"></a>LR(0)分析算法</h2><p><strong>LR(0)文法中L指的是从左到右扫描输入串，R代表了最右推导，0表示进行分析动作的决策只考虑栈顶状态，不需要看输入串。（没有lookahead)</strong></p><p><strong>1.扩展文法。</strong></p><p>在决定状态间的转移前，我们必须先加入一条扩展文法：$S\rightarrow E$其中$S$是新的起始符号（start symbol）而<em>E</em>是原先的起始符号。这一做法是为了保证分析器能有一个唯一的起始状态。</p><p><strong>2.列LR(0)项。</strong>(点号的左侧是已经读入的，点号的剩余是还没有读入的)</p><p><strong>3.起始状态是所有点在最左侧的LR(0)项组成的封闭集,构建LR(0)自动机</strong></p><p><strong>4.构建LR(0)分析表。</strong></p><p><strong>5.进行分析。</strong></p><p>如果X是终结符，只要有移进项先移进。</p><h4 id="LR-0-文法"><a href="#LR-0-文法" class="headerlink" title="LR(0)文法"></a>LR(0)文法</h4><p>无歧义需要没有归约归约冲突或移进归约冲突。</p><h2 id="SLR-1-分析算法"><a href="#SLR-1-分析算法" class="headerlink" title="SLR(1)分析算法"></a>SLR(1)分析算法</h2><p>如果当前栈顶状态可以支持终结符移进，并且<strong>下一个记号也就是该终结符</strong>，才会移进。如果当前栈顶状态包含了归约项$A\rightarrow\gamma.$，且<strong>下一个记号在$Follow（A)$</strong>时，才会使用$A\rightarrow\gamma$归约，如果不在$Follow(A)$也不会做归约。$GOTO$项与LR(0)类似。</p><blockquote><p>歧义的产生：</p><p>1)有归约项和移进项，且移进项$A\rightarrow \alpha . X \beta$的下一个字符$X$在$Follow（B）$中,当然如果下一个记号不是$X$那么就没有歧义了。</p><p>2)有两个不同的归约项$A\rightarrow\beta.$，$B\rightarrow\gamma.$，且下一个记号即在A的Follow集也在B的Follow集，或者两个Follow集都没有$X$,此时要报错。</p></blockquote><p>当确认没有歧义的时候，归约项$r(A\rightarrow \gamma)$就会被填入A的Follow集对应的Input下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;这将是一个非常口语化的总结，因为这就是我口述的总结。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;语法分析过程主要包括两种方法：自底向上的语法分析和自顶向下的语法分析。其中，“底”指的就是原始串，而“顶”指的是开始符号。分析的目的就是确定某一个确定的</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://codingclaire.github.io/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题1】上下文无关文法和正则表达式</title>
    <link href="https://codingclaire.github.io/2020/05/21/%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>https://codingclaire.github.io/2020/05/21/%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</id>
    <published>2020-05-21T01:35:48.000Z</published>
    <updated>2020-06-14T16:30:45.896Z</updated>
    
    <content type="html"><![CDATA[<p>扫描器是词法分析器，它接收输入的源程序，对源程序进行词法分析并识别出一个个单词符号，输出单词符号。</p><h1 id="上下文无关语言"><a href="#上下文无关语言" class="headerlink" title="上下文无关语言"></a>上下文无关语言</h1><p>表示上下文无关文法规则的形式被称为BNF，其扩展表示就是EBNF。</p><p>在BNF中，重复是使用递归表示的，重复实际分两种：嵌套重复和并列重复，并列重复对应到程序是可以用循环来实现的。</p><h2 id="EBNF"><a href="#EBNF" class="headerlink" title="EBNF"></a>EBNF</h2><p><strong>重复表示{…}</strong></p><blockquote><p>下面两种重复的递归形式表达的就是并列重复：</p><p>$A\rightarrow A \alpha|\beta$</p><p>$A\rightarrow \alpha A |\beta$</p><p>其中第一条中要求$\beta$ 不能以A开头， 而第二条中要求$\beta$不能以A结尾。对应的正则表达式为：$\beta \alpha^<em>$和$\alpha^</em>\beta$<br>EBNF中使用{…}来表示这种重复：<br>$A\rightarrow \beta {\alpha}$</p><p>$A\rightarrow{\alpha} \beta$</p></blockquote><p><strong>可选表示[…]</strong> 有点类似消除左因子。</p><blockquote><p>语句序列：<br>$stmt-sequence \rightarrow stmt; stmt-sequence | stmt$<br>可以表示为：<br>$stmt-sequence \rightarrow  stmt [ ; stmt-sequence ]$<br>$stmt-sequence \rightarrow  stmt { ; stmt }  $       </p></blockquote><h2 id="消除左递归"><a href="#消除左递归" class="headerlink" title="消除左递归"></a>消除左递归</h2><h3 id="直接简单左递归"><a href="#直接简单左递归" class="headerlink" title="直接简单左递归"></a>直接简单左递归</h3><p>$A\rightarrow A \alpha |\beta$</p><p>改写文法为：</p><p>$A\rightarrow \beta A’$</p><p>$A’\rightarrow \alpha A’ |\epsilon$</p><h3 id="间接左递归"><a href="#间接左递归" class="headerlink" title="间接左递归"></a>间接左递归</h3><p>会出现$A\Rightarrow^*A$的左递归。</p><p>处理方法：</p><p>将文法的所有非终结符按任意一种顺序排序，得到$A_1,A_2…A_n$</p><p>对每个$A_i$，如果存在一个编号比它小的非终结符，编号大的非终结符可以含有推出编号小的非终结符的句型，而且编号小的非终结符还能够推出一个句型，那么就可以进行代入操作。<br>如果有直接左递归，那么直接消除即可。</p><blockquote><p>$S\rightarrow Qc|c$</p><p>$Q\rightarrow Rb|b$</p><p>$R\rightarrow Sa|a$</p><p>1) 对S、Q、R编号1、2、3</p><p>2）i=1，无法代入，i=2，无法代入</p><p>i=3, 代入有 $R\rightarrow Qca|ca|a$,可再次代入：$R\rightarrow Rbca|bca|ca|a$</p><p>3)化简直接左递归：</p><p>$R\rightarrow bcaR’|caR’|aR’$</p><p>$R\rightarrow bcaR’|\epsilon$</p></blockquote><h2 id="消除左公因子"><a href="#消除左公因子" class="headerlink" title="消除左公因子"></a><strong>消除左公因子</strong></h2><p>对每个非终结符A，找出它的两个或多个选项之间的最长公共前缀$\alpha$,如果$\alpha$不为空，即存在一个非平凡的公共前缀，那么将所有A的产生式$A\rightarrow \alpha\beta_1|\alpha\beta_2|…\alpha\beta_n|\gamma$,</p><p>替换为：</p><p>$A\rightarrow \alpha A’|\gamma$</p><p>$A’\rightarrow \beta_1|\beta_2|…|\beta_n$</p><h2 id="递归构造上下文无关文法"><a href="#递归构造上下文无关文法" class="headerlink" title="递归构造上下文无关文法"></a>递归构造上下文无关文法</h2><p>左递归：左侧非终结符出现在右侧第一个位置。<br>$A\rightarrow A a | a$<br>右递归：左侧非终结符出现在右侧最后一个位置<br>$A \rightarrow a A | a$</p><blockquote><p>表示$\alpha\beta^*\gamma$一样的语言：</p><p>1）$A\rightarrow B\gamma$ ,$B\rightarrow B\beta|\alpha$</p><p>2) $A\rightarrow \alpha B$ ,$B\rightarrow \beta B|\gamma$</p><p>3)$A\rightarrow\alpha B\gamma$,$B\rightarrow\beta B| \epsilon$</p></blockquote><hr><p>所有的正则语言都能被上下文无关文法表示。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;扫描器是词法分析器，它接收输入的源程序，对源程序进行词法分析并识别出一个个单词符号，输出单词符号。&lt;/p&gt;
&lt;h1 id=&quot;上下文无关语言&quot;&gt;&lt;a href=&quot;#上下文无关语言&quot; class=&quot;headerlink&quot; title=&quot;上下文无关语言&quot;&gt;&lt;/a&gt;上下文无关语言&lt;</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://codingclaire.github.io/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
</feed>

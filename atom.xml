<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ruoting Wu&#39;s Blog</title>
  
  
  <link href="https://wuruoting.club/atom.xml" rel="self"/>
  
  <link href="https://wuruoting.club/"/>
  <updated>2021-05-11T13:06:22.549Z</updated>
  <id>https://wuruoting.club/</id>
  
  <author>
    <name>Ruoting Wu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【异常检测1】基本概念</title>
    <link href="https://wuruoting.club/2021/05/11/2021-05-11-anomaly-detection-1/"/>
    <id>https://wuruoting.club/2021/05/11/2021-05-11-anomaly-detection-1/</id>
    <published>2021-05-11T11:46:14.000Z</published>
    <updated>2021-05-11T13:06:22.549Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%B8%80%E3%80%81%E6%A6%82%E8%BF%B0.md" target="_blank" rel="noopener">DataWhale组队学习活动</a></p></blockquote><h2 id="什么是异常检测"><a href="#什么是异常检测" class="headerlink" title="什么是异常检测?"></a>什么是异常检测?</h2><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p>在统计学中，离群值（Outliers）是与其他观测值显著不同的数据点，也成为异常点。异常点的出现可能是因为观测的可变性或实验的错误。异常是指在大量的数据中较为稀少的数据点、事件或者行为。异常通常与正常数据不同，通常代表着数据中出现的一些问题，如欺诈行为、网络、文字的错误等。异常也被成为噪音、偏差和异常。异常可以被分为三类：点异常、条件异常和群体异常。</p><h3 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h3><p>异常检测是在数据集中找到非正常的数据、条件或群体。找到异常节点面临着一些挑战，如用于异常检测的数据集有样本类别不均衡的问题，还有异常节点是不规则的，不同的异常可能表现上完全不同。</p><h2 id="异常检测的方法有哪些？"><a href="#异常检测的方法有哪些？" class="headerlink" title="异常检测的方法有哪些？"></a>异常检测的方法有哪些？</h2><h3 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h3><p>有监督学习方法适用于有标签的数据集，也就是说在训练过程中会知道数据是否是异常的，这就相当于一个分类问题。可以用很多基础的机器学习分类算法进行检测，如SVM，决策树、GBDT、XGBoost等进行分类。但值得注意的是，包含异常的数据集是不均衡的，因此可能会影响性能。可以考虑使用集成学习的方法，如feature bagging。</p><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>无监督学习方法适用于没有标签的训练集，可以对数据集进行聚类。用于聚类的方法可以运用在异常检测的场景中，如DBSCAN算法、KNN算法、LOF(local outlier factor)算法等。但无监督学习聚类的方法，有时候会面临维度灾难，可能相似性的度量在高维数据重失效。</p><h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><p>半监督学习的异常检测指的是在训练集中只有正常的数据集，没有异常的实例参与训练。然后会构造出一个表示正常行为的模型，然后会测试该模型生成的实例的可能性。比较适用于数据的标签不足的时候。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://gitee.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/%E4%B8%80%E3%80%81%E6%A6%82</summary>
      
    
    
    
    
    <category term="异常检测" scheme="https://wuruoting.club/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>如何读取HDF5保存的权重</title>
    <link href="https://wuruoting.club/2021/04/20/2021-04-20-hdf5-weights/"/>
    <id>https://wuruoting.club/2021/04/20/2021-04-20-hdf5-weights/</id>
    <published>2021-04-20T11:23:37.000Z</published>
    <updated>2021-05-11T11:50:57.200Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://keras.io/api/layers/base_layer/#set_weights-method" target="_blank" rel="noopener">https://keras.io/api/layers/base_layer/#set_weights-method</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(<span class="string">"encoder_weights_0.hdf5"</span>, <span class="string">"r"</span>)</span><br><span class="line">print(f.filename, <span class="string">":"</span>)</span><br><span class="line">print(f[<span class="string">'dense_1'</span>])</span><br><span class="line">print([key <span class="keyword">for</span> key <span class="keyword">in</span> f.keys()], <span class="string">"\n"</span>)</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> f.keys():</span><br><span class="line">    print(key,f[key])</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> f[key].keys():</span><br><span class="line">        print(k,f[key][k])</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> f[key][k].keys():</span><br><span class="line">            print(l, f[key][k][l])</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;HDF5 group "/dense_1" (1 members)&gt;</span><br><span class="line">['dense_1', 'dense_2', 'dense_3', 'input_1']</span><br><span class="line"></span><br><span class="line">dense_1 &lt;HDF5 group "/dense_1" (1 members)&gt;</span><br><span class="line">dense_1 &lt;HDF5 group "/dense_1/dense_1" (2 members)&gt;</span><br><span class="line">bias:0 &lt;HDF5 dataset "bias:0": shape (64,), type "&lt;f4"&gt;</span><br><span class="line">kernel:0 &lt;HDF5 dataset "kernel:0": shape (100, 64), type "&lt;f4"&gt;</span><br><span class="line">dense_2 &lt;HDF5 group "/dense_2" (1 members)&gt;</span><br><span class="line">dense_2 &lt;HDF5 group "/dense_2/dense_2" (2 members)&gt;</span><br><span class="line">bias:0 &lt;HDF5 dataset "bias:0": shape (16,), type "&lt;f4"&gt;</span><br><span class="line">kernel:0 &lt;HDF5 dataset "kernel:0": shape (64, 16), type "&lt;f4"&gt;</span><br><span class="line">dense_3 &lt;HDF5 group "/dense_3" (1 members)&gt;</span><br><span class="line">dense_3 &lt;HDF5 group "/dense_3/dense_3" (2 members)&gt;</span><br><span class="line">bias:0 &lt;HDF5 dataset "bias:0": shape (8,), type "&lt;f4"&gt;</span><br><span class="line">kernel:0 &lt;HDF5 dataset "kernel:0": shape (16, 8), type "&lt;f4"&gt;</span><br><span class="line">input_1 &lt;HDF5 group "/input_1" (0 members)&gt;</span><br></pre></td></tr></table></figure><p>a Dense layer returns a list of two values– per-output weights and the bias value.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://keras.io/api/layers/base_layer/#set_weights-method&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://keras.io/api/layers/base_laye</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Boosting提升</title>
    <link href="https://wuruoting.club/2021/04/14/2021-04-14-boosting/"/>
    <id>https://wuruoting.club/2021/04/14/2021-04-14-boosting/</id>
    <published>2021-04-14T01:56:20.000Z</published>
    <updated>2021-05-11T12:26:01.651Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Boosting"><a href="#什么是Boosting" class="headerlink" title="什么是Boosting"></a>什么是Boosting</h2><p>TODO</p><h2 id="Boosting的两种方法"><a href="#Boosting的两种方法" class="headerlink" title="Boosting的两种方法"></a>Boosting的两种方法</h2><h3 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h3><h3 id="Gradient-Boost"><a href="#Gradient-Boost" class="headerlink" title="Gradient Boost"></a>Gradient Boost</h3><h2 id="Gradient-Boost-Decision-Tree-GBDT"><a href="#Gradient-Boost-Decision-Tree-GBDT" class="headerlink" title="Gradient Boost Decision Tree(GBDT)"></a>Gradient Boost Decision Tree(GBDT)</h2><p>在每个树节点中找到最佳分割点非常耗时，而且会消耗内存</p><h2 id="Boosting框架"><a href="#Boosting框架" class="headerlink" title="Boosting框架"></a>Boosting框架</h2><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><h3 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h3><ul><li><p>基于histogram</p></li><li><p>leaf-wise</p></li></ul><p><img src="/2021/04/14/2021-04-14-boosting/image-20210414095754315.png" alt="image-20210414095754315"></p><p><img src="/2021/04/14/2021-04-14-boosting/image-20210414095812474.png" alt="image-20210414095812474"></p><p><img src="/2021/04/14/2021-04-14-boosting/image-20210414095908847.png" alt="image-20210414095908847"></p><p>直方图优化：基于分桶，减少内存的使用，正则化不容易overfit</p><p>控制max_depth来控制num_leaves</p><p>num_leaves=2^max_depth</p><p>lightGBM控制num_leaves，而不是树的最大深度，因为lightGBM不会生成满二叉树，因此通过控制num_leaves确保树的深度不过大，防止过拟合。</p><h4 id="防止过拟合的方法"><a href="#防止过拟合的方法" class="headerlink" title="防止过拟合的方法"></a>防止过拟合的方法</h4><p><img src="/2021/04/14/2021-04-14-boosting/image-20210414101304982.png" alt="image-20210414101304982"></p><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><h5 id="num-leaves"><a href="#num-leaves" class="headerlink" title="num_leaves"></a>num_leaves</h5><p>num_leaves越大，增加了训练集的精确度，但增加了过拟合的几率</p><h5 id="num-iterations"><a href="#num-iterations" class="headerlink" title="num_iterations"></a>num_iterations</h5><hr><p>参考资料：</p><p>1.微软亚洲研究院AI头条分享-<a href="https%3A//v.qq.com/x/page/k0362z6lqix.html">Introduction to LightGBM</a>-Taifeng Wang</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是Boosting&quot;&gt;&lt;a href=&quot;#什么是Boosting&quot; class=&quot;headerlink&quot; title=&quot;什么是Boosting&quot;&gt;&lt;/a&gt;什么是Boosting&lt;/h2&gt;&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&quot;Boosting的两种方法&quot;&gt;&lt;a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>TensorBoard相关知识</title>
    <link href="https://wuruoting.club/2021/04/13/2021-04-13-Tensorboard/"/>
    <id>https://wuruoting.club/2021/04/13/2021-04-13-Tensorboard/</id>
    <published>2021-04-13T03:48:30.000Z</published>
    <updated>2021-04-13T03:49:37.963Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TensorBoard查看结果"><a href="#TensorBoard查看结果" class="headerlink" title="TensorBoard查看结果"></a>TensorBoard查看结果</h3><p><code>tensorboard –logdir /path/to/logs</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;TensorBoard查看结果&quot;&gt;&lt;a href=&quot;#TensorBoard查看结果&quot; class=&quot;headerlink&quot; title=&quot;TensorBoard查看结果&quot;&gt;&lt;/a&gt;TensorBoard查看结果&lt;/h3&gt;&lt;p&gt;&lt;code&gt;tensorboard </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>图神经网络里的监督/半监督和转导/归纳概念解析</title>
    <link href="https://wuruoting.club/2021/04/09/2021-04-09-GNN-classification/"/>
    <id>https://wuruoting.club/2021/04/09/2021-04-09-GNN-classification/</id>
    <published>2021-04-09T13:23:45.000Z</published>
    <updated>2021-04-09T13:40:52.261Z</updated>
    
    <content type="html"><![CDATA[<p>监督(supervised)和半监督(semi-supervised)其实是转导（transductive)和归纳(inductive)</p><p>对于节点分类来说，如何区分监督学习和半监督学习的关键在于节点在训练过程中被使用的区别，也就是训练集节点是否被用在GNN的消息传递操作中，并且会被用于计算损失。</p><p>transductive的测试节点是无标签的，并且不会被使用在损失计算中，但是这些点和相关的边会被用于消息传递中，也就是说图神经网络会生成测试集节点的潜在表示中，但是最后一层的表示不会被用在损失函数的计算中。</p><p>indutive的测试节点既不会用在GNN的消息传递过程中，也不会用在损失函数计算中，也就是说，inductive在GNN训练的过程中完全不会被包括。</p><p>半监督指的就是GNN会使用transductive的测试节点组成的测试集，也就是说在训练的过程中实际上是能够看到测试的节点的（但不能看到label）。监督指的就是在做归纳式的节点分类时，测试的节点是完全不会被检测到的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;监督(supervised)和半监督(semi-supervised)其实是转导（transductive)和归纳(inductive)&lt;/p&gt;
&lt;p&gt;对于节点分类来说，如何区分监督学习和半监督学习的关键在于节点在训练过程中被使用的区别，也就是训练集节点是否被用在GNN的消</summary>
      
    
    
    
    
    <category term="图" scheme="https://wuruoting.club/tags/%E5%9B%BE/"/>
    
    <category term="机器学习" scheme="https://wuruoting.club/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Keras的一些基本操作</title>
    <link href="https://wuruoting.club/2021/04/08/2021-04-08-keras/"/>
    <id>https://wuruoting.club/2021/04/08/2021-04-08-keras/</id>
    <published>2021-04-08T02:47:25.000Z</published>
    <updated>2021-05-11T12:17:11.949Z</updated>
    
    <content type="html"><![CDATA[<h2 id="生成模型的关键步骤"><a href="#生成模型的关键步骤" class="headerlink" title="生成模型的关键步骤"></a>生成模型的关键步骤</h2><h3 id="model-add"><a href="#model-add" class="headerlink" title="model.add()"></a><code>model.add()</code></h3><h3 id="model-summary"><a href="#model-summary" class="headerlink" title="model.summary()"></a><code>model.summary()</code></h3><p><code>model.summary() #打印神经网络结构，统计参数数目</code></p><h3 id="model-compile"><a href="#model-compile" class="headerlink" title="model.compile()"></a><code>model.compile()</code></h3><p>在配置训练方法时，告知训练时用的优化器、损失函数和准确率评测标准</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer = 优化器</span><br><span class="line">              loss = 损失函数，</span><br><span class="line">              metrics = [<span class="string">"准确率"</span>])</span><br></pre></td></tr></table></figure><h3 id="model-fit"><a href="#model-fit" class="headerlink" title="model.fit()"></a><code>model.fit()</code></h3><p>The history object returned by model.fit() is a simple class with some fields, e.g. a reference to the model, a params dict and, most importantly, a history dict. It stores the values of loss and acc (or any other used metric) at the end of each epoch.</p><h2 id="模型的保存"><a href="#模型的保存" class="headerlink" title="模型的保存"></a>模型的保存</h2><h3 id="保存模型参数：model-to-json"><a href="#保存模型参数：model-to-json" class="headerlink" title="保存模型参数：model.to_json()"></a>保存模型参数：<code>model.to_json()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"model.json"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">        json_file.write(model_json)</span><br></pre></td></tr></table></figure><h3 id="保存-weights-model-save-weights"><a href="#保存-weights-model-save-weights" class="headerlink" title="保存 weights:model.save_weights()"></a>保存 weights:<code>model.save_weights()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_weights(<span class="string">"model.h5"</span>)</span><br></pre></td></tr></table></figure><p>.h5 文件</p><h3 id="保存某一层的输出：model-layers-index-output"><a href="#保存某一层的输出：model-layers-index-output" class="headerlink" title="保存某一层的输出：model.layers[index].output"></a>保存某一层的输出：<code>model.layers[index].output</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inp = model.input</span><br><span class="line">outputs = [layer.output <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers]</span><br></pre></td></tr></table></figure><p>outputs 里的元素的类型是：<br><code>&lt;class &#39;tensorflow.python.framework.ops.Tensor&#39;&gt;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># serialize model to JSON</span></span><br><span class="line">model_json = model.to_json()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"model.json"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json_file.write(model_json)</span><br><span class="line"><span class="comment"># serialize weights to HDF5</span></span><br><span class="line">model.save_weights(<span class="string">"model.h5"</span>)</span><br><span class="line">print(<span class="string">"Saved model to disk"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># later...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load json and create model</span></span><br><span class="line">json_file = open(<span class="string">'model.json'</span>, <span class="string">'r'</span>)</span><br><span class="line">loaded_model_json = json_file.read()</span><br><span class="line">json_file.close()</span><br><span class="line">loaded_model = model_from_json(loaded_model_json)</span><br><span class="line"><span class="comment"># load weights into new model</span></span><br><span class="line">loaded_model.load_weights(<span class="string">"model.h5"</span>)</span><br><span class="line">print(<span class="string">"Loaded model from disk"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate loaded model on test data</span></span><br><span class="line">loaded_model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">score = loaded_model.evaluate(X, Y, verbose=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">"%s: %.2f%%"</span> % (loaded_model.metrics_names[<span class="number">1</span>], score[<span class="number">1</span>]*<span class="number">100</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights_0_list = new_model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(weights_0_list)):</span><br><span class="line">    print(weights_0_list[i].shape)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;生成模型的关键步骤&quot;&gt;&lt;a href=&quot;#生成模型的关键步骤&quot; class=&quot;headerlink&quot; title=&quot;生成模型的关键步骤&quot;&gt;&lt;/a&gt;生成模型的关键步骤&lt;/h2&gt;&lt;h3 id=&quot;model-add&quot;&gt;&lt;a href=&quot;#model-add&quot; class</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Keras中RNNLayer的输入输出总结</title>
    <link href="https://wuruoting.club/2021/04/07/2021-04-07-keras-LSTM/"/>
    <id>https://wuruoting.club/2021/04/07/2021-04-07-keras-LSTM/</id>
    <published>2021-04-07T14:00:24.000Z</published>
    <updated>2021-04-12T03:48:33.407Z</updated>
    
    <content type="html"><![CDATA[<p>网上关于 Keras 的 RNNLayer 中的输入写的很不清楚，整理如下：</p><h2 id="LSTM-的输入"><a href="#LSTM-的输入" class="headerlink" title="LSTM 的输入"></a>LSTM 的输入</h2><h3 id="tf-keras-layers-LSTM-参数"><a href="#tf-keras-layers-LSTM-参数" class="headerlink" title="tf.keras.layers.LSTM()参数"></a><code>tf.keras.layers.LSTM()</code>参数</h3><p><a href="https://keras.io/api/layers/recurrent_layers/lstm/" target="_blank" rel="noopener">文档</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.LSTM(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'sigmoid'</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, recurrent_regularizer=<span class="literal">None</span>, bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>, kernel_constraint=<span class="literal">None</span>, recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>, dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">    return_sequences=<span class="literal">False</span>, return_state=<span class="literal">False</span>, go_backwards=<span class="literal">False</span>, stateful=<span class="literal">False</span>,</span><br><span class="line">    time_major=<span class="literal">False</span>, unroll=<span class="literal">False</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="input-dim、input-length、input-shape-的关系"><a href="#input-dim、input-length、input-shape-的关系" class="headerlink" title="input_dim、input_length、input_shape 的关系"></a>input_dim、input_length、input_shape 的关系</h3><p>LSTM 的输入是一个三维的张量（numpy narray), 三维张量的 shape 是[samples, time steps, features]，也就是[样本数量，时间步长（序列数量），特征长度]。LSTM layer 的参数需要确定其中的两个，在 model.fit 时，就能够对 trainX 进行训练。因此 input_dim 表示单个样本的特征长度，可以用 trainX.shape[2]赋值； input_length 表示的就是时间步长，序列长度，可以用 trainX.shape[1]进行赋值。</p><p>另外一种写法是 input_shape，其实就是这两个量的结合：input_shape = (input_length, input_dim)</p><p>因此以下的两种写法是等价的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.add(LSTM(units=<span class="number">256</span>, return_sequences=<span class="literal">True</span>,</span><br><span class="line">            input_dim=trainX.shape[<span class="number">2</span>], input_length=trainX.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.add(LSTM(units=<span class="number">256</span>, return_sequences=<span class="literal">True</span>,</span><br><span class="line">            input_shape=(trainX.shape[<span class="number">1</span>], trainX.shape[<span class="number">2</span>])))</span><br></pre></td></tr></table></figure><p>但比较奇怪的是这样设置最终的结果第一维会是 None,最终输出的是<code>[None,timesteps, feature]</code>。如果设置<code>input_size=trainX.size</code>的话，会出现以下错误：<br><code>ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4.</code><br>但是如果使用 batch_input_shape=trainX.shape就可以正常运行，并且最终得到训练的每一个样本的 <code>[timesteps,feature]</code>张量。</p><p>后来查了keras LSTM的官方文档，它对input的定义是<code>[batch, timesteps, feature]</code>，也就是说第一个参数指的是 batch 的大小，如果没有就默认为 None。在<code>model.fit</code>里有一个batch_size，如果设置了该batch_size的值，那么LSTM的层的input会自动根据trainX.shape[0]和batch_size的值来确定每一个输入的batch的大小。</p><p>batch size 限制了在可以执行权重更新之前向网络显示的样本数。拟合模型时使用的 batch size 控制一次必须进行多少预测。</p><h2 id="GRU-的输入"><a href="#GRU-的输入" class="headerlink" title="GRU 的输入"></a>GRU 的输入</h2><h3 id="tf-keras-layers-GRU-参数"><a href="#tf-keras-layers-GRU-参数" class="headerlink" title="tf.keras.layers.GRU()参数"></a><code>tf.keras.layers.GRU()</code>参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.GRU(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'sigmoid'</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="literal">None</span>, bias_regularizer=<span class="literal">None</span>, activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>, recurrent_constraint=<span class="literal">None</span>, bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, return_sequences=<span class="literal">False</span>, return_state=<span class="literal">False</span>,</span><br><span class="line">    go_backwards=<span class="literal">False</span>, stateful=<span class="literal">False</span>, unroll=<span class="literal">False</span>, time_major=<span class="literal">False</span>,</span><br><span class="line">    reset_after=<span class="literal">True</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网上关于 Keras 的 RNNLayer 中的输入写的很不清楚，整理如下：&lt;/p&gt;
&lt;h2 id=&quot;LSTM-的输入&quot;&gt;&lt;a href=&quot;#LSTM-的输入&quot; class=&quot;headerlink&quot; title=&quot;LSTM 的输入&quot;&gt;&lt;/a&gt;LSTM 的输入&lt;/h2&gt;&lt;h3</summary>
      
    
    
    
    
    <category term="Keras" scheme="https://wuruoting.club/tags/Keras/"/>
    
    <category term="RNN" scheme="https://wuruoting.club/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>gitbook+Typora打造舒适的笔记环境</title>
    <link href="https://wuruoting.club/2020/06/22/gitbook-Typora%E6%89%93%E9%80%A0%E8%88%92%E9%80%82%E7%9A%84%E7%AC%94%E8%AE%B0%E7%8E%AF%E5%A2%83/"/>
    <id>https://wuruoting.club/2020/06/22/gitbook-Typora%E6%89%93%E9%80%A0%E8%88%92%E9%80%82%E7%9A%84%E7%AC%94%E8%AE%B0%E7%8E%AF%E5%A2%83/</id>
    <published>2020-06-22T08:53:38.000Z</published>
    <updated>2021-04-07T11:51:16.764Z</updated>
    
    <content type="html"><![CDATA[<p>GitBook 是一个基于 Node.js 的命令行工具，可使用它来制作精美的电子书。gitbook简洁而且高效，能够用一种结构化的方式组织文章或者笔记，所以不管是学习的输入还是撰写文章的输出，gitbook都不失为一个很好的工具。</p><p>网上关于如何安装gitbook的文章有很多，此处不进行总结了。</p><a id="more"></a><h1 id="为什么用Typora？"><a href="#为什么用Typora？" class="headerlink" title="为什么用Typora？"></a>为什么用Typora？</h1><p>Typora可以支持实时预览，比起印象笔记等分屏的markdown写作工具,Typora这种所见即所得的极简给做笔记或者写作带来的体验感是非常强的。Typora的大部分语言都是传统markdown，使用Typora在官网下载对应版本即可 。</p><p>emoji的使用方法，这个我之前不知道，是无意中触发的：</p><p>只要用<code>:emoji-name:</code>的形式就可以插入emoji，通常一个冒号后面加字母就会自动提示emoji了。</p><p>:accept::clinking_glasses::v::ok::zap:</p><h1 id="gitbook-常用命令"><a href="#gitbook-常用命令" class="headerlink" title="gitbook 常用命令"></a>gitbook 常用命令</h1><h2 id="1-gitbook-init"><a href="#1-gitbook-init" class="headerlink" title="1.gitbook init"></a>1.gitbook init</h2><p>这个命令会在指定文件夹创建README.md和SUMMARY.md。</p><h2 id="2-gitbook-build"><a href="#2-gitbook-build" class="headerlink" title="2.gitbook build"></a>2.gitbook build</h2><p>运行该命令后会在书籍的文件夹中生成一个 <code>_book</code> 文件夹, 里面的内容即为生成的 html 文件,可以将这个文件发布自己到github的仓库中，可以作为项目的文档或者其他笔记等，使用<code>nameofUser.github.io/nameofRepository</code>域名就可以访问到在线笔记。</p><p>注意如果是一个有其他文件的仓库的话，需要在git中创建<code>docs</code>分支，然后将<code>_book</code>的内容传入该仓库的该分支中才能够访问。</p><h2 id="3-gitbook-serve"><a href="#3-gitbook-serve" class="headerlink" title="3.gitbook serve"></a>3.gitbook serve</h2><p>这一命令能够让我们在浏览器预览gitbook，通常能够在<code>http://localhost:4000</code> 预览。</p><h1 id="常用插件"><a href="#常用插件" class="headerlink" title="常用插件"></a>常用插件</h1><p>gitbook支持很多插件，能够更方便地帮助你使用gitbook。插件安装时需要在gitbook所在根目录下新建book.json， 并按照下面的配置进行修改或创建，最后使用<code>gitbook install</code>命令将对应的node_modules下载。</p><h2 id="显示文章目录：toc"><a href="#显示文章目录：toc" class="headerlink" title="显示文章目录：toc"></a>显示文章目录：toc</h2><p>一般来说如果想要显示文章目录的话，在Typora中可以在文章最开始加入<code>[toc]</code>，Typora就能够自动生成对应目录，但这个目录无法在gitbook中显示，这个插件让文档能够插入目录，在浏览器显示时也能够看见目录。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"plugins"</span> : [</span><br><span class="line">        <span class="string">"toc"</span>,</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"pluginsConfig"</span>: &#123;</span><br><span class="line">        <span class="attr">"toc"</span>: &#123;</span><br><span class="line">            <span class="attr">"addClass"</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"className"</span>: <span class="string">"toc"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要目录时须在文章开始添加<code>&lt;!-- toc --&gt;</code>，这样才会显示目录。</p><h2 id="总目录折叠：expandable-chapters"><a href="#总目录折叠：expandable-chapters" class="headerlink" title="总目录折叠：expandable-chapters"></a>总目录折叠：expandable-chapters</h2><p>这个插件使目录具有折叠功能。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"plugins"</span> : [</span><br><span class="line">        <span class="string">"expandable-chapters"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>！这里将不断继续更新 ！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;GitBook 是一个基于 Node.js 的命令行工具，可使用它来制作精美的电子书。gitbook简洁而且高效，能够用一种结构化的方式组织文章或者笔记，所以不管是学习的输入还是撰写文章的输出，gitbook都不失为一个很好的工具。&lt;/p&gt;
&lt;p&gt;网上关于如何安装gitbook的文章有很多，此处不进行总结了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="gitbook" scheme="https://wuruoting.club/tags/gitbook/"/>
    
    <category term="小技巧" scheme="https://wuruoting.club/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>文本数据的聚类分析综述</title>
    <link href="https://wuruoting.club/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/"/>
    <id>https://wuruoting.club/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/</id>
    <published>2020-06-10T08:25:51.000Z</published>
    <updated>2020-06-14T16:33:32.909Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p>聚类分析是一种无监督学习方法，在模式识别中，对于给定的数据样本，类别标号已知的情况下，分类问题通过训练，使得能够对未知类别的样本进行分类。而现实世界中，相当多的数据是没有已知类别的，它们的类别缺失或者需要大量的人工标注才能获取类别。为了发现数据的内在知识、检测并分析异常点和从数据中提取模式，聚类分析是非常重要的。</p><p>聚类分析依据相似性，将给定数据样本划分成若干个类别，相似性越高的两个物体划分为同一类，最终会将数据形成若干个簇，簇与簇可根据它们的形状、大小和密度等有所区别。</p><p>生活中的多个方面聚类都能够辅助模式识别和数据挖掘。在产品市场上，聚类可以基于用户的购买对商品进行聚类，使得市场营销人员能够利用这些知识开发有针对性地计划；在城市规划上，聚类可以将相似性高的区域进行划分，为土地建设提供选址方案等。</p><p>随着全球信息化的不断发展，大量文本数据隐含着潜在的信息和知识。文本数据是一种非常常见的非结构化数据，针对文本数据的聚类应用领域也很广泛。在信息检索方向，文本聚类可对搜索引擎进行聚类，提升用户获取信息的精确度；在信息推荐方向，聚类还可以提取出热点主题或发现事件、自动归档文本并帮助完善文本可视化。</p><p>实现文本聚类主要由三个步骤组成：1.文档的表示（提取文档特征并对特征降维处理）；2.文本聚类算法的选择和应用；3.评估文本聚类算法的有效性。三个步骤将在接下来的4章中进行详细的探讨。</p><h1 id="二、文本数据的特征提取"><a href="#二、文本数据的特征提取" class="headerlink" title="二、文本数据的特征提取"></a>二、文本数据的特征提取</h1><p>计算机难以直接对字符串文本进行处理，需要将实际的文字转化成数值型数据。对文本本身来说，它具有一些显式的特征，如字数、词频、停止词数量、单词平均长度等。为了实现文本的聚类，上述的特征需要进行处理和调整，按照某种完整的模型对文档进行数值化或向量化。</p><p>当前的主要的文档模型可被分为五个类别：布尔模型、向量空间模型、概率模型、统计语言模型和分布表示模型。</p><h2 id="（一）布尔模型"><a href="#（一）布尔模型" class="headerlink" title="（一）布尔模型"></a>（一）布尔模型</h2><p>布尔模型具有简洁的形式，容易理解。它的基础是集合论和布尔代数。我们考虑单词在文档中出现或缺失时，一个文档能够用二进制向量表示。</p><h2 id="（二）向量空间模型"><a href="#（二）向量空间模型" class="headerlink" title="（二）向量空间模型"></a>（二）向量空间模型</h2><p>向量空间模型是将文档表达为向量空间的一个矢量或点，向量空间的维数是词的数量。在向量空间的文档向量的长度是由出现的词和词的权重共同决定的[1]。在向量空间中，单词的顺序并不被考虑，这种方法也称为词袋表示方法（Bag of Words）。它是一种简单、经典的表示方法，但它对出现在文本中的词无法判定其重要性的差异，导致准确率不高。</p><p>1983年，Salton等提出了扩展布尔模型[2]，它结合了布尔模型和向量空间模型，并表现出检索性能的提升。</p><p>1986年，TF-IDF被提出[3]，这种表示改进了词袋表示法，每个单词的词频都由逆文档频率（IDF）规范化。在单词集合中，出现频率更高的项权重更低，降低了常用词在文档中的重要性，保证后续文档聚类的结果更受文档出现频率低的词的影响。</p><h2 id="（三）概率模型"><a href="#（三）概率模型" class="headerlink" title="（三）概率模型"></a>（三）概率模型</h2><p>概率模型中，文档<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image002.png" alt="img">)与查询<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image004.png" alt="img">)的相似度有如下关系：<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">)表示相关文档集，<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image008.png" alt="img">)表示<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">的补集。</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615001825039.png" alt="image-20200615001825039"></p><p>对文档而言，根据独立性假设，文档的各个词相互独立，用<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image012.png" alt="img">表示词可得到：</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615001859217.png" alt="image-20200615001859217"></p><p>其中词权重<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image016.png" alt="img">。</p><p>用<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image018.png" alt="img">)表示相关文档数，<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image020.png" alt="img">)表示包含索引词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image022.png" alt="img">)的文档数，相关文档中<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image022.png" alt="img">)的分布<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image024.png" alt="img">, 不相关文档中<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image022.png" alt="img">)的分布<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615001927832.png" alt="image-20200615001927832">),<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image028.png" alt="img">)表示包含索引词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image022.png" alt="img">的文档数。</p><p>则可推出：</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615001955999.png" alt="image-20200615001955999"></p><p>概率模型的优点在于，文档可以按照相关概率递减顺序来计算秩；但概率模型需要把文档分为相关和不相关的两个集合，未考虑到单词的频率，没有权重系数[4]。 </p><h2 id="（四）-统计语言模型"><a href="#（四）-统计语言模型" class="headerlink" title="（四） 统计语言模型"></a>（四） 统计语言模型</h2><p>统计语言模型(Statistics Language Models)是基于统计学和概率论对语言进行建模的，主要思想是语言是字母表上的概率分布，该分布表示一种可能性：即任何一个字母序列成为该语言的一个句子。这一分布就是语言的统计语言模型。目前较流行的统计语言模型是n元模型（N-gram），表示一个词的出现与否和其前面的n-1个词有关。</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image002-1591777871572.png" alt="img"></p><h2 id="（五）分布表示模型"><a href="#（五）分布表示模型" class="headerlink" title="（五）分布表示模型"></a>（五）分布表示模型</h2><p>分布式表示模型不仅考虑将单词符号化，还考虑将语义信息融入到词表示中。 1954 年，Harris提出了分布假说（ distributional hypothesis）上下文相似的词，其语义也相似[5]，这一假说为语义信息的融入提供了理论基础。</p><p>分布式表示根据任务、算法的区别，可被分为基于矩阵的分布表示、基于聚类的分布表示和基于神经网络的分布表示。在基于聚类的分布表示中，较典型的算法为布朗聚类方法(Brown clustering)，在第四章会具体介绍该算法。</p><h1 id="三、样本的相似性度量"><a href="#三、样本的相似性度量" class="headerlink" title="三、样本的相似性度量"></a>三、样本的相似性度量</h1><p>文本聚类根据不同的粒度可以分为文档、段落、语句或者单词的聚类。样本在不同的粒度下代表的事物也有所区别，如文档聚类时，每一个样本表示一个文档。对文档进行聚类时，我们需要获知文档样本与样本之间的相似度，需要相似性的度量标准。</p><p>相似性度量可使用空间两点的欧式距离、向量内积、余弦相似度、Jaccard相似度等。</p><p>相似度的度量会在一定程度上影响算法的效果，目前也有大量的研究针对聚类的相似性度量，如2009年，Luo[6]等人应用了邻居和链接的概念，将全局信息引入到两个文档的相似性度量上，提出了新的相似性度量方式：使用余弦和链接函数组合等，总而言之，相似性度量并不存在最优的方法，需要和聚类算法结合。</p><h1 id="四、文本聚类方法"><a href="#四、文本聚类方法" class="headerlink" title="四、文本聚类方法"></a>四、文本聚类方法</h1><p>传统的聚类分析算法不仅可以用在文本数据上，其他数据也是通用的。针对文本表示的不同形式，使用的聚类算法也有所区别，文本聚类主要可以分为三类方法：划分聚类方法、层次聚类方法和基于标准参数化模型的方法。</p><h2 id="（一）划分聚类方法"><a href="#（一）划分聚类方法" class="headerlink" title="（一）划分聚类方法"></a>（一）划分聚类方法</h2><p>划分方法符合我们对聚类的直观感受，将多个样本点组织成多个簇，通常簇的个数会在聚类前被给定，融合了相关领域的主观知识。</p><p>划分方法最初指定类别的初始数目，并不断迭代分配样本点，最终收敛时确定所有簇。划分方法运用在文本领域的主要有K-means和K-medoids两种聚类算法。</p><h3 id="1-K-means聚类算法"><a href="#1-K-means聚类算法" class="headerlink" title="1. K-means聚类算法"></a>1. K-means聚类算法</h3><p>K-means算法最早是从不同的科学领域中提出来的，包括1956年的Steinhaus[7], 和1957年的Lloyd[8]，至今已经提出了近60年，但它仍然是目前应用于聚类的算法之一。</p><p>K-means通过判断根据平方误差法计算出的目标函数是否达到最优解，而逐步对聚类结果进行优化。在运行前需要指定簇的类别、初始的簇的中心点，在每次迭代中，将每个点分配给中心最近的聚类。中心是群中所有点的平均值，平均点的坐标是簇中所有点上每个维度的算术平均值。</p><p>原始的K-means的缺点主要有以下几点：首先，它只考虑了样本点之间的距离，通常结果均为球状簇。若从样本点的密度考虑，以DBSCAN算法为代表的基于密度的方法能够发现任意形状的簇。其次，K值、初始化分方向等均是需要用户给定的，容易陷入局部最优。</p><h3 id="2-K-medoids聚类算法"><a href="#2-K-medoids聚类算法" class="headerlink" title="2.K-medoids聚类算法"></a>2.K-medoids聚类算法</h3><p>K-medoids聚类算法使用类中的某个点来代表簇，最早提出的K-mediods算法之一PAM(Partitioning Around Medoids) [9]的基本思想就是最初选取k个代表对象作为初始的中心点，依据当前cluster中所有其他点到该中心点的距离之和最小的准则函数，不断迭代找到更好的中心点。</p><p>该算法在一定程度上削弱了异常值的影响，但缺点是计算较为复杂，耗费的计算机时间比K-means多。它能处理任意类型的属性，但对异常数据不敏感。</p><h2 id="（二）-层次聚类方法"><a href="#（二）-层次聚类方法" class="headerlink" title="（二） 层次聚类方法"></a>（二） 层次聚类方法</h2><p>按照层次的聚类方法源于对数据需要组成层次结构的需求，数据需要进行层次结构上的汇总和特征化，因此层次划分方法被引入。层次划分方法可分为凝聚和分裂两种策略。</p><p>凝聚策略是将每个样本点在聚类最初都形成一个簇，随着迭代的进行，会将所有簇合并，直到终止条件为止。分类策略与凝聚策略正好相反，它将所有的样本点都看成同一个簇，相当于层次结构的根，将簇不断划分为更小的簇，直到划分的每一个簇都达到凝聚的条件。</p><p>以文档聚类为例，凝聚层次聚类方法可以被分为三类[10]：单连接算法（Single Linkage Clustering）、平均连接算法（Group-Average Linkage Clustering）、全连接算法（Complete Linkage Clustering）。</p><p>单连接算法的基本思想是两个簇的距离度量是从两个簇中抽取的每一对样本的最小距离<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image002-1591777984132.png" alt="img">,一旦最近的两个簇的距离超过某个任意给定的阈值，则算法结束。平均连接的基本算法是两个簇的距离度量是所有样本对的距离的平均值，全连接算法的距离度量则是两个簇所有样本对的最坏情况。</p><p>在针对文本数据的聚类中流行的层次聚类算法包括：综合的层次聚类方法BIRCH[11]，其优点在于能够通过单词扫描获取一个较好的聚类效果，但它只适用于数值型数据；基于质心和代表对象方法的CURE聚类方法[12]从每个类中抽取固定数量、分布较好的点作为代表点，并乘收缩收缩因子，减小噪音对聚类的影响；适用于分类属性层次的聚类算法ROCK[13],和使用动态模型的层次聚类算法Chameleon[14]。</p><p>1992年提出的布朗聚类方法[15]是一种针对词汇聚类的算法，它借鉴了层次聚类的凝聚策略，它的输入时一个语料库，语料库是一个词序列，输出是一个二叉树，二叉树的叶子节点是词，中间节点是对应的类别。它的评价函数是对于<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image004-1591777984132.png" alt="img">)个连续的词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006-1591777984133.png" alt="img">)序列能否组成依据话的概率的对数的归一化结果，评价函数为:$Quality(C)=\frac{1}{n}logP(w_1,w_2…w_n)$。该函数描述了某个词上下文单词对当前聚类中单词的出现的预测程度。</p><p>Gil-García[16]等在2006年提出了一个基于图的凝聚层次聚类的通用框架，这一框架指定簇间相似度度量、β-相似度图的子图和覆盖例程，可以得到不同的层次的凝聚型的聚类算法；在2010年[17]，作者又提出了针对文档聚类的动态层次算法，该算法在获取和其他传统分层算法相似的聚类质量的前提下，层次结构更小、更利于浏览，可用于创建文档分类法和分层主题检测等模式识别问题。</p><p>层次聚类的鲁棒性较强，因为它通常需要比较所有的文档，因此复杂度达到<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image010-1591777984134.png" alt="img">。为了提升层次聚类方法的效率，多种改进方法被提出。如2018年，Zhang等人[18]提出了一个分区合并方案（PMHC）用于快速分层群集，它将数据对象分成适当的组并将它们合并到组中以节省计算成本。</p><h2 id="（三）基于标准参数化模型的方法"><a href="#（三）基于标准参数化模型的方法" class="headerlink" title="（三）基于标准参数化模型的方法"></a>（三）基于标准参数化模型的方法</h2><p>给定文档<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image012-1591777984134.png" alt="img">)，获取该文档属于不同簇的概率向量q，也是文档聚类的任务之一。考虑第二章中用统计语言模型表示文档的方式，可假定文档的生成过程是先以一定概率<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image014-1591777984134.png" alt="img">)选择簇<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image016-1591777984135.png" alt="img">),然后再按照词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image018-1591777984135.png" alt="img">)的概率分布<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image020-1591777984135.png" alt="img">)选择词<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image018.png" alt="img">生成文档d,观测的所有文档在混合模型中被生成的概率为：</p><p>  <img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image024-1591777984136.png" alt="img"></p><p>其中<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image014-1591777984134.png" alt="img">为“聚簇参数”，可通过期望最大化算法学习。该方法会陷入局部最优导致收敛速度较慢。</p><p>基于EM算法进行聚类的研究主要是基于EM算法对聚类方法的改进和提升，如2005年，Rigutini[19]等人将EM算法与基于信息增益的特征选择技术相结合，该算法只需要少量文档初始化聚类，并且能够正确地提取隐藏在大量未标记集合中的规则。2011年，Kim[20]等人基于EM算法，提出了一种文本文档的主题聚类算法，使用EM方法确保文档被分配给正确的主题，从而收敛到局部最优解，其结果具有较好的性能和可解释性。</p><p>EM算法衍生出了主题建模，它是一种对文档进行聚类并提取主题的无监督学习方法，可用来识别大规模文档集或语料库中潜藏的主题信息，广泛应用在文本分类、文本聚类、摘要抽取、情感分析等领域。</p><p>主题建模起源于潜在语义分析（LSA）[21]，该方法通过奇异值分解，将高维文档向量近似地映射到一个低维潜在地语义空间上，以达到降低文档维数和消除词语存在的同义、多义等问题。在LSA基础上，Hofmann引入了概率统计的思想，提出了概率潜在语义分析模型[22]。然而pLSA模型的参数容易与特定的文档相关，有时会出现过拟合现象，因此，Blei等人在2003年提出了LDA概率主题模型[23]，把模型的参数也看作随机变量，引入控制参数的参数，实现进一步的概率化。LDA本质上是一种无监督无指导的机器学习模型，将高维文本单词空间表示为低维主题空间，忽略了和文本相关的类别信息。</p><h2 id="（四）其他聚类学习方法"><a href="#（四）其他聚类学习方法" class="headerlink" title="（四）其他聚类学习方法"></a>（四）其他聚类学习方法</h2><p>除了上述三点主要的聚类算法之外，针对文本数据的部分其他聚类算法将在本节进行简短的阐述。</p><p>模糊聚类需要根据研究对象本身的属性来构造模糊矩阵，并根据隶属度来构造模糊矩阵，最终确定聚类关系。它可允许一个文档属于不同的局促，使得聚类结果更稳定[24]。</p><p>半监督聚类是一种更新的研究算法，半监督聚类的核心思想是把半监督学习的思想结合到聚类中，通过少量的标签数据和先验知识提高聚类性能，得到性能更优的结果。在文本聚类中，使用半监督获取少量标签的聚类算法也有部分研究。</p><p>Zhang W 等提出了基于频繁项集和相似度计算的最大获取的文本聚类方法[25]。</p><h1 id="五、聚类结果的评价"><a href="#五、聚类结果的评价" class="headerlink" title="五、聚类结果的评价"></a>五、聚类结果的评价</h1><p>  聚类结果并没有没有适用于所有算法的统一的评价指标，聚类算法结果的好坏取决于聚类算法的使用的相似性度量和相应的聚类算法。首先好的聚类的簇需要满足两个特点：簇内高内聚，簇间低耦合。其次，好的聚类能够发现隐含的模式，簇的形状没有较大限制；最后从用户的角度来说，能够产生一个满足用户的聚类结果，结果具有可解释性、可理解性。</p><h2 id="（一）分类评价指标"><a href="#（一）分类评价指标" class="headerlink" title="（一）分类评价指标"></a>（一）分类评价指标</h2><p>  通常，聚类任务可以使用分类任务的数据集（包含分类标签），衡量聚类的质量可以使用分类任务的评价指标。</p><h3 id="1-召回率和准确率"><a href="#1-召回率和准确率" class="headerlink" title="1.召回率和准确率"></a>1.召回率和准确率</h3><p>对于信息检索的结果，其计算包括了两个指标：召回率（Recall Rate）和准确率（Precision Rate）。召回率表示检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率；准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；F 值为两者的调和平均值。</p><h3 id="2-宏平均和微平均"><a href="#2-宏平均和微平均" class="headerlink" title="2.宏平均和微平均"></a>2.宏平均和微平均</h3><p>宏平均（Macro-averaging），是先对每一个类统计指标值，然后在对所有类求算术平均值。微平均（Micro-averaging**），是对数据集中的每个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标[26]。</p><h2 id="（二）交叉检验方法"><a href="#（二）交叉检验方法" class="headerlink" title="（二）交叉检验方法"></a>（二）交叉检验方法</h2><p>将用于聚类的数据集划分为m个部分，随机使用m-1个部分建立聚类模型，并用剩下的1个部分检验聚类的质量。这一部分可以计算与他们最近形心的距离平方和作为度量，重复m次后，总体质量度量由质量度量的平均值计算出来，对不同的k，可以比较总体质量度量，最终选取最佳拟合数据的簇数[27]。</p><h2 id="（三）聚类质量的测定"><a href="#（三）聚类质量的测定" class="headerlink" title="（三）聚类质量的测定"></a>（三）聚类质量的测定</h2><p>当有专家构建的基准时，可将聚类模型和基准进行比较，比较时聚类质量度量Q如满足以下4项基本标准：簇的同质性、簇的完全性、碎布袋、小簇保持性，那么可以使用Q进行比较和评估。</p><p>当基准不存在时，可以采用轮廓系数对距离进行内部评估。</p><p>假设数据集D有<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image002-1591778129766.png" alt="img">)个样本被分为<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image004-1591778129766.png" alt="img">)个类别，则对于任意一个样本<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006-1591778129767.png" alt="img">),计算<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">)与<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">)所在簇中其他对象的平均距离<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image008-1591778129767.png" alt="img">),<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">)与其他簇的最小平均距离为<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image010-1591778129768.png" alt="img">。轮廓系数的定义为：</p><p><img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/image-20200615002122028.png" alt="image-20200615002122028"></p><p>  当轮廓系数为越接近1时，包含<img src="/2020/06/10/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/clip_image006.png" alt="img">的簇是紧凑的，当轮廓系数值为负时，这种情况是糟糕的，应该避免。</p><h1 id="六、聚类的局限性和挑战"><a href="#六、聚类的局限性和挑战" class="headerlink" title="六、聚类的局限性和挑战"></a>六、聚类的局限性和挑战</h1><p>不同的文本数据有不同的特性，目前文本数据聚类的局限性也给文本聚类这一领域带来了新的挑战。</p><p>目前文本数据仍存在数据稀疏等问题，文档的词汇可能很多，但这些词汇是相互关联的，数据中主成分的数量远小于特征空间的特征数量。因此上述的所有聚类方法并不能解决所有文本的聚类问题。</p><p>近年来社交网络媒体和在线聊天应用创造了大量的文本数据，特别是短文本，短文本表示维数大，如何探索出更有效率、更节省空间的数据表示形式、如何将表示形式与聚类算法更好地结合在一起，是未来仍值得研究的课题。</p><p>文本数据也越来越多地出现在异构应用程序中，有效地将基于文本的算法应用于异构多媒体场景是非常关键的。P2P分布式文档聚类算法解决了其中的一些难题[28]，但对于开发结合优化技术的新型混合算法的研究仍有很大的需求。近年来的研究热点也集中在高维数据的处理上，不断提高处理速度和规模。</p><p>【后记-如果你还能看到这里】<br>这是模式识别课程的最终提交论文（我靠着这个论文得了98分），找了几十篇论文掐头去尾粗略的看了，还是有很多不懂的地方，但至少对于这个方向有了一个框架上的概念。写综述真的很锻炼人哇…</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Salton, G. Some experiments in the generation of word and document associations [A].Proceedings of the December 4–6, 1962, fall joint computer conference[C].1962.234–250. </p><p>[2] Salton, G.&amp; E. A. Fox.&amp; H. Wu. Extended Boolean information retrieval[J]. Communications of the ACM, 1983,26(11):1022–1036. </p><p>[3]Salton, G.&amp;M.J.McGill. Introduction to modern information retrieval[M].New York.The McGraw-Hill Companies,1986.</p><p>[4]McCullagh, P. What is a statistical model?[J]. Annals of Statistics,2002,30:1225–1310. </p><p>[5]Harris, Z. Distributional structure[J]. Word,1954,10(23):146-162.</p><p>[6]Luo, C., Li, Y., &amp; Chung, S. M. Text document clustering based on neighbors[J]. Data &amp; Knowledge Engineering,2009,68(11):1271–1288.</p><p>[7]Steinhaus, H. Sur la division des corp materiels en parties[J]. Bull. Acad. Polon. Sci,1956, IV (C1.III):801–804.</p><p>[8]Lloyd, S. Least squares quantization in PCM[J].IEEE Trans Inform Theory.1982,28:129–137. </p><p>[9]Kaufman,L.&amp;,P.J.Rousseeuw.,Clustering by means of Medoids[J].Statistical Data Analysis Based on the L1–Norm and Related Methods,1987: 405–416.</p><p>[10]Aggarwal, C. C.&amp;C.Zhai.A Survey of Text Clustering Algorithms[J]. Mining Text Data,2012: 77–128.</p><p>[11]Charikar,M.&amp;C.Chekuri. Incremental clustering and dynamic information retrieval[J]. SIAM J Comput, 2004,33(6):1417-1440.</p><p>[12]Guha,S.&amp;R.Rastogi.CURE: an efficient clustering algorithm for large databases[J]. Inf Syst,2003,26(1):35-58.</p><p>[13]Dutta,M.&amp;AK.Mahanta.QROCK: a quick version of the ROCK algorithm for clustering of categorical data[J]. Pattern Recognit Letter, 2005,26(15):2364-2373.</p><p>[14]Karypi,G.&amp;EH.Han.Chameleon: a hierarchical clustering algorithm using dynamic modeling. Computer,1999,32:68-75.</p><p>[15]Brown,P,F&amp;V.J.Della Pietra.Class-Based n-gram Models of Natural Language[J].Computational Linguistics,1992,18:467-480.</p><p>[16]Gil-García,J.&amp;M. Badía-Contelles&amp;A.Pons-Porrata. Extended Star Clustering Algorithm[J]. Lecture Notes on Computer Sciences,2003,2905:480-487.</p><p>[17]Gil-García,R.&amp;A.Pons-Porrata.Dynamic hierarchical algorithms for document clustering[J].Pattern Recognition Letters,2010,31(6):469-477.</p><p>[18]Zhang, Y.&amp; Cheung, Y. A fast hierarchical clustering approach based on partition and merging scheme[A]. 2018 Tenth International Conference on Advanced Computational Intelligence (ICACI).[C].Xiamen,2018.846-851.</p><p>[19]Kim, S.&amp; Wilbur, W. Thematic clustering of text documents using an EM-based approach[J]. Journal of Biomedical Semantics, 2012,3(Suppl 3), S6.</p><p>[20]Rigutini,L&amp;U.Adegli Studi di Siena.A semi-supervised document clustering algorithm based on EM[A].IEEE/WIC/ACM International Conference on Web Intelligence[C], Compiègne (France): Proceedings of the IEEE/ACM/WI International Conference on Web Intelligence,2005.200-206.</p><p>[21]Deerwester,S&amp;S.Dumais.Indexing by latent semantic analysis[J].Journal of the American Society for Informatlon Science,1990,41(6):391-407.</p><p>[22]Hofmann,T.Probabilistic latent semantic analysis[A].Proc.of the Conference on Uncertainty in Artificial Intelligence[C].1999:289—296.</p><p>[23]Blei,D&amp;A.Ng A.Latent Dirichlet Allocation[J].Journal of Machine Learning Research,2003,3:993—1022．</p><p>[24]C. Borgelt and A. Nurnberger.Fast Fuzzy Clustering of Web Page Collections[A].Proc. of PKDD Workshop on Statistical Approaches for WebMining（SAWM)[C],Pisa(Italy) 2004.</p><p>[25]Zhang,W.&amp;T.Yoshida.Text Clustering Using Frequent Itemsets[J]. Knowledge-Based Systems,2010,23(5):379-388.</p><p>[26]Yang Y. An evaluation of statistical approaches to text categorization[J]. Information retrieval, 1999, 1(1-2): 69-90.</p><p>[27]Han,J&amp;M.Kamber.数据挖掘：概念与技术(原书第3版)[M].北京：机械工业出版社.2012.</p><p>[28]Judith, J.E.&amp;J.Jayakumari.Distributed document clustering algorithms: a recent survey[J].Int. J. Enterprise Network Management,2015,Vol. 6, No. 3:207–221.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、引言&quot;&gt;&lt;a href=&quot;#一、引言&quot; class=&quot;headerlink&quot; title=&quot;一、引言&quot;&gt;&lt;/a&gt;一、引言&lt;/h1&gt;&lt;p&gt;聚类分析是一种无监督学习方法，在模式识别中，对于给定的数据样本，类别标号已知的情况下，分类问题通过训练，使得能够对未知类别的</summary>
      
    
    
    
    
    <category term="文本挖掘" scheme="https://wuruoting.club/tags/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/"/>
    
    <category term="聚类分析" scheme="https://wuruoting.club/tags/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题5】中间代码生成</title>
    <link href="https://wuruoting.club/2020/05/23/%E4%B8%AD%E9%97%B4%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/"/>
    <id>https://wuruoting.club/2020/05/23/%E4%B8%AD%E9%97%B4%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/</id>
    <published>2020-05-23T09:27:48.000Z</published>
    <updated>2020-06-14T15:45:18.837Z</updated>
    
    <content type="html"><![CDATA[<p>中间代码生成就是把经过语法分析和语义分析的源程序中间表示翻译为中间代码展示，中间表示可能有多个种类，如语法树、DAG、后缀式、三地址代码等。</p><p>如果中间代码独立于机器的话，那么各便于编译系统的建立和移植，并且便于进行独立于机器的代码优化工作。</p><h1 id="三地址代码"><a href="#三地址代码" class="headerlink" title="三地址代码"></a>三地址代码</h1><p>三地址代码包含一个运算和三个地址，两个地址用于存放运算对象，一个用于存放运算结果。</p><p>具体实现：四元式、三元式、间接三元式。</p><h2 id="四元式"><a href="#四元式" class="headerlink" title="四元式"></a>四元式</h2><p>op、arg1、arg2、result</p><h2 id="三元式"><a href="#三元式" class="headerlink" title="三元式"></a>三元式</h2><p>op、arg1、arg2 使用运算x op y 的位置来表示计算的结果</p><h2 id="间接三元式"><a href="#间接三元式" class="headerlink" title="间接三元式"></a>间接三元式</h2><h1 id="类型和声明"><a href="#类型和声明" class="headerlink" title="类型和声明"></a>类型和声明</h1><p>类型表达式是用于表示类型的结构的，如基本类型int、char、float，</p><p>类型表达式名也是类型表达式。</p><p>类型构造算子:作用于类型表达式可以构造新的类型表达式。</p><p><strong>数组构造符array</strong></p><table><thead><tr><th align="center">类型</th><th align="center">类型表达式</th></tr></thead><tbody><tr><td align="center">int[3]</td><td align="center">array(3,int)</td></tr><tr><td align="center">int[2][3]</td><td align="center">array(2,array(3,int))</td></tr></tbody></table><p><strong>指针构造符pointer</strong></p><p><strong>笛卡尔乘积构造符x</strong></p><p><strong>函数构造符-&gt;</strong></p><p><strong>记录构造符record</strong></p><h2 id="类型检查-type-checking"><a href="#类型检查-type-checking" class="headerlink" title="类型检查 type checking"></a>类型检查 type checking</h2><p>保证参与的运算分量和运算符预期的类型相匹配。</p><p><strong>如果两个类型表达式相等，那么返回某种类型，否则出错</strong></p><h3 id="类型等价"><a href="#类型等价" class="headerlink" title="类型等价"></a>类型等价</h3><blockquote><p>两种类型之间结构等价当且仅当下面某个条件为真： </p><p>1.是相同的类型</p><p>2.是相同的类型构造算子应用于结构等价的类型而构造得到的。</p><p>3.一个类型是另一个类型表达式的名字</p></blockquote><p><strong>类型检查有两种形式：类型综合和类型推导。</strong></p><p>类型综合是根据子表达式的类型构造出表达式的类型，<strong>要求名字先声明再使用</strong>。表达式$E1+E2$的类型是根据$E1$和$E2$的类型定义的。</p><p>类型推导是根据一个语言结构的使用来确定结构的类型，就类似如果使用了某个类型才能用的函数的话，那么可以指出使用该函数的变量就是对应的类型。</p><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><p>浮点数和整型相加，编译器内部需要进行转换。</p><p>不同的语言有不同的类型转换，主要转换有两种：拓宽转换（保持信息）、窄化转换（丢失信息）。</p><h2 id="类型翻译"><a href="#类型翻译" class="headerlink" title="类型翻译"></a>类型翻译</h2><h2 id="类型的声明"><a href="#类型的声明" class="headerlink" title="类型的声明"></a>类型的声明</h2><p>语义分析在遇到声明语句时，主要做两件事情：1.收集标识符的类型等属性信息；2.为每一个名字分配一个相对地址。</p><h3 id="声明的SDT"><a href="#声明的SDT" class="headerlink" title="声明的SDT"></a>声明的SDT</h3><h2 id="表达式和赋值语句的翻译"><a href="#表达式和赋值语句的翻译" class="headerlink" title="表达式和赋值语句的翻译"></a>表达式和赋值语句的翻译</h2><h3 id="为赋值语句生成三地址码的SDD"><a href="#为赋值语句生成三地址码的SDD" class="headerlink" title="为赋值语句生成三地址码的SDD"></a>为赋值语句生成三地址码的SDD</h3><p>gen 一个函数，生成括号内代表信息的三地址码</p><table><thead><tr><th>Production</th><th>Semantic Rules</th></tr></thead><tbody><tr><td>$S\rightarrow id=E$</td><td>$S.code=E.code</td></tr><tr><td>$E\rightarrow E_1+E_2$</td><td>$E.addr=new Temp()$, $E.code=E1.code</td></tr><tr><td>$E\rightarrow -E_1$</td><td>$E.addr=new Temp()$ ,$E.code=E_1.code</td></tr><tr><td>$E\rightarrow (E_1)$</td><td>$E.addr=E1.addr$,$E.code=E_1.code$</td></tr><tr><td>$E\rightarrow id$</td><td>$E.addr=top.get(id.lexeme)$, $E.code=’’$</td></tr></tbody></table><p>将$a=b+-c;$ 编译成三地址码：</p><p>$S\Rightarrow id=E_0;$</p><p>$\Rightarrow id=E_1+E_2;$</p><p>$\Rightarrow id=E_1+-E_3;$ </p><p>$\Rightarrow id=E_1+-id;$</p><p>$\Rightarrow id=id+-id;$</p><table><thead><tr><th>产生式</th><th>属性变化</th></tr></thead><tbody><tr><td>$E_1\rightarrow id$</td><td>$E_1.addr=addr(b)$, $E_1.code=’’$</td></tr><tr><td>$E3\rightarrow id$</td><td>$E_3.addr=addr(c)$, $E_3.code=’’$</td></tr><tr><td>$E_2\rightarrow -E_3$</td><td>$E_2.addr=t1$ ,$E_2.code=E_3.code</td></tr><tr><td>$E_0\rightarrow E_1+E_2$</td><td>$E_0.addr=t2$,$E_0.code=E_1.code</td></tr><tr><td>$S\rightarrow id=E_0$</td><td>$S.code=E_0.code</td></tr></tbody></table><p>刚好三行就是赋值语句的三地址码。</p><h3 id="布尔表达式的翻译"><a href="#布尔表达式的翻译" class="headerlink" title="布尔表达式的翻译"></a>布尔表达式的翻译</h3><h4 id="短路代码"><a href="#短路代码" class="headerlink" title="短路代码"></a>短路代码</h4><p>跳转代码中&amp;&amp; || ！都被翻译成跳转指令。</p><p>语句：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(x&lt;<span class="number">100</span>||x&gt;<span class="number">200</span> &amp;&amp; x!=y)</span><br><span class="line">x=<span class="number">0</span>;</span><br></pre></td></tr></table></figure><p>三地址代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if x&lt;100 goto L2</span><br><span class="line">goto L3</span><br><span class="line">L3: if x&gt;200 goto L4</span><br><span class="line">goto L1</span><br><span class="line">L4: if x!&#x3D;y goto L2</span><br><span class="line">goto L1</span><br><span class="line">L2:x&#x3D;0</span><br><span class="line">L1:</span><br></pre></td></tr></table></figure><p>其实运算符并不在代码中，布尔表达式的值是通过代码序列中的位置来表示的。</p><h3 id="控制流语句"><a href="#控制流语句" class="headerlink" title="控制流语句"></a>控制流语句</h3><p>控制流语句：(S表示语句，B表示布尔表达式)</p><p>1.$P\rightarrow S$</p><p>2.$S\rightarrow assign$</p><p>3.$S\rightarrow if(B) S1$</p><p>4.$S\rightarrow if(B) \quad S1 \quad else \quad S2$</p><p>5.$S\rightarrow while(B)\quad S1$</p><p>6.$S\rightarrow S1 \quad S2$</p><p>SDD</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>$P\rightarrow S$</td><td>$S.next=newlable()$</td></tr><tr><td>$S\rightarrow assign$</td><td>$S.code=assign.code$</td></tr><tr><td>$S\rightarrow if(B) S1$</td><td>$B.true=newlabel()$,$B.false=S_1.next=S.next$, $S.code=B.code</td></tr><tr><td>$S\rightarrow if(B) \quad S1 \quad else \quad S2$</td><td>$B.true=newlabel()$,$B.false=newlabel()$,$S_1.next=S_2.next=S.next$,$S.code=B.code</td></tr><tr><td>$S\rightarrow while(B)\quad S1$</td><td></td></tr></tbody></table><p><strong>(1) $B\rightarrow E1 \quad rel \quad R2$ (假设形如$a&lt;b$)</strong></p><p>$B.true: if \quad a&lt;b\quad goto \quad B.true$ (j&lt;,a,b,B.true)</p><p>$B.FALSE: goto B.false$     (j,,,B.false)</p><p>(2) <strong>B是常量</strong>, 就直接翻译为跳转指令。</p><p>(3) 不需要为$B\rightarrow!B$产生新的代码，只需要将真假出口交换就可以了。(继承属性)。</p><p>(4) 对$B\rightarrow B1||B2$,</p><p>如果B1为真则B为真，B1.true从B.true继承而来，如果B1为假，则对B2求值，B1.false就可以设置为B2的代码的第一条指令的标号。B2的真假出口标号可直接从B继承获得。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;中间代码生成就是把经过语法分析和语义分析的源程序中间表示翻译为中间代码展示，中间表示可能有多个种类，如语法树、DAG、后缀式、三地址代码等。&lt;/p&gt;
&lt;p&gt;如果中间代码独立于机器的话，那么各便于编译系统的建立和移植，并且便于进行独立于机器的代码优化工作。&lt;/p&gt;
&lt;h1 i</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://wuruoting.club/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题4】语法制导翻译</title>
    <link href="https://wuruoting.club/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/"/>
    <id>https://wuruoting.club/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/</id>
    <published>2020-05-22T02:27:00.000Z</published>
    <updated>2020-06-14T15:45:00.324Z</updated>
    
    <content type="html"><![CDATA[<p>语法制导翻译，边做语法分析，边做语义分析。它使用CFG引导对语言的翻译，是一种面向文法的翻译技术。</p><h1 id="语义信息"><a href="#语义信息" class="headerlink" title="语义信息"></a>语义信息</h1><p><strong>如何表示语义信息？</strong></p><p>将语言结构的语义以属性(attribute)的形式赋予代表此结构的文法符号。</p><p><strong>如何计算语义属性？</strong></p><p>属性的计算以语义规则(semantic rules)的形式赋予由文法符号组成的产生式。在语法分析推导或归约的每一步骤中，通过语义规则实现对属性的计算，以达到对语义的处理。</p><p>换句话说就是：为每一个产生式配上语义规则并且在适当的时候执行这些规则。</p><h1 id="SDD-语法制导定义"><a href="#SDD-语法制导定义" class="headerlink" title="SDD 语法制导定义"></a>SDD 语法制导定义</h1><p>SDD是一个上下文无关文法和属性及规则的结合。属性和文法符号相关联，而规则和产生式相关联，有时也称为属性文法。<br>如果𝑿是一个符号，而𝒂是𝑿的一个属性，那么用𝑿.𝒂来表示在某个标号为𝑿的分析树节点上的属性值。属性可以有很多类型，比如变量的数据类型、表达式的值、变量的地址、数字的有效位数等等。</p><h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><p>属性分为综合属性和继承属性。</p><p><strong>综合属性</strong>只能由当前结点或者结点的子节点的属性值来计算。通常，产生式左侧的属性都来自右侧的话，那么左侧的属性就是综合属性。</p><p><strong>继承属性</strong>是由当前结点的父节点或兄弟节点或本身的属性值来定义的。（只要有父节点或兄弟结点定义就是继承属性了）</p><blockquote><p>终结符可以有综合属性，就是词法分析的词法值。终结符没有继承属性。</p></blockquote><p>属性文法写成表格形式，相同的非终结符需要用下标区分。</p><h2 id="S属性的SDD"><a href="#S属性的SDD" class="headerlink" title="S属性的SDD"></a>S属性的SDD</h2><p>只包含综合属性的SDD称为S属性的SDD。它可以按照任何自底向上的顺序进行求值。</p><p>L属性SDD的特例。</p><h2 id="L属性的SDD"><a href="#L属性的SDD" class="headerlink" title="L属性的SDD"></a>L属性的SDD</h2><p>要么是综合属性，要么是继承属性，且满足以下i条件：</p><p>对于产生式$A\rightarrow X_1 X_2 …X_n$,$X_i$的继承属性仅能依赖于：</p><ul><li><p>A的继承属性（如果是综合属性可能会有环路）</p></li><li><p>产生式$X_i$左侧的属性。（继承属性只能右侧的继承左侧的，规定了依赖图的边只能从左往右)</p></li></ul><h2 id="SDD的求值"><a href="#SDD的求值" class="headerlink" title="SDD的求值"></a>SDD的求值</h2><p>如果是综合属性，就可以按照任何自底向上的顺序进行求值，如果是同时具有继承属性和综合属性的话，首先要看有没有出现环状的依赖关系，最好不要出现循环的情况。</p><p>1.绘制依赖图dependency graph</p><p>2.求DAG的依赖图的拓扑排序（如果图存在环，就不存在拓扑排序）</p><p>拓扑排序不是唯一的，平行关系可以交换。</p><h1 id="SDT-语法制导的翻译方案"><a href="#SDT-语法制导的翻译方案" class="headerlink" title="SDT 语法制导的翻译方案"></a>SDT 语法制导的翻译方案</h1><p>SDT是在产生式中嵌入了程序片段的一个上下文无关文法。这些片段称为语义动作，它们可以出现在产生式的任何位置。默认用{}括起来。</p><blockquote><p>SDD时语言翻译的高层次规格说明，隐藏了很多具体实现细节，使用户不必显式地说明翻译发生的顺序。</p><p>SDT是SDD的一种补充，是SDD的具体实施方案，显式地指明了语义规则的计算顺序，以便说明某些实现细节。</p></blockquote><p>语法制导翻译可以用于抽象语法树的构建，</p><h2 id="如何用SDT实现两类重要的SDD"><a href="#如何用SDT实现两类重要的SDD" class="headerlink" title="如何用SDT实现两类重要的SDD"></a>如何用SDT实现两类重要的SDD</h2><p>产生式右侧的动作在它左边的所有文法符号后被匹配后立即执行。</p><p>将内嵌语义动作替换成一个新的非终结符，可以执行相应的语义动作。</p><h3 id="S属性的SDD-1"><a href="#S属性的SDD-1" class="headerlink" title="S属性的SDD"></a>S属性的SDD</h3><p><strong>后缀翻译方案：</strong></p><p>S属性的SDD可以构造出SDT: <strong>每个动作都放在产生式的结尾。</strong></p><p>所有属性都是综合属性。</p><h3 id="产生式内部带有语义动作的SDT"><a href="#产生式内部带有语义动作的SDT" class="headerlink" title="产生式内部带有语义动作的SDT"></a>产生式内部带有语义动作的SDT</h3><p>$B\rightarrow X{a}Y$</p><p>自底向上，X出现在分析栈栈顶时，立即执行动作a。</p><p>自顶向下，在展开Y的本次出现或者在输入中检测Y之前执行动作a。</p><h3 id="L属性的SDD-1"><a href="#L属性的SDD-1" class="headerlink" title="L属性的SDD"></a>L属性的SDD</h3><p>将计算某个非终结符号A的<strong>继承属性</strong>的动作插入到产生式<strong>右部中紧靠在A的本次出现之前的位置上</strong>。</p><p>将计算一个产生式左部符号的<strong>综合属性</strong>的动作放置在这个产生式右部的<strong>最右端</strong> <strong>。</strong></p><p><img src="/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/image-20200523200226341.png" alt="image-20200523200226341"></p><p>如果基本文法可以用LL分析，那么可以用递归下降、在LL预测分析过程中翻译(属性值存放在语法分析栈中)或者用LR分析。</p><h3 id="在递归下降分析中加入语义翻译"><a href="#在递归下降分析中加入语义翻译" class="headerlink" title="在递归下降分析中加入语义翻译"></a>在递归下降分析中加入语义翻译</h3><p>函数A的参数是非终结符A的继承属性<br>函数A的返回值是非终结符A的综合属性</p><p><img src="/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/image-20200523200609909.png" alt="image-20200523200609909"></p><p><img src="/2020/05/22/%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91/image-20200523200643560.png" alt="image-20200523200643560"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;语法制导翻译，边做语法分析，边做语义分析。它使用CFG引导对语言的翻译，是一种面向文法的翻译技术。&lt;/p&gt;
&lt;h1 id=&quot;语义信息&quot;&gt;&lt;a href=&quot;#语义信息&quot; class=&quot;headerlink&quot; title=&quot;语义信息&quot;&gt;&lt;/a&gt;语义信息&lt;/h1&gt;&lt;p&gt;&lt;stro</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://wuruoting.club/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题3】语法分析的例子整理</title>
    <link href="https://wuruoting.club/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/"/>
    <id>https://wuruoting.club/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/</id>
    <published>2020-05-21T12:58:37.000Z</published>
    <updated>2020-06-14T15:44:54.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SLR-1"><a href="#SLR-1" class="headerlink" title="SLR(1)"></a>SLR(1)</h2><p>考虑文法：</p><p>$E\rightarrow E+T|T$</p><p>$T\rightarrow T*F|F$</p><p>$F\rightarrow (E) |id$</p><p>1.扩展文法：</p><p>$E’\rightarrow E$<br>$E\rightarrow E+T|T$<br>$T\rightarrow T*F|F$<br>$F\rightarrow (E) |id$</p><p>2.LR(0)项：</p><p><img src="/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/image-20200522080711106.png" alt="LR(0)项"></p><p>3.绘制LR(0)自动机：</p><p><img src="/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/image-20200521205727392.png" alt="LR(0)自动机"></p><p>4.由状态1、2、9可发现，这个语法有移进归约冲突，因此不是LR(0)文法，</p><p>而在状态1中，Follow(E’)={$},+不在E’的Follow集里面的，因此无歧义，在状态2和9中，Follow(E)={+,(,$},*不在E的Follow集里，也无歧义，该文法是SLR(1)文法。</p><p>5.构建SLR(1)分析表。</p><p><img src="/2020/05/21/%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%E6%95%B4%E7%90%86/image-20200522095458808.png" alt="image-20200522095458808"></p><p>6.串(id+id)*id的分析过程:</p><table><thead><tr><th></th><th>stack</th><th>input</th><th>action</th></tr></thead><tbody><tr><td>1</td><td>$0</td><td>(id+id)*id$</td><td>S4</td></tr><tr><td>2</td><td>$0(4</td><td>id+id)*id$</td><td>S5</td></tr><tr><td>3</td><td>$0(4id5</td><td>+id)*id$</td><td>$r(F\rightarrow id)$</td></tr><tr><td>4</td><td>$0(4F3</td><td>+id)*id$</td><td>$r(T\rightarrow F)$</td></tr><tr><td>5</td><td>$0(4T2</td><td>+id)*id$</td><td>$r(E\rightarrow T)$</td></tr><tr><td>6</td><td>$0(4E8</td><td>+id)*id$</td><td>S6</td></tr><tr><td>7</td><td>$0(4E8+6</td><td>id)*id$</td><td>S5</td></tr><tr><td>8</td><td>$0(4E8+6id5</td><td>)*id$</td><td>$r(F\rightarrow id)$</td></tr><tr><td>9</td><td>$0(4E8+6F3</td><td>)*id$</td><td>$r(T\rightarrow F)$</td></tr><tr><td>10</td><td>$0(4E8+6T9</td><td>)*id$</td><td>$r(E\rightarrow E+T)$</td></tr><tr><td>11</td><td>$0(4E8</td><td>)*id$</td><td>S11</td></tr><tr><td>12</td><td>$0(4E8)11</td><td>*id$</td><td>$r(F\rightarrow (E))$</td></tr><tr><td>13</td><td>$0F3</td><td>*id$</td><td>$r(T\rightarrow F)$</td></tr><tr><td>14</td><td>$0T2</td><td>*id$</td><td>S7</td></tr><tr><td>15</td><td>$0T2*7</td><td>id$</td><td>S5</td></tr><tr><td>16</td><td>$0T2*7id5</td><td>$</td><td>$r(F\rightarrow id)$</td></tr><tr><td>17</td><td>$0T2*7F10</td><td>$</td><td>$r(T\rightarrow  T*F)$</td></tr><tr><td>18</td><td>$0T2</td><td>$</td><td>$r(E\rightarrow T)$</td></tr><tr><td>19</td><td>$0E1</td><td>$</td><td>accept</td></tr></tbody></table><p>因此该串被接受。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SLR-1&quot;&gt;&lt;a href=&quot;#SLR-1&quot; class=&quot;headerlink&quot; title=&quot;SLR(1)&quot;&gt;&lt;/a&gt;SLR(1)&lt;/h2&gt;&lt;p&gt;考虑文法：&lt;/p&gt;
&lt;p&gt;$E\rightarrow E+T|T$&lt;/p&gt;
&lt;p&gt;$T\rightarrow T</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://wuruoting.club/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题2】关于语法分析</title>
    <link href="https://wuruoting.club/2020/05/21/%E5%85%B3%E4%BA%8E%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>https://wuruoting.club/2020/05/21/%E5%85%B3%E4%BA%8E%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90/</id>
    <published>2020-05-21T02:07:43.000Z</published>
    <updated>2020-06-14T15:44:35.422Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这将是一个非常口语化的总结，因为这就是我口述的总结。</p></blockquote><p>语法分析过程主要包括两种方法：自底向上的语法分析和自顶向下的语法分析。其中，“底”指的就是原始串，而“顶”指的是开始符号。分析的目的就是确定某一个确定的字符串是否属于文法描述的语言。</p><p>而这些分析方法，最终都是要让串形成对应的语法分析树，因此它们将一个判定问题，转化成了生成语法分析树的过程。</p><h1 id="First集和Follow集"><a href="#First集和Follow集" class="headerlink" title="First集和Follow集"></a>First集和Follow集</h1><p>First集和Follow集应该是对于任意的文法都是能够确定的。</p><p>文法中的任意文法符号串都是有First集的，First集相当于这个文法符号串能推出的串中最左侧的终结符的集合。First集可以包含$\epsilon$。</p><blockquote><p>求First集的规则：</p><ol><li><p>把所有的终结符语法规则列出来（我感觉First集求的时候不能有或？还是也可以）</p></li><li><p>如果X是终结符或者$\epsilon$,$First(X)={X}$</p><p>如果X是非终结符，对每个产生式$X-&gt;X_1X_2…X_n$,$First(X_1)$是$First(X)$的子集。</p><p>如果有$X_1X_2…X_i\Rightarrow\epsilon(i&lt;n)$,那么$First(X_{i+1})$是$First(X)$的子集。</p></li></ol></blockquote><p>Follow集能够让一个非终结符消失（推出空），就是说Follow是确定当某一个非终结符后面出现了哪些终结符的时候，我们需要用推出空这个产生式。</p><blockquote><p>求Follow集的规则：</p><p>1.先将$放入Follow(S)中，S为开始字符。(构建LL(1)分析表的时候，如果有$S\Rightarrow\epsilon$,那么就可以写在[S,$]里，表示如果接受的是一个空串，就可以用这个产生式)</p><p>2.如果存在产生式$A\rightarrow\alpha B\beta$,那么求解Follow(B)的时候，要将$First(\beta)$中除了$\epsilon$所有的元素都加入Follow(B)。$\beta$可包含终结符或非终结符。</p><p>3.产生式右侧被推导出之后，左侧的Follow集就是右侧最右（需要考虑右侧是否为空，若为空就不断考虑向左移动的符号）的非终结符的Follow集的子集。</p><p>【如果存在产生式$A\rightarrow\alpha B\beta$,且$\beta$可空（或者说B的First集包含$\epsilon$)，那么$Follow(B)\Leftarrow Follow(A)$】</p></blockquote><p>因为我们确定某个非终结符的Follow集，都是通过它在右侧才能确定的，因此我们不需要考虑那些右侧全是终结符的产生式。</p><p>LL(1)分析表做的是这件事：横轴是预测的下一个字符，然后当前的栈顶的非终结符已知，那么要通过哪一个产生式能够最终推出预测的下一个字符。所以我们需要通过计算First集和Follow集来确定LL(1)分析表。</p><h1 id="自顶向下的语法分析"><a href="#自顶向下的语法分析" class="headerlink" title="自顶向下的语法分析"></a>自顶向下的语法分析</h1><p>从开始符号最终到实际的字符串，<strong>自顶向下</strong>中主要分为<strong>回溯分析程序</strong>和<strong>预测分析程序</strong>。我们主要学了两种预测分析方法：<strong>递归下降和LL(1)</strong>。</p><p><strong>为何叫“预测分析”，</strong>我们可以这么理解：首先，自顶向下分析方法的基础就是将字符串看成输入串，就是说从开始到结束，我们可以认为是逐步读取这个串的，因此字符之间有了先后被读取的，那么我们构建语法分析树也就是一个先根次序创建树的过程，我们也可以说<strong>自顶向下分析就是要找到对应串的最左推导</strong>。因此预测分析首先是要求给定的文法中没有左因子、左递归，文法不能是二义性的，其次预测分析需要看文法的下一个字符，也就是<strong>下一个输出符号</strong>，所以我们称之为“预测”。</p><h2 id="回溯分析程序"><a href="#回溯分析程序" class="headerlink" title="回溯分析程序"></a>回溯分析程序</h2><h2 id="预测分析程序"><a href="#预测分析程序" class="headerlink" title="预测分析程序"></a>预测分析程序</h2><h3 id="递归下降"><a href="#递归下降" class="headerlink" title="递归下降"></a>递归下降</h3><p>改写为$EBNF$(消除左递归、去除左因子)</p><h3 id="LL-1-分析算法"><a href="#LL-1-分析算法" class="headerlink" title="LL(1)分析算法"></a>LL(1)分析算法</h3><p>第一个L表示从左向右扫描输入，第二个L表示最左推导，1表示每一步中只需要向前看一个输入符号来决定语法分析动作。</p><h4 id="预测分析表的构建"><a href="#预测分析表的构建" class="headerlink" title="预测分析表的构建"></a>预测分析表的构建</h4><blockquote><p>$LL(1)$构建预测分析表的步骤：</p><ol><li>$First(\alpha)$中的每个记号$s$，都将$A\rightarrow\alpha$添加至$M[A,s]$中。</li><li>$\alpha$可空的话，对$Follow(A)$中的每一个元素$k$，将$A\rightarrow\alpha$添加到$M[A,k]$中。</li></ol><p>如果$M[A,\alpha]$没有产生式的话，就将其设置为$error$。</p></blockquote><h4 id="LL-1-文法"><a href="#LL-1-文法" class="headerlink" title="LL(1)文法"></a>LL(1)文法</h4><p>一个文法若满足以下条件，则该文法就是LL(1)文法：</p><p>在每个产生式$A\rightarrow{\alpha}_1 |{\alpha}_2⋯|{\alpha}_n$中，对于所有的i和j:$1≤i, j≤n, i≠j$，$First(α_i )∩First(α_j )$为空。（若不为空，假设有一个相同元素$k$,那么在$M[A,k]$就会加入两个产生式：$A\rightarrow{\alpha}_i$和$A\rightarrow{\alpha}_j$)</p><p>若对于非终结符A可空，那么$First(A)∩Follow(A)$为空。(若有相同元素k，根据分析表也会发现$M[A,k]$有两个产生式)</p><p><strong>如果一个文法G，由它构造的LL(1)分析表中的每个子项最多只含有一个产生式，那么它就是LL(1)文法。</strong></p><p>在LL(1)分析表中有两项产生式的文法不一定是二义性的文法，可能是有左递归的。</p><blockquote><p>一个不是$LL(1)$的文法同样可以用$LL(1)$方法。</p></blockquote><p>LL(1）方法对应的是非递归的预测分析器，显示维护栈结构，应该和计算理论里的下推自动机类似。下推自动机所定义的语言恰好就是上下文无关语言。</p><h1 id="自底向上的语法分析"><a href="#自底向上的语法分析" class="headerlink" title="自底向上的语法分析"></a>自底向上的语法分析</h1><p>归约其实就是推导的反向操作。如果反向构造一个推导过程，那么就会是最右推导的。推导的方法是从记号串开始，使用产生式进行归约，期望得到开始符号，如果能够得到开始符号，那么这个字符串就是文法可以识别的语句。</p><p>两个动作：移进 shift和规约 reduce。</p><p>自底向下就是从输入串到开始符号的归约，归约的方向是从左到右，可以认为是最左归约，逆向的过程就是最右推导。</p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="短语、直接短语和句柄"><a href="#短语、直接短语和句柄" class="headerlink" title="短语、直接短语和句柄"></a>短语、直接短语和句柄</h3><p>短语就是在一个句型中对应的分析树，里以非终结符为根的子树的所有叶子节点构成的排列就是对于该非终结符的短语，如果子树只有两层，那么就是直接短语。最左侧的非终结符的子树对应的短语就是句柄。</p><p>句柄的定义：如果$S\Rightarrow_{lm}^{*}\alpha A\omega \Rightarrow_{lm} \alpha \beta \omega$，A是输入串中最右的非终结符，则$\beta$称为一个句柄。</p><p>句柄可以理解为一个归约点，可以允许解析器通过进一步的归约操作回到开始符号的位置。而实际上我们做的归约就是最左归约。</p><p>对于下列文法：</p><p>$E\rightarrow E+T|T$<br>$T\rightarrow T*F|F$<br>$F\rightarrow (E) |id$</p><p>对于输入串$id*id$，从左到右相当于一个最左归约的过程。从左至右：</p><table><thead><tr><th>产生式</th><th>句柄</th><th>最右句型</th></tr></thead><tbody><tr><td>$F\rightarrow id$</td><td>$id$</td><td>id*id</td></tr><tr><td>$T\rightarrow F$</td><td>$F$</td><td>F*id</td></tr><tr><td>$F\rightarrow id $</td><td>$id$</td><td>T*id</td></tr><tr><td>$T\rightarrow T*F$</td><td>$T*F$</td><td>T*F</td></tr><tr><td>$E\rightarrow T$</td><td>$T$</td><td>T</td></tr></tbody></table><p><img src="/2020/05/21/%E5%85%B3%E4%BA%8E%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90/image-20200521195029382.png" alt="image-20200521195029382"></p><h3 id="可行前缀"><a href="#可行前缀" class="headerlink" title="可行前缀"></a>可行前缀</h3><h2 id="LR-0-分析算法"><a href="#LR-0-分析算法" class="headerlink" title="LR(0)分析算法"></a>LR(0)分析算法</h2><p><strong>LR(0)文法中L指的是从左到右扫描输入串，R代表了最右推导，0表示进行分析动作的决策只考虑栈顶状态，不需要看输入串。（没有lookahead)</strong></p><p><strong>1.扩展文法。</strong></p><p>在决定状态间的转移前，我们必须先加入一条扩展文法：$S\rightarrow E$其中$S$是新的起始符号（start symbol）而<em>E</em>是原先的起始符号。这一做法是为了保证分析器能有一个唯一的起始状态。</p><p><strong>2.列LR(0)项。</strong>(点号的左侧是已经读入的，点号的剩余是还没有读入的)</p><p><strong>3.起始状态是所有点在最左侧的LR(0)项组成的封闭集,构建LR(0)自动机</strong></p><p><strong>4.构建LR(0)分析表。</strong></p><p><strong>5.进行分析。</strong></p><p>如果X是终结符，只要有移进项先移进。</p><h4 id="LR-0-文法"><a href="#LR-0-文法" class="headerlink" title="LR(0)文法"></a>LR(0)文法</h4><p>无歧义需要没有归约归约冲突或移进归约冲突。</p><h2 id="SLR-1-分析算法"><a href="#SLR-1-分析算法" class="headerlink" title="SLR(1)分析算法"></a>SLR(1)分析算法</h2><p>如果当前栈顶状态可以支持终结符移进，并且<strong>下一个记号也就是该终结符</strong>，才会移进。如果当前栈顶状态包含了归约项$A\rightarrow\gamma.$，且<strong>下一个记号在$Follow（A)$</strong>时，才会使用$A\rightarrow\gamma$归约，如果不在$Follow(A)$也不会做归约。$GOTO$项与LR(0)类似。</p><blockquote><p>歧义的产生：</p><p>1)有归约项和移进项，且移进项$A\rightarrow \alpha . X \beta$的下一个字符$X$在$Follow（B）$中,当然如果下一个记号不是$X$那么就没有歧义了。</p><p>2)有两个不同的归约项$A\rightarrow\beta.$，$B\rightarrow\gamma.$，且下一个记号即在A的Follow集也在B的Follow集，或者两个Follow集都没有$X$,此时要报错。</p></blockquote><p>当确认没有歧义的时候，归约项$r(A\rightarrow \gamma)$就会被填入A的Follow集对应的Input下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;这将是一个非常口语化的总结，因为这就是我口述的总结。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;语法分析过程主要包括两种方法：自底向上的语法分析和自顶向下的语法分析。其中，“底”指的就是原始串，而“顶”指的是开始符号。分析的目的就是确定某一个确定的</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://wuruoting.club/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【编译原理复习专题1】上下文无关文法和正则表达式</title>
    <link href="https://wuruoting.club/2020/05/21/%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>https://wuruoting.club/2020/05/21/%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</id>
    <published>2020-05-21T01:35:48.000Z</published>
    <updated>2020-06-14T16:30:45.896Z</updated>
    
    <content type="html"><![CDATA[<p>扫描器是词法分析器，它接收输入的源程序，对源程序进行词法分析并识别出一个个单词符号，输出单词符号。</p><h1 id="上下文无关语言"><a href="#上下文无关语言" class="headerlink" title="上下文无关语言"></a>上下文无关语言</h1><p>表示上下文无关文法规则的形式被称为BNF，其扩展表示就是EBNF。</p><p>在BNF中，重复是使用递归表示的，重复实际分两种：嵌套重复和并列重复，并列重复对应到程序是可以用循环来实现的。</p><h2 id="EBNF"><a href="#EBNF" class="headerlink" title="EBNF"></a>EBNF</h2><p><strong>重复表示{…}</strong></p><blockquote><p>下面两种重复的递归形式表达的就是并列重复：</p><p>$A\rightarrow A \alpha|\beta$</p><p>$A\rightarrow \alpha A |\beta$</p><p>其中第一条中要求$\beta$ 不能以A开头， 而第二条中要求$\beta$不能以A结尾。对应的正则表达式为：$\beta \alpha^<em>$和$\alpha^</em>\beta$<br>EBNF中使用{…}来表示这种重复：<br>$A\rightarrow \beta {\alpha}$</p><p>$A\rightarrow{\alpha} \beta$</p></blockquote><p><strong>可选表示[…]</strong> 有点类似消除左因子。</p><blockquote><p>语句序列：<br>$stmt-sequence \rightarrow stmt; stmt-sequence | stmt$<br>可以表示为：<br>$stmt-sequence \rightarrow  stmt [ ; stmt-sequence ]$<br>$stmt-sequence \rightarrow  stmt { ; stmt }  $       </p></blockquote><h2 id="消除左递归"><a href="#消除左递归" class="headerlink" title="消除左递归"></a>消除左递归</h2><h3 id="直接简单左递归"><a href="#直接简单左递归" class="headerlink" title="直接简单左递归"></a>直接简单左递归</h3><p>$A\rightarrow A \alpha |\beta$</p><p>改写文法为：</p><p>$A\rightarrow \beta A’$</p><p>$A’\rightarrow \alpha A’ |\epsilon$</p><h3 id="间接左递归"><a href="#间接左递归" class="headerlink" title="间接左递归"></a>间接左递归</h3><p>会出现$A\Rightarrow^*A$的左递归。</p><p>处理方法：</p><p>将文法的所有非终结符按任意一种顺序排序，得到$A_1,A_2…A_n$</p><p>对每个$A_i$，如果存在一个编号比它小的非终结符，编号大的非终结符可以含有推出编号小的非终结符的句型，而且编号小的非终结符还能够推出一个句型，那么就可以进行代入操作。<br>如果有直接左递归，那么直接消除即可。</p><blockquote><p>$S\rightarrow Qc|c$</p><p>$Q\rightarrow Rb|b$</p><p>$R\rightarrow Sa|a$</p><p>1) 对S、Q、R编号1、2、3</p><p>2）i=1，无法代入，i=2，无法代入</p><p>i=3, 代入有 $R\rightarrow Qca|ca|a$,可再次代入：$R\rightarrow Rbca|bca|ca|a$</p><p>3)化简直接左递归：</p><p>$R\rightarrow bcaR’|caR’|aR’$</p><p>$R\rightarrow bcaR’|\epsilon$</p></blockquote><h2 id="消除左公因子"><a href="#消除左公因子" class="headerlink" title="消除左公因子"></a><strong>消除左公因子</strong></h2><p>对每个非终结符A，找出它的两个或多个选项之间的最长公共前缀$\alpha$,如果$\alpha$不为空，即存在一个非平凡的公共前缀，那么将所有A的产生式$A\rightarrow \alpha\beta_1|\alpha\beta_2|…\alpha\beta_n|\gamma$,</p><p>替换为：</p><p>$A\rightarrow \alpha A’|\gamma$</p><p>$A’\rightarrow \beta_1|\beta_2|…|\beta_n$</p><h2 id="递归构造上下文无关文法"><a href="#递归构造上下文无关文法" class="headerlink" title="递归构造上下文无关文法"></a>递归构造上下文无关文法</h2><p>左递归：左侧非终结符出现在右侧第一个位置。<br>$A\rightarrow A a | a$<br>右递归：左侧非终结符出现在右侧最后一个位置<br>$A \rightarrow a A | a$</p><blockquote><p>表示$\alpha\beta^*\gamma$一样的语言：</p><p>1）$A\rightarrow B\gamma$ ,$B\rightarrow B\beta|\alpha$</p><p>2) $A\rightarrow \alpha B$ ,$B\rightarrow \beta B|\gamma$</p><p>3)$A\rightarrow\alpha B\gamma$,$B\rightarrow\beta B| \epsilon$</p></blockquote><hr><p>所有的正则语言都能被上下文无关文法表示。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;扫描器是词法分析器，它接收输入的源程序，对源程序进行词法分析并识别出一个个单词符号，输出单词符号。&lt;/p&gt;
&lt;h1 id=&quot;上下文无关语言&quot;&gt;&lt;a href=&quot;#上下文无关语言&quot; class=&quot;headerlink&quot; title=&quot;上下文无关语言&quot;&gt;&lt;/a&gt;上下文无关语言&lt;</summary>
      
    
    
    
    
    <category term="编译原理" scheme="https://wuruoting.club/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>一个解决word页码错乱的小方法</title>
    <link href="https://wuruoting.club/2020/05/20/%E4%B8%80%E4%B8%AA%E8%A7%A3%E5%86%B3word%E9%A1%B5%E7%A0%81%E9%94%99%E4%B9%B1%E7%9A%84%E5%B0%8F%E6%96%B9%E6%B3%95/"/>
    <id>https://wuruoting.club/2020/05/20/%E4%B8%80%E4%B8%AA%E8%A7%A3%E5%86%B3word%E9%A1%B5%E7%A0%81%E9%94%99%E4%B9%B1%E7%9A%84%E5%B0%8F%E6%96%B9%E6%B3%95/</id>
    <published>2020-05-20T08:11:28.000Z</published>
    <updated>2020-06-14T16:31:18.836Z</updated>
    
    <content type="html"><![CDATA[<p>说实话，我常常因为word多此一举的方便用户而感到困扰。</p><p>比如拿到了一个页码错乱的文档模板，然后强迫症的我页码绝对不能错一个。</p><p>那么如果出现了这样恐怖的情况:文档页码在某一页开始一直往下都相同的话，其实是因为word看见的文档并不和你看见的一样，如果一个节没有结束的话，那么word就默认这一页还没有结束，可以理解为word看见的文档按照节来说实际上是一个不知道多长的羊皮卷。</p><p>通常如果出现页码相同的情况，可以双击页码并注意看导航栏里是否勾选了链接到前一节。我们看字面意思就能猜到，链接到前一节就指的是前后两个页面(不一定是你看到的页面，而是word认为的页面，总之）它们被绑定在一起了，修改一个另一个也会被更改。如果你发现勾选了这项的话，把它取消。我们不需要自作多情的链接。</p><p>但是还有一种情况是你发现链接到前一节没有被勾选，但页码也还是从某一页往下相同，那么就是因为word把”节“理解成了”页“，对于未完成的节，它是不会变换页码的。所以，你要做的就是告诉word这一页已经结束了。</p><p>那么具体的方法如下：</p><blockquote><p>1.光标移动到相同页面中的第一张的最末尾，点击布局-&gt; 分隔符，选择下一页，就会在该页插入分节符（下一页）</p><p>2.选中第二页的页码，就会发现此时会显示勾选了链接到前一节，取消它</p><p>3.正确地命名页码</p><p>4.如此往复直到问题解决</p></blockquote><p>不知道这个方法是不是正确，总之页码错乱问题是能够解决了，但是随之而来的是新的问题，自动目录并不能识别到对应的页码，还好自动页码可以手动改页码。</p><p>结尾也不知道要说我是该去学习一下Word呢，还是高喊”Latex真香“, 那就这样吧。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;说实话，我常常因为word多此一举的方便用户而感到困扰。&lt;/p&gt;
&lt;p&gt;比如拿到了一个页码错乱的文档模板，然后强迫症的我页码绝对不能错一个。&lt;/p&gt;
&lt;p&gt;那么如果出现了这样恐怖的情况:文档页码在某一页开始一直往下都相同的话，其实是因为word看见的文档并不和你看见的一样，</summary>
      
    
    
    
    
    <category term="小技巧" scheme="https://wuruoting.club/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
    <category term="word" scheme="https://wuruoting.club/tags/word/"/>
    
  </entry>
  
  <entry>
    <title>【综述总结2】More Data,More Relations,More Context and More Openness:A Review and Outlook for Relation Extraction</title>
    <link href="https://wuruoting.club/2020/05/02/%E7%BB%BC%E8%BF%B0%E6%80%BB%E7%BB%93-2/"/>
    <id>https://wuruoting.club/2020/05/02/%E7%BB%BC%E8%BF%B0%E6%80%BB%E7%BB%93-2/</id>
    <published>2020-05-02T00:15:09.000Z</published>
    <updated>2020-05-02T00:21:06.422Z</updated>
    
    <content type="html"><![CDATA[<h2 id="行文结构"><a href="#行文结构" class="headerlink" title="行文结构"></a>行文结构</h2><p>先关注于现有的工作，按照模型分类，其次探讨了关系提取的更多方向，主要是从数据量、学习表现、场景、领域四个方面，恰好和题目呼应；最后提出了其他的挑战。</p><h3 id="Section-1-Introduction"><a href="#Section-1-Introduction" class="headerlink" title="Section 1 Introduction"></a>Section 1 Introduction</h3><p>RE: relation extraction</p><h3 id="Section-2-背景和现有的工作"><a href="#Section-2-背景和现有的工作" class="headerlink" title="Section 2 背景和现有的工作"></a>Section 2 背景和现有的工作</h3><h4 id="2-1-模式提取模型"><a href="#2-1-模式提取模型" class="headerlink" title="2.1 模式提取模型"></a>2.1 模式提取模型</h4><h4 id="2-2-统计关系提取模型"><a href="#2-2-统计关系提取模型" class="headerlink" title="2.2 统计关系提取模型"></a>2.2 统计关系提取模型</h4><h4 id="2-3-神经网络关系提取模型"><a href="#2-3-神经网络关系提取模型" class="headerlink" title="2.3 神经网络关系提取模型"></a>2.3 神经网络关系提取模型</h4><h3 id="Section-3-RE的更多方向"><a href="#Section-3-RE的更多方向" class="headerlink" title="Section 3 RE的更多方向"></a>Section 3 RE的更多方向</h3><h4 id="3-1-使用更多数据"><a href="#3-1-使用更多数据" class="headerlink" title="3.1 使用更多数据"></a>3.1 使用更多数据</h4><h4 id="3-2-学习表现更有效"><a href="#3-2-学习表现更有效" class="headerlink" title="3.2  学习表现更有效"></a>3.2  学习表现更有效</h4><h4 id="3-3-在更复杂的场景中实现"><a href="#3-3-在更复杂的场景中实现" class="headerlink" title="3.3 在更复杂的场景中实现"></a>3.3 在更复杂的场景中实现</h4><h4 id="3-4-面向更开放的领域"><a href="#3-4-面向更开放的领域" class="headerlink" title="3.4 面向更开放的领域"></a>3.4 面向更开放的领域</h4><h3 id="Section-4-其他挑战"><a href="#Section-4-其他挑战" class="headerlink" title="Section  4 其他挑战"></a>Section  4 其他挑战</h3><h4 id="4-1-从文本或实体名中学习"><a href="#4-1-从文本或实体名中学习" class="headerlink" title="4.1 从文本或实体名中学习"></a>4.1 从文本或实体名中学习</h4><h4 id="4-2关系提取数据集"><a href="#4-2关系提取数据集" class="headerlink" title="4.2关系提取数据集"></a>4.2关系提取数据集</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;行文结构&quot;&gt;&lt;a href=&quot;#行文结构&quot; class=&quot;headerlink&quot; title=&quot;行文结构&quot;&gt;&lt;/a&gt;行文结构&lt;/h2&gt;&lt;p&gt;先关注于现有的工作，按照模型分类，其次探讨了关系提取的更多方向，主要是从数据量、学习表现、场景、领域四个方面，恰好和题目呼应</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://wuruoting.club/tags/NLP/"/>
    
    <category term="综述" scheme="https://wuruoting.club/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>【综述总结1】Analysis Methods in Neural Language Processing:A Survey</title>
    <link href="https://wuruoting.club/2020/04/30/%E7%BB%BC%E8%BF%B0%E6%80%BB%E7%BB%93/"/>
    <id>https://wuruoting.club/2020/04/30/%E7%BB%BC%E8%BF%B0%E6%80%BB%E7%BB%93/</id>
    <published>2020-04-29T16:00:00.000Z</published>
    <updated>2020-05-02T00:19:08.047Z</updated>
    
    <content type="html"><![CDATA[<h2 id="行文结构"><a href="#行文结构" class="headerlink" title="行文结构"></a>行文结构</h2><p>论文进行综述时很善于从多个维度对工作进行分类，比如使用的方法、语言现象等。（作者也输出了一张表格、通过不同的维度归纳了相关的工作）。整体来说是按照不同角度下的方法进行综述，比如有一章专门写了自然语言处理中的可视化方法和衡量的难度性。同时最后将所有本文的结论写在了结语。</p><h3 id="Section-1-Introduction"><a href="#Section-1-Introduction" class="headerlink" title="Section 1 Introduction"></a>Section 1 Introduction</h3><p>对全文脉络进行梳理</p><h3 id="Section-2-什么样的语言信息会被神经网络使用？"><a href="#Section-2-什么样的语言信息会被神经网络使用？" class="headerlink" title="Section 2 什么样的语言信息会被神经网络使用？"></a>Section 2 什么样的语言信息会被神经网络使用？</h3><p>阐述从三个方面回答这一问题（和小标题对应）：</p><ul><li>which method are used  使用什么方法</li><li>what kind of linguistic information is sought 什么信息</li><li>which objects in the neural network are being investigated  什么被观测</li></ul><h4 id="2-2-Linguistic-Phenomena"><a href="#2-2-Linguistic-Phenomena" class="headerlink" title="2.2 Linguistic Phenomena"></a>2.2 Linguistic Phenomena</h4><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>sentence length</td><td>句子长度</td><td></td></tr><tr><td>word position</td><td>单词位置</td><td></td></tr><tr><td>word presence</td><td>文字出现</td><td></td></tr><tr><td>simple word order</td><td>简单词序</td><td></td></tr><tr><td>morphological information</td><td>形态信息</td><td></td></tr><tr><td>syntactic information</td><td>句法信息</td><td></td></tr><tr><td>semantic information</td><td>语义信息</td><td></td></tr></tbody></table><p><a href="https://boknilev.github.io/nlp-analysis-methods/table2.html" target="_blank" rel="noopener">other phenomena</a></p><h3 id="Section-3-可视化方法和衡量可视化工作的难度性"><a href="#Section-3-可视化方法和衡量可视化工作的难度性" class="headerlink" title="Section 3 可视化方法和衡量可视化工作的难度性"></a>Section 3 可视化方法和衡量可视化工作的难度性</h3><h3 id="Section-4-用于细粒度评估的挑战集的编译"><a href="#Section-4-用于细粒度评估的挑战集的编译" class="headerlink" title="Section 4 用于细粒度评估的挑战集的编译"></a>Section 4 用于细粒度评估的挑战集的编译</h3><p>datasets used for evaluating neural network models that diverge from the common average case evaluatio</p><blockquote><p>分类数据集的依据：</p><ul><li>the task they seek to evaluate</li><li>the linguistic phenomena they aim to study</li><li>the language(s) they target</li><li>their size</li><li>their method of construction</li><li>how performance is evaluated</li></ul></blockquote><h3 id="Section-5-对抗性例子的产生和使用、神经网络的弱点"><a href="#Section-5-对抗性例子的产生和使用、神经网络的弱点" class="headerlink" title="Section 5 对抗性例子的产生和使用、神经网络的弱点"></a>Section 5 对抗性例子的产生和使用、神经网络的弱点</h3><h3 id="Section-6-解释模型预测的工作"><a href="#Section-6-解释模型预测的工作" class="headerlink" title="Section 6 解释模型预测的工作"></a>Section 6 解释模型预测的工作</h3><h3 id="Section-7-其他不归于上述主题的方法"><a href="#Section-7-其他不归于上述主题的方法" class="headerlink" title="Section 7 其他不归于上述主题的方法"></a>Section 7 其他不归于上述主题的方法</h3><h2 id="引用他人工作的句式"><a href="#引用他人工作的句式" class="headerlink" title="引用他人工作的句式"></a>引用他人工作的句式</h2><ul><li><p>see somebody for example</p></li><li><p>They found … suggesting that </p></li><li><p>In contrast,</p></li><li><p>Somebody <strong>made some headway on</strong> this question.</p></li><li><p>Somebody noted several key properties…</p></li><li><p>conducted behavioral experiments</p></li></ul><h2 id="其他句式"><a href="#其他句式" class="headerlink" title="其他句式"></a>其他句式</h2><ul><li><p>enable them to draw conclusions about…</p></li><li><p>synthesize a holistic picture from this diverse body of work</p></li><li><p>Another theme that emerges in several  studies is…</p></li><li><p>Much recent work has focused on …</p></li><li><p>… have gained renewed popularity in the NLP community.</p></li><li><p>Most of the relevant analysis work is concerned with…</p></li><li><p>Method … may shed new light on some of these questions.</p></li><li><p>A long tradition in work on (domain) is to …</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;行文结构&quot;&gt;&lt;a href=&quot;#行文结构&quot; class=&quot;headerlink&quot; title=&quot;行文结构&quot;&gt;&lt;/a&gt;行文结构&lt;/h2&gt;&lt;p&gt;论文进行综述时很善于从多个维度对工作进行分类，比如使用的方法、语言现象等。（作者也输出了一张表格、通过不同的维度归纳了相关的</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://wuruoting.club/tags/NLP/"/>
    
    <category term="综述" scheme="https://wuruoting.club/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>D3笔记</title>
    <link href="https://wuruoting.club/2020/04/03/D3%E7%AC%94%E8%AE%B0/"/>
    <id>https://wuruoting.club/2020/04/03/D3%E7%AC%94%E8%AE%B0/</id>
    <published>2020-04-03T03:38:29.000Z</published>
    <updated>2020-04-05T09:53:00.526Z</updated>
    
    <content type="html"><![CDATA[<p>loading data onto the page</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">waitSeconds=<span class="function"><span class="params">numSeconds</span>=&gt;</span><span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function"><span class="params">resolve</span>=&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">const</span> message=<span class="string">`<span class="subst">$&#123;numSeconds&#125;</span> seconds have passed!`</span>;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span>=&gt;</span>resolve(message),numSeconds*<span class="number">1000</span>);</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">waitSeconds(<span class="number">2</span>).then(<span class="function"><span class="params">message</span>=&gt;</span><span class="built_in">console</span>.log(message));</span><br></pre></td></tr></table></figure><blockquote><p>结果：等待两秒后显示 2 seconds have passed!</p></blockquote><h2 id="ECMAScript-6-Features"><a href="#ECMAScript-6-Features" class="headerlink" title="ECMAScript 6 Features"></a>ECMAScript 6 Features</h2><h2 id="HTML-CSS-SVG"><a href="#HTML-CSS-SVG" class="headerlink" title="HTML CSS SVG"></a>HTML CSS SVG</h2><p>Scalable Vector Graph</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>&#123;select,arc&#125; <span class="keyword">from</span> <span class="string">'d3'</span></span><br></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> width=+svg.attr(<span class="string">'width'</span>) <span class="comment">//string转换为float</span></span><br><span class="line"><span class="keyword">const</span> width=<span class="built_in">parseFloat</span>(svg.attr(<span class="string">'width'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> circle=svg.append(<span class="string">'circle'</span>)</span><br><span class="line">.attr(<span class="string">'fill'</span>,<span class="string">'yellow'</span>) </span><br><span class="line">.attr(<span class="string">'stroke'</span>,<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> g=svg.append(<span class="string">'g'</span>)</span><br><span class="line">.attr(<span class="string">'transform'</span>,<span class="string">'translate($&#123;width/2&#125;,$&#123;height/2&#125;)'</span>);</span><br><span class="line"><span class="keyword">const</span> mouth=svg.append(<span class="string">'path'</span>)</span><br><span class="line">.attr(<span class="string">'d'</span>,acr()(&#123;</span><br><span class="line">        innerRadius:<span class="number">80</span>,</span><br><span class="line">        outerRadius:<span class="number">100</span>,</span><br><span class="line">        startAngle:<span class="built_in">Math</span>.PI/<span class="number">2</span>,</span><br><span class="line">        endAngle:<span class="built_in">Math</span>,PI*<span class="number">3</span>/<span class="number">2</span></span><br><span class="line">    &#125;));</span><br></pre></td></tr></table></figure><h2 id="d3"><a href="#d3" class="headerlink" title="d3"></a>d3</h2><p>csv comma seperated value</p><h3 id="customizing-axis"><a href="#customizing-axis" class="headerlink" title="customizing  axis"></a>customizing  axis</h3><p>d3.format() 函数</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;loading data onto the page&lt;/p&gt;
&lt;figure class=&quot;highlight javascript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;sp</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>hexo的坑和新博客</title>
    <link href="https://wuruoting.club/2020/03/31/hexo%E7%9A%84%E5%9D%91%E5%92%8C%E6%96%B0%E5%8D%9A%E5%AE%A2/"/>
    <id>https://wuruoting.club/2020/03/31/hexo%E7%9A%84%E5%9D%91%E5%92%8C%E6%96%B0%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-03-31T01:45:41.955Z</published>
    <updated>2020-05-01T08:58:11.845Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h2><h3 id="给一篇文章加多个tags"><a href="#给一篇文章加多个tags" class="headerlink" title="给一篇文章加多个tags"></a>给一篇文章加多个tags</h3><p>tags:[‘a’,’b’]</p><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;hexo&quot;&gt;&lt;a href=&quot;#hexo&quot; class=&quot;headerlink&quot; title=&quot;hexo&quot;&gt;&lt;/a&gt;hexo&lt;/h2&gt;&lt;h3 id=&quot;给一篇文章加多个tags&quot;&gt;&lt;a href=&quot;#给一篇文章加多个tags&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
    <category term="博客" scheme="https://wuruoting.club/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
  <entry>
    <title>文档主题建模</title>
    <link href="https://wuruoting.club/2019/11/26/%E6%96%87%E6%A1%A3%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1/"/>
    <id>https://wuruoting.club/2019/11/26/%E6%96%87%E6%A1%A3%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1/</id>
    <published>2019-11-26T12:00:00.000Z</published>
    <updated>2020-04-01T01:50:24.958Z</updated>
    
    <content type="html"><![CDATA[<p>对于一篇文章来说，分析它的主题能够达到理解文本的效果。主题建模就是通过在文档集合里面学习、识别和提取主题的过程。对于一篇文章 或者说一个文档来说，它包含着多个主题，而如何去区分不同的主题，是通过主题下面包含的多个单词来进行分析，我们能够将文档转化为一个数值向量，每一个维度对应一个主题。</p><h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><ul><li><strong>分类文档</strong> （比如说不同领域的新闻：科技、金融、体育新闻。通过对新闻的主题建模，能够将文本按照主题来归类）</li><li><strong>检索</strong>（当用户输入关键字的时候，就能够确认检索的文本的主题，从而在数据库进行匹配，最终返回相符的文本）</li></ul><h2 id="文本模型的可交换性"><a href="#文本模型的可交换性" class="headerlink" title="文本模型的可交换性"></a>文本模型的可交换性</h2><p>目前，大多数文本模型都基于“bag-of-words”的假设，即</p><ul><li>1.一篇文档内N个词之间的顺序可以随意互换，不影响建模过程</li><li>2.一个语料库内M个文档可以随意互换顺序，哪个文档在前哪个文档在后都无所谓。这两个性质合称为文本模型的可交换性</li></ul><h2 id="四种流行的用于主题建模的算法"><a href="#四种流行的用于主题建模的算法" class="headerlink" title="四种流行的用于主题建模的算法"></a>四种流行的用于主题建模的算法</h2><h3 id="1-LSA-Latent-semantic-analysis"><a href="#1-LSA-Latent-semantic-analysis" class="headerlink" title="1.LSA(Latent semantic analysis)"></a>1.LSA(Latent semantic analysis)</h3><p>LSA的核心思想就是将我们所拥有的文档-术语矩阵分解成相互独立的文档-主题矩阵和主题-术语矩阵。词和文档是用向量来表示的，通过向量之间的关系，来判断词与词之间 或者文档与文档之间的关系。</p><h3 id="2-pLSA"><a href="#2-pLSA" class="headerlink" title="2.pLSA"></a>2.pLSA</h3><p>pLSA，即概率潜在语义分析，采取概率方法替代 SVD 以解决问题。其核心思想是找到一个潜在主题的概率模型，该模型可以生成我们在文档-术语矩阵中观察到的数据。</p><h3 id="3-LDA"><a href="#3-LDA" class="headerlink" title="3.LDA"></a>3.LDA</h3><p>将狄利克雷视为「分布的分布」。本质上，它回答了这样一个问题：「给定某种分布，我看到的实际概率分布可能是什么样子？」</p><p>一篇文档，可以看成是一组有序的词的序列。从统计学角度来看，文档的生成可以看成是上帝抛掷骰子生成的结果，每一次抛掷骰子都生成一个词汇，抛掷N词生成一篇文档。在统计文本建模中，我们希望猜测出上帝是如何玩这个游戏的，这会涉及到两个最核心的问题：上帝都有什么样的骰子；上帝是如何抛掷这些骰子的；第一个问题就是表示模型中都有哪些参数，骰子的每一个面的概率都对应于模型中的参数；第二个问题就表示游戏规则是什么，上帝可能有各种不同类型的骰子，上帝可以按照一定的规则抛掷这些骰子从而产生词序列。</p><h3 id="4-lda2vec"><a href="#4-lda2vec" class="headerlink" title="4.lda2vec"></a>4.lda2vec</h3><p>社交媒体如微博、脸书上也会有大量值得研究的文本，这些文本规模大、更新速度更快而且语义信息不丰富、噪声高。传统的pLSA和LDA模型泛化能力弱、主题词可解释性差、分类准确性低。</p><p>文档向量表示随着word2vec模型的提出和深度学习的发展,近年来出现了很多相关研究成果。以LDA为代表的主题模型认为文档的生成是不同主题混合的结果;神经网络模型习惯于将文档表示为稠密向量。如果结合前者覆盖范围广和后者维度低的特点生成新的模型,可以做到快速检测,同对隐含语义的解释也会更好。lda2vec模型就是基于这一思想提出的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;对于一篇文章来说，分析它的主题能够达到理解文本的效果。主题建模就是通过在文档集合里面学习、识别和提取主题的过程。对于一篇文章 或者说一个文档来说，它包含着多个主题，而如何去区分不同的主题，是通过主题下面包含的多个单词来进行分析，我们能够将文档转化为一个数值向量，每一个维度对</summary>
      
    
    
    
    
    <category term="文本挖掘" scheme="https://wuruoting.club/tags/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/"/>
    
    <category term="主题建模" scheme="https://wuruoting.club/tags/%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
</feed>
